{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "In this exercise, we will use PyTorch to build a U-Net to segment tumors in brain images.\n",
    "\n",
    "We have discussed brain tumors segmentation in lecture 2. You can review from the slides of lecture 2 (slides 66-72). For more general image segmentation, you can check this [link](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-1-89e8297a0923) for a quick tutorial.\n",
    "\n",
    "Please use this [link](https://drive.google.com/file/d/1xtmBoayrtbhmtOp1gfkcxgCSNzXMPpmu/view?usp=sharing) to download the dataset we use for this exercise session. After downloading, you should unzip it and then put the folder *dataset* to the same path as *Exercise_2.ipynb*. This dataset contains 3064 T1-weighted contrast-enhanced images with brain tumor. For a detailed information about the dataset please refer to this [site](https://figshare.com/articles/dataset/brain_tumor_dataset/1512427).\n",
    "\n",
    "There are four important files for this exercise:\n",
    "- dataset.py: We create dataset from our own data here.\n",
    "- model.py: We build the model U-Net using PyTorch here.\n",
    "- training.py: We define the training loop, test loop, and predict phase here.\n",
    "- utils.py: We define some other functions here.\n",
    "\n",
    "In this exercise, we will:\n",
    "- Create our dataset in **dataset.py**.\n",
    "- Build the model in **model.py**.\n",
    "- Complete the training loop in **training.py**.\n",
    "\n",
    "The goal is to help you become more familiar with the following:\n",
    "- How to create your dataset by inheriting the *Dataset* class from pytorch.\n",
    "- How to build a network using *torch.nn*.\n",
    "- How to create an optimizer using *torch.optim*.\n",
    "\n",
    "Reference:\n",
    "\n",
    "- https://github.com/sdsubhajitdas/Brain-Tumor-Segmentation\n",
    "- https://www.analyticsvidhya.com/blog/2022/10/image-segmentation-with-u-net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the package and define the device here. Change the DATASET_PATH to the path to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import dataset\n",
    "import model\n",
    "import training\n",
    "import utils\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset part used for testing\n",
    "TEST_SPLIT = 0.2\n",
    "# Batch size for training. Limited by GPU memory\n",
    "BATCH_SIZE = 6\n",
    "# Dataset folder used\n",
    "DATASET_USED = 'png_dataset'\n",
    "# Full Dataset path\n",
    "DATASET_PATH = os.path.join('dataset',DATASET_USED)\n",
    "# Training Epochs\n",
    "EPOCHS = 20\n",
    "# Filters used in UNet Model\n",
    "filter_num = [16,32,64,128,256]\n",
    "\n",
    "MODEL_NAME = f\"UNet-{filter_num}.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset and Dataloader\n",
    "\n",
    "Let's build a dataset from our own data! Here, you can try how to create your dataset by inheriting the *Dataset* class from pytorch.\n",
    "\n",
    "Please open **dataset.py** to check what to implement. The three ToDo are ordered as ToDo 1, ToDo 2, ... Please check them and make sure you do not miss anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_dataset = dataset.TumorDataset(DATASET_PATH)\n",
    "\n",
    "train_indices, test_indices = utils.get_indices(len(tumor_dataset), DATASET_USED, TEST_SPLIT)\n",
    "train_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(tumor_dataset, BATCH_SIZE, sampler=train_sampler)\n",
    "testloader = torch.utils.data.DataLoader(tumor_dataset, 1, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "U-Net is a network structure always used in biomedical image segmentation. In this section, you will try to build an U-Net with torch.nn.\n",
    "\n",
    "Here, we explain the structure of the U-Net. An example structure is shown in the following figure. (We use this figure only to show a general structure, and the details can be changed flexibly. For example, the number of blocks, the number of convolution layers in each block, and the size of each convolution layer.)\n",
    "\n",
    "<img src=\"u-net-architecture.png\" width=\"800\" height=\"550\">\n",
    "\n",
    "There are three parts in the U-net: \n",
    "- **Encoder**: Each encoder block contains two convolutional layers and one max-pooling layer. (In the Figure, two *conv 3x3, ReLU* and one *max pool 2x2*.)\n",
    "- **Bottleneck**: There is only one bottleneck block, containing two convolutional layers. (In the Figure, two *conv 3x3, ReLU*.)\n",
    "- **Decoder**: The decoder is more complicated. Each decoder block takes the output of the previous decoder block as a part of the input. Importantly, it also takes the output of a corresponding encoder block as a part of the input (the *copy and crop* part in the figure), which is named a skip connection. Each decoder block contains an upsample convolution layer, a skip connection part, and two convolutional layers. (In the Figure, one *up-conv 2x2*, one *copy and crop*, and two *conv 3x3, ReLU*)\n",
    "\n",
    "You can check this [link](https://www.analyticsvidhya.com/blog/2022/10/image-segmentation-with-u-net/) for more details.\n",
    "\n",
    "Let's build a U-Net using PyTorch in file **model.py**. We provide pseudo code in model.py. Again, the eight ToDo are ordered as ToDo 1, ToDo 2, ... Please check them and make sure you do not miss anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = model.UNet(filter_num).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Training\n",
    "\n",
    "To train the model, we need to define the training loop. We need to complete the 'train' and 'train_step' function in file **training.py**. Again, the seven ToDo are ordered as ToDo 1, ToDo 2, ... Please check them and make sure you do not miss anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Batch: 01,\tBatch Loss: 0.2746434\n",
      "Batch: 02,\tBatch Loss: 0.2749901\n",
      "Batch: 03,\tBatch Loss: 0.2736833\n",
      "Batch: 04,\tBatch Loss: 0.2720812\n",
      "Batch: 05,\tBatch Loss: 0.2696763\n",
      "Batch: 06,\tBatch Loss: 0.2719970\n",
      "Batch: 07,\tBatch Loss: 0.2717732\n",
      "Batch: 08,\tBatch Loss: 0.2668708\n",
      "Batch: 09,\tBatch Loss: 0.2634313\n",
      "Batch: 10,\tBatch Loss: 0.2488616\n",
      "Batch: 11,\tBatch Loss: 0.1965094\n",
      "Batch: 12,\tBatch Loss: 0.2436749\n",
      "Batch: 13,\tBatch Loss: 0.1960908\n",
      "Batch: 14,\tBatch Loss: 0.1862632\n",
      "Batch: 15,\tBatch Loss: 0.2026550\n",
      "Batch: 16,\tBatch Loss: 0.1996802\n",
      "Batch: 17,\tBatch Loss: 0.1906321\n",
      "Batch: 18,\tBatch Loss: 0.1824203\n",
      "Batch: 19,\tBatch Loss: 0.1958179\n",
      "Batch: 20,\tBatch Loss: 0.1829164\n",
      "Batch: 21,\tBatch Loss: 0.1844488\n",
      "Batch: 22,\tBatch Loss: 0.1813248\n",
      "Batch: 23,\tBatch Loss: 0.1811126\n",
      "Batch: 24,\tBatch Loss: 0.1858442\n",
      "Batch: 25,\tBatch Loss: 0.1822037\n",
      "Batch: 26,\tBatch Loss: 0.1801827\n",
      "Batch: 27,\tBatch Loss: 0.1816039\n",
      "Batch: 28,\tBatch Loss: 0.1754501\n",
      "Batch: 29,\tBatch Loss: 0.1949594\n",
      "Batch: 30,\tBatch Loss: 0.1753143\n",
      "Batch: 31,\tBatch Loss: 0.1784821\n",
      "Batch: 32,\tBatch Loss: 0.1841375\n",
      "Batch: 33,\tBatch Loss: 0.1802893\n",
      "Batch: 34,\tBatch Loss: 0.1800190\n",
      "Batch: 35,\tBatch Loss: 0.1809838\n",
      "Batch: 36,\tBatch Loss: 0.1781475\n",
      "Batch: 37,\tBatch Loss: 0.1777272\n",
      "Batch: 38,\tBatch Loss: 0.1778069\n",
      "Batch: 39,\tBatch Loss: 0.1820232\n",
      "Batch: 40,\tBatch Loss: 0.1745515\n",
      "Batch: 41,\tBatch Loss: 0.1754279\n",
      "Batch: 42,\tBatch Loss: 0.1765686\n",
      "Batch: 43,\tBatch Loss: 0.1784340\n",
      "Batch: 44,\tBatch Loss: 0.1788499\n",
      "Batch: 45,\tBatch Loss: 0.1779347\n",
      "Batch: 46,\tBatch Loss: 0.1769664\n",
      "Batch: 47,\tBatch Loss: 0.1774787\n",
      "Batch: 48,\tBatch Loss: 0.1787894\n",
      "Batch: 49,\tBatch Loss: 0.1845028\n",
      "Batch: 50,\tBatch Loss: 0.1794244\n",
      "Batch: 51,\tBatch Loss: 0.1783518\n",
      "Batch: 52,\tBatch Loss: 0.1790497\n",
      "Batch: 53,\tBatch Loss: 0.1771659\n",
      "Batch: 54,\tBatch Loss: 0.1824990\n",
      "Batch: 55,\tBatch Loss: 0.1727881\n",
      "Batch: 56,\tBatch Loss: 0.1731156\n",
      "Batch: 57,\tBatch Loss: 0.1723934\n",
      "Batch: 58,\tBatch Loss: 0.1800895\n",
      "Batch: 59,\tBatch Loss: 0.1764297\n",
      "Batch: 60,\tBatch Loss: 0.1750164\n",
      "Batch: 61,\tBatch Loss: 0.1748377\n",
      "Batch: 62,\tBatch Loss: 0.1721377\n",
      "Batch: 63,\tBatch Loss: 0.1772215\n",
      "Batch: 64,\tBatch Loss: 0.1795027\n",
      "Batch: 65,\tBatch Loss: 0.1725335\n",
      "Batch: 66,\tBatch Loss: 0.1731100\n",
      "Batch: 67,\tBatch Loss: 0.1750452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m unet_trainer \u001b[38;5;241m=\u001b[39m training\u001b[38;5;241m.\u001b[39mTrainer(unet_model,device)\n\u001b[0;32m----> 4\u001b[0m loss_record \u001b[38;5;241m=\u001b[39m \u001b[43munet_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining finished!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Dev/personal/epfl-cs502/coursework/exercises/w2/training.py:56\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, trainloader, mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Epoch Loop\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Training a single epoch\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Collecting all epoch loss values for future visualization.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     loss_record\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "File \u001b[0;32m~/Dev/personal/epfl-cs502/coursework/exercises/w2/training.py:96\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, trainloader, mini_batch)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# ToDo 4: Calculation predicted output using forward pass.\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# ToDo 5: Calculating the loss value.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Hint: self.criterion\u001b[39;00m\n\u001b[1;32m    100\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, mask)\n",
      "File \u001b[0;32m~/Dev/personal/epfl-cs502/coursework/exercises/w2/model.py:151\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv8_2(x))\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m#   Block 1\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m xup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv9_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# print(xup.shape, x1out.shape,)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv9_1(torch\u001b[38;5;241m.\u001b[39mcat([x1out, xup], \u001b[38;5;241m1\u001b[39m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/cs502/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs502/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs502/lib/python3.9/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "unet_trainer = training.Trainer(unet_model,device)\n",
    "\n",
    "loss_record = unet_trainer.train(EPOCHS,trainloader,mini_batch=1)\n",
    "\n",
    "print(f'Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Testing\n",
    "\n",
    "We test our model here. We use Dice Score for testing. We provide the code of testing. \n",
    "\n",
    "To know more about Dice Score, check this link: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing process on test data.\n",
    "unet_score = unet_trainer.test(testloader)\n",
    "\n",
    "print(f'Dice Score {unet_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Test Dataset Predictions\n",
    "\n",
    "Now, you can visualize the prediction of your model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = test_indices[0]\n",
    "sample = tumor_dataset[image_index]\n",
    "image, mask, output, d_score = unet_trainer.predict(sample,0.65)\n",
    "title = f'Name: {image_index}.png   Dice Score: {d_score:.5f}'\n",
    "utils.result(image,mask,output,title,save_path=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
