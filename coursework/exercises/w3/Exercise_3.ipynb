{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04225f52-cdd2-4ad0-82e1-45479f29e90d",
   "metadata": {},
   "source": [
    "# Exercise 3 - GNNs\n",
    "## Protein-Protein Interaction Prediction using Graph Neural Networks\n",
    "\n",
    "In this exercise, you will learn how to implement and apply Graph Neural Networks (GNNs) to predict [protein-protein interactions](https://en.wikipedia.org/wiki/Protein%E2%80%93protein_interaction) (PPIs). GNNs have shown great potential in various biological applications, including predicting protein functions, drug discovery, and more.\n",
    "\n",
    "Protein-protein interactions (PPIs) are essential to almost every process in a cell. Understanding PPIs is crucial for understanding cell physiology in normal and disease states. Furthermore, knowledge of PPIs can be used:\n",
    "* for drug development, since drugs can affect PPIs,\n",
    "* to assign roles (i.e., protein functions) to uncharacterized proteins,\n",
    "* to characterize the relationships between proteins that form multi-molecular complexes, such as the proteasome.\n",
    "\n",
    "We represent the totality of PPIs that happen in a cell, an organism or a specific biological context with a protein-protein interaction network. These networks are mathematical representations of all physical contacts between proteins in the cell.\n",
    "\n",
    "However, current knowledge of protein-protein interaction networks is both incomplete and noisy, as PPI screening techniques are limited in how many true interactions they can detect. Furthermore, PPI screening techniques often have high false positive and negative rates. These limitations present a great opportunity for computational methods to predict protein-protein interactions.\n",
    "\n",
    "Therefore, we are interested to take a protein-protein interaction network and use it to build a model for predicting new protein-protein interactions. We formulate this prediction task as a **link prediction problem** on unweighted and undirected networks and use a graph neural network to solve the task.\n",
    "\n",
    "We will use a sample protein-protein interaction network from [yeast *S. cerevisiae*](https://en.wikipedia.org/wiki/Saccharomyces_cerevisiae), in the form of an adjacency matrix and node feature matrix. Each node represents a protein, and the goal is to predict whether two proteins interact, i.e., is there an edge between the two nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ad672-a1b1-4c3d-a688-d5a0da4ead52",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Let's import the necessary libraries. Note that this week, we'll implement the GNNs in pure [PyTorch](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f549bf9b-5f64-4aff-831d-5dd68ce2a2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f96c08-78cf-45c0-b88f-340f8217327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954700de-eaf8-4691-8833-e71c8d6f0c69",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "\n",
    "### 2.1. Downloading the data\n",
    "First, download the data from [here](https://snap.stanford.edu/deepnetbio-ismb/ipynb/yeast.edgelist) and copy it into a file called `yeast.edgelist`. This data contains the edges between the proteins.\n",
    "We then build a graph from this edge-list data, and extract the adjacency matrix.\n",
    "\n",
    "For this exercise, we will sample a subset of the graph to make it faster and easier.\n",
    "\n",
    "The entire dataset is represented as a single graph and our goal will be to train a GNN to predict new edges on this graph, representing potential new protein-protein interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36dded35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Some utility functions\n",
    "def load_data(filename='yeast.edgelist'):\n",
    "    \"\"\"Load the edgelist data and build a graph from it.\"\"\"\n",
    "    graph = nx.read_edgelist(filename)\n",
    "    adjacency = nx.adjacency_matrix(graph)\n",
    "    return graph, adjacency\n",
    "\n",
    "def subsample_graph(graph, max_nodes=1000):\n",
    "    \"\"\"Sub-sample the graph to make it smaller.\"\"\"\n",
    "    graph = graph.subgraph(np.random.choice(graph.nodes(), max_nodes, replace=False)).copy()\n",
    "    graph = graph.subgraph(sorted(nx.connected_components(graph), key=len, reverse=True)[0]).copy()  # keep only the largest connected component\n",
    "    graph.remove_nodes_from(list(nx.isolates(graph)))\n",
    "    adjacency = nx.adjacency_matrix(graph)\n",
    "    return graph, adjacency\n",
    "\n",
    "def get_edge_index(adjacency):\n",
    "    \"\"\"Return the indices of the edges in the graph.\"\"\"\n",
    "    # Edges are undirected here, so we consider only one half of the adjacency matrix\n",
    "    adjacency = sp.triu(adjacency)\n",
    "    return np.stack(adjacency.nonzero(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bafd0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 934 nodes and 12916 edges\n"
     ]
    }
   ],
   "source": [
    "# Load the data and prepare a subgraph\n",
    "graph, adjacency = load_data('yeast.edgelist')\n",
    "graph, adjacency = subsample_graph(graph, max_nodes=1000)\n",
    "edge_index = get_edge_index(adjacency)\n",
    "print(graph)\n",
    "\n",
    "N_NODES = adjacency.shape[0]\n",
    "N_EDGES = edge_index.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55495a09",
   "metadata": {},
   "source": [
    "### 2.2. Data splitting\n",
    "\n",
    "Now, let's split the data into training, validation, and test sets. To create the validation and test sets, we will select some existing edges to create positive examples and then sample some absent edges (by verifying they are not in the graph) to have negative examples.\n",
    "\n",
    "So, for each set, we:\n",
    "1. sample X% of the edges as the positive examples,\n",
    "2. create negative examples by selecting pairs of node that are not connected in the original graph, as many as we have positive edges.\n",
    "\n",
    "Finally, the train set will consist of the rest of the edges. In order to avoid training on the validation and test edges, we also build a mask for the adjacency matrix that will indicate if we supervise a given edge during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faea399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges split into:\n",
      " training: 11626\n",
      " validation: 645\n",
      " test: 645\n",
      "Ratio of negative/positive edges: 36.68\n"
     ]
    }
   ],
   "source": [
    "TEST_EDGE_RATIO = 0.05  # percent of edges to use for the validation and test set respectively\n",
    "\n",
    "def split_graph_edges(adj, edge_index, test_edge_ratio=TEST_EDGE_RATIO):\n",
    "    \"\"\"Split the graph into train, valid and test edge sets.\"\"\"\n",
    "    # Get validation and test edges\n",
    "    n_valid = n_test = int(N_EDGES * test_edge_ratio)\n",
    "    valid_edges = edge_index[:, :n_valid]\n",
    "    test_edges = edge_index[:, n_valid:n_valid + n_test]\n",
    "\n",
    "    # Remove them from the training set\n",
    "    train_edges = edge_index[:, n_valid + n_test:]\n",
    "\n",
    "    # Sample \"negative edges\" for validation and test sets\n",
    "    # As the adjacency matrix is sparse, we can sample them\n",
    "    # randomly instead of enumerating all non-edges.\n",
    "    neg_edge = []\n",
    "    num_neg = n_valid + n_test\n",
    "    while len(neg_edge) < num_neg:\n",
    "        i, j = np.random.randint(0, N_NODES, size=(2,))\n",
    "        if i != j and adj[i, j] == 0 and [i, j] not in neg_edge:\n",
    "            neg_edge.append([i, j])\n",
    "    valid_edges_neg = np.array(neg_edge[:n_valid]).T\n",
    "    test_edges_neg = np.array(neg_edge[n_valid:]).T\n",
    "\n",
    "    # Rebuild the training adjacency matrix\n",
    "    adj_train = sp.csr_matrix((np.ones(train_edges.shape[1]), train_edges), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T - sp.diags(adj_train.diagonal(), dtype=int)\n",
    "    train_edges = get_edge_index(adj_train)\n",
    "    \n",
    "    # Build a training mask to avoid training on valid/test edges\n",
    "    train_mask = np.ones(adj_train.shape, dtype=bool)\n",
    "    for edges in [valid_edges, valid_edges_neg, test_edges, test_edges_neg]:\n",
    "        train_mask[edges[0], edges[1]] = False\n",
    "        train_mask[edges[1], edges[0]] = False  # also remove the reverse direction\n",
    "\n",
    "    return adj_train, train_edges, train_mask, valid_edges, valid_edges_neg, test_edges, test_edges_neg\n",
    "\n",
    "adj_train, train_edges, train_mask, valid_edges, valid_edges_neg, test_edges, test_edges_neg = split_graph_edges(adjacency, edge_index, TEST_EDGE_RATIO)\n",
    "print(f\"Edges split into:\\n training: {train_edges.shape[1]}\\n validation: {valid_edges.shape[1]}\\n test: {test_edges.shape[1]}\")\n",
    "\n",
    "n_pos_edge = adj_train[train_mask].sum()\n",
    "n_neg_edge = (1 - adj_train.todense())[train_mask].sum()\n",
    "pos_weight = n_neg_edge / n_pos_edge  # weight of positive edges to compensate for the large imbalance\n",
    "print(f\"Ratio of negative/positive edges: {pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e75c23",
   "metadata": {},
   "source": [
    "Let's visualize (a part of) the graph data below. Note that we randomly sample nodes for this visualization, so it is possible we end up with more than a single connected component, even though the full graph has only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bf36d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 278 nodes and 1430 edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 300  # number of node to keep for the visualization\n",
    "\n",
    "nodelist = np.random.permutation(graph.nodes())[:N]\n",
    "subgraph = graph.subgraph(nodelist).copy()\n",
    "subgraph.remove_edges_from(list(nx.selfloop_edges(subgraph)))  # remove self-loops for visualization\n",
    "subgraph.remove_nodes_from(list(nx.isolates(subgraph)))  # remove isolated nodes for visualization\n",
    "print(subgraph)\n",
    "\n",
    "pos = nx.spring_layout(subgraph, iterations=100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_title('Sub-graph of the yeast protein-protein interaction network')\n",
    "for edge_set, color in [(train_edges, 'C0'), (valid_edges, 'C1'), (test_edges, 'C2')]:\n",
    "    edgelist = subgraph.edges(np.array(graph.nodes())[edge_set[0]])\n",
    "    nx.draw_networkx(subgraph, pos, edgelist=edgelist, ax=ax, with_labels=False, node_size=5, node_color='black', edge_color=color, width=1., alpha=0.5)\n",
    "ax.legend([Line2D([], [], color=c, linewidth=1., alpha=0.5) for c in ('C0', 'C1', 'C2')], ['train', 'validation', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55a097",
   "metadata": {},
   "source": [
    "### 2.3. Preparing the data\n",
    "\n",
    "Now, we can prepare the data for PyTorch. The model takes as input the node embeddings and the adjacency matrix, and will predict edges.\n",
    "\n",
    "Note that this dataset does not have any node embeddings, the only available information are the existing interactions (the edges). So we will manually create embeddings for every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a0aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_array_to_tensor(adj):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    adj = adj.tocoo()\n",
    "    return torch.sparse_coo_tensor(\n",
    "        torch.LongTensor(np.vstack((adj.row, adj.col))), \n",
    "        torch.FloatTensor(adj.data),\n",
    "        torch.Size(adj.shape)\n",
    "    )\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "data = {\n",
    "    'adj_train': sparse_array_to_tensor(adj_train).to_dense(),  # adjacency matrix of training set\n",
    "    'train_edges': torch.LongTensor(train_edges),               # indices of edges in the training set\n",
    "    'train_mask': torch.BoolTensor(train_mask),                 # boolean mask of edges in the training set\n",
    "    'valid_edges': torch.LongTensor(valid_edges),               # indices of edges in the validation set\n",
    "    'valid_edges_neg': torch.LongTensor(valid_edges_neg),       # indices of negative edges in the validation set\n",
    "    'test_edges': torch.LongTensor(test_edges),                 # indices of edges in the test set\n",
    "    'test_edges_neg': torch.LongTensor(test_edges_neg),         # indices of negative edges in the test set\n",
    "}\n",
    "\n",
    "# Embed the nodes as one-hot vectors\n",
    "D_NODES = N_NODES  # number of dimensions\n",
    "data['x_train'] = torch.eye(N_NODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa41cb-91f7-4d53-be10-cfb58402e44d",
   "metadata": {},
   "source": [
    "## 3. Graph Neural Networks\n",
    "Let's implement here a basic GNN using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cddfdc-9fc4-4be9-b065-fbcf1fe2a506",
   "metadata": {},
   "source": [
    "### 3.1. Graph Convolutional Layers\n",
    "Start by implementing layers to perform graph convolutions with simple neighborhood aggregation. They will be useful to build the Deep Graph Encoder part of the whole GNN model.\n",
    "\n",
    "<img src=\"img/neighb_agg.png\" width=\"600\" />\n",
    "\n",
    "As a reminder, the equation you saw in the course for such a layer is:\n",
    "$$\n",
    "\\mathbf{h}_v^{(l+1)} = \\sigma\\left( \\mathbf{W}_l \\sum_{u\\in N(v)} \\frac{\\mathbf{h}_u^{(l)}}{|N(v)|} + \\mathbf{B}_l \\mathbf{h}_v^{(l)} \\right),\n",
    "$$\n",
    "where $v$ index the node, $l$ the layer, $\\mathbf{h}$ are the node embeddings, $\\sigma$ is a non-linearity, $N(v)$ is the set of neighbor of node $v$, and $\\mathbf{W}$ and $\\mathbf{B}$ are the trainable weight matrices of the layer. Or, in matrix form with the adjacency matrix (corrected by the inverse of the degree) $\\tilde{A}$:\n",
    "$$\n",
    "H^{(l+1)} = \\sigma\\left( \\tilde{A}H^{(l)}W_l^\\top + H^{(l)}B_l^\\top \\right).\n",
    "$$\n",
    "\n",
    "**Note**: because of our naive data splitting and the sparsity of connections in the graph, some node may have no neighbors. For these nodes, be careful about the neighborhood aggregation as their degree is 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61da7b95-37b5-4759-84cc-7ee844df2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    \"\"\"Basic graph convolutional layer implementing the simple neighborhood aggregation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the graph convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            out_features (int): number of output node features.\n",
    "            activation (nn.Module or callable): activation function to apply. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.rand(in_features, out_features) * np.sqrt(2 / in_features))\n",
    "        self.B = nn.Parameter(torch.rand(in_features, out_features) * np.sqrt(2 / in_features))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        # Adjust the adjaceny matrix\n",
    "        deg = x.shape[0]\n",
    "        if deg > 0: \n",
    "            deg_inv =  1 / deg\n",
    "        else:\n",
    "            deg_inv = 1 / 0.000001 # TODO: this could be a potential source of error\n",
    "        \n",
    "        A_adj = (adj * deg_inv)\n",
    "        \n",
    "        # Make sure we also have the same type\n",
    "        A_adj = A_adj.float()\n",
    "        x = x.float()\n",
    "        \n",
    "        # Compute the result\n",
    "        result = A_adj @ x @ self.W + x @ self.B\n",
    "        \n",
    "        # Apply activation if neccessary\n",
    "        if self.activation is not None:\n",
    "            result = self.activation(result)\n",
    "        \n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003c7c8",
   "metadata": {},
   "source": [
    "Let's do a quick test to verify if it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92400d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GraphConv(D_NODES, 5, nn.ReLU())\n",
    "out = conv(data['x_train'], data['adj_train'])\n",
    "\n",
    "assert out.shape == (N_NODES, 5), f\"Output shape should be (num_nodes, out_features), was {out.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652eaba",
   "metadata": {},
   "source": [
    "### 3.2. Prediction head\n",
    "\n",
    "In this exercise, we are interested in predicting potential new protein-protein interactions, meaning we want to do *edge prediction*.\n",
    "\n",
    "<img src=\"img/edge_pred.png\" width=\"300\" />\n",
    "\n",
    "In the lecture, you have seen two possibilities for the prediction heads at edge-level: (1) Concatenation + Linear or (2) Dot product. \n",
    "\n",
    "A potential problem with (1) is that it's not a symmetric operation, as $\\mathrm{Concat}\\left(\\mathbf{h}_v, \\mathbf{h}_u\\right) \\neq \\mathrm{Concat}\\left(\\mathbf{h}_u, \\mathbf{h}_v\\right)$ in the general case. If  we have *directed* edges, this can be beneficial, but here the edges are undirected.\n",
    "\n",
    "In this exercise, as we want to do 1-way prediction (is there an edge or not) we only ask you to implement (2):\n",
    "* `DotProductHead`: dot product between the node embeddings, and we add a linear transformation on the results: $\\hat{\\mathbf{y}}_{uv} = w\\left( \\mathbf{h}_v ^\\top \\mathbf{h}_u\\right) + b$, where $w$ and $b$ are both trainable parameters.\n",
    "\n",
    "**Remark**: to get a probability for each edge, we could use the sigmoid function on the final results: $\\mathrm{sigmoid}(\\hat{\\mathbf{y}}_{uv})$. However, here we will not directly include it in the model. The reason is that during training, we'll then be able to use [`torch.nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html?highlight=bcewithlogits#torch.nn.BCEWithLogitsLoss) instead of [`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=bce#torch.nn.BCELoss), which is more stable numerically.\n",
    "Then, when using the model for prediction, we will manually apply the sigmoid on its output to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf274a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductHead(nn.Module):\n",
    "    \"\"\"Prediction head using dot product between node embeddings for edge prediction.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the prediction head.\"\"\"\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.rand(1))\n",
    "        self.b = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Edge-level prediction given the node embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Edge predictions of shape (num_nodes, num_nodes).\n",
    "        \"\"\"\n",
    "        return (x @ x.T)*self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b424dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a quick test as well.\n",
    "head = DotProductHead()\n",
    "out = head(data['x_train'])\n",
    "\n",
    "assert out.shape == (N_NODES, N_NODES), f\"Output shape should be (num_nodes, num_nodes), was {out.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351c36d-ad5d-493e-a9e1-27c8622c773c",
   "metadata": {},
   "source": [
    "### 3.3. GNN full model\n",
    "Implement the entire GNN model, which consists in an initial Deep Graph Encoder part, to obtain node embeddings, followed by a prediction head.\n",
    "\n",
    "<img src=\"img/gnn_net.png\" width=\"700\" />\n",
    "\n",
    "**Important**: as our prediction head is a simple dot product, it might be better to avoid having an activation function on the previous layer. Indeed, for example, a ReLU would prevent any negative values, so that all the dot product would be strictly positive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29a2ac18-4ddc-4feb-ad83-4caa82b28503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\"A full Graph Neural Network for edge prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, num_features, conv_dims, activation, dropout=0.):\n",
    "        \"\"\"\n",
    "        Initialize the GNN model for edge prediction.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Number of input node features.\n",
    "            conv_dims (list of int): Number of hidden features in each graph convolution layers.\n",
    "            activation (nn.Module or callable): Activation function to apply.\n",
    "            dropout (float): Dropout probability. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "        \n",
    "        # Create the conv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        dims = [num_features] + conv_dims\n",
    "        for i in range(1, len(dims)):\n",
    "            fin, fout = dims[i - 1], dims[i]\n",
    "            # No ReLU (last layer)\n",
    "            if i == (len(dims) - 1): \n",
    "                conv = GraphConv(fin, fout, None)\n",
    "            # With ReLU\n",
    "            else:\n",
    "                conv = GraphConv(fin, fout, activation)\n",
    "            self.convs.append(conv)\n",
    "        \n",
    "        # Setup the prediction head\n",
    "        self.head = DotProductHead()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform forward pass for edge prediction.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, num_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted edge probabilities for each pair of nodes, shape (num_nodes, num_nodes).\n",
    "        \"\"\"\n",
    "        # Run throught the conv. layers to obtain embeddings of nodes\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, adj)\n",
    "        \n",
    "        # Get the prediction\n",
    "        yhat = self.head(x)\n",
    "        return yhat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5274cc",
   "metadata": {},
   "source": [
    "Again, let's quickly verify that our model runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ba7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(num_features=D_NODES, conv_dims=[5, 5], activation=nn.ReLU())\n",
    "out = model(data['x_train'], data['adj_train'])\n",
    "\n",
    "assert out.shape == (N_NODES, N_NODES), f\"Output shape should be (num_nodes, num_nodes), was {out.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffdeb1a-14da-42df-b6a0-f2c77b7fabca",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f30f80-e879-4c22-b101-64caab6104a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1. Utilities\n",
    "Define functions to implement a *single training step*, compute train and test metrics, and visualize the Receiver Operating Characteristic Curbe (ROC curve), see [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html).\n",
    "\n",
    "For the metrics, we use scikit-learn to compute the following:\n",
    "1. `roc_auc_score`: Area Under the Receiver Operating Characteristic Curve (ROC AUC), see [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).\n",
    "2. `average_precision_score`: Average Precision (AP), see [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ef23b7b-e68b-45e2-9d0a-45ca92b7ef51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(data, model, edges_pos, edges_neg, threshold=0.5):\n",
    "    # Predictions\n",
    "    logits = model.forward(data['x_train'], data['adj_train'])\n",
    "\n",
    "    # Predictions (binary)\n",
    "    probs = F.sigmoid(logits)\n",
    "    preds = (probs > threshold).numpy().astype(int)\n",
    "\n",
    "    # Find out predictions for positive and negative edges\n",
    "    n_pos = edges_pos.shape[1]\n",
    "    y_pos = np.ones(n_pos).astype(int)\n",
    "    rows_indices, cols_indices = edges_pos\n",
    "    pred_pos = preds[rows_indices, cols_indices]\n",
    "\n",
    "    n_neg = edges_neg.shape[1]\n",
    "    y_neg = np.zeros(n_neg).astype(int)\n",
    "    rows_indices, cols_indices = edges_neg\n",
    "    pred_neg = preds[rows_indices, cols_indices]\n",
    "\n",
    "    # Stack all the results\n",
    "    y = np.concatenate([y_pos, y_neg])\n",
    "    pred = np.concatenate([pred_pos, pred_neg])\n",
    "    \n",
    "    return y, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ebc503-9971-4afe-b350-2853f78e4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(data, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model for one step on the network data.\n",
    "\n",
    "    Hint: do not forget to mask the loss!\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Dictionary of tensors containing the data.\n",
    "        model (nn.Module): Model to be trained.\n",
    "        loss_fn (nn.Module): Loss function to be optimized.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
    "\n",
    "    Returns:\n",
    "        float: Loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clearing gradients of optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Make a prediction\n",
    "    logits = model.forward(data['x_train'], data['adj_train'])\n",
    "    \n",
    "    # Compute the loss\n",
    "    train_mask = data['train_mask']\n",
    "    y_masked = data['adj_train'][train_mask]\n",
    "    preds_masked = logits[train_mask]\n",
    "    loss_value = loss_fn(preds_masked, y_masked)\n",
    "\n",
    "    # Computing the gradients.\n",
    "    loss_value.backward()\n",
    "\n",
    "    # Optimizing the network parameters.\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return the loss\n",
    "    return loss_value.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()  # decorator to deactivate autograd functionality in this function\n",
    "def get_train_accuracy(data, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute accuracy of the edge prediction on the training data.\n",
    "\n",
    "    Hint: do not forget to mask the train edges.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary of tensors containing the data.\n",
    "        model (nn.Module): Model to be tested.\n",
    "\n",
    "    Returns:\n",
    "        tuple(float, float): Accuracy for positive edges, accuracy for negative edges (in %).\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get the train mask\n",
    "    train_mask = data['train_mask']\n",
    "\n",
    "    # Make a prediction\n",
    "    logits = model.forward(data['x_train'], data['adj_train'])\n",
    "     \n",
    "    # Predictions (binary)\n",
    "    probs = F.sigmoid(logits)\n",
    "    pred = (probs > threshold).numpy().astype(int)[train_mask]\n",
    "    \n",
    "    # Get y\n",
    "    # row_indices, column_indices = data['train_edges']\n",
    "    y = data['adj_train'][train_mask].numpy().astype(int)\n",
    "    \n",
    "    # Get accuracy\n",
    "    # - Pos\n",
    "    pos_edges_mask = y == 1\n",
    "    pred_pos = pred[pos_edges_mask]\n",
    "    y_pos = y[pos_edges_mask]\n",
    "    acc_pos = np.mean(pred_pos == y_pos)\n",
    "    \n",
    "    # - Neg\n",
    "    neg_edges_mask = y == 0\n",
    "    pred_neg = pred[neg_edges_mask]\n",
    "    y_neg = y[neg_edges_mask]\n",
    "    acc_neg = np.mean(pred_neg == y_neg)\n",
    "\n",
    "    # Convert to %\n",
    "    acc_pos *= 100 \n",
    "    acc_neg *= 100\n",
    "\n",
    "    return (acc_pos, acc_neg)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data, model, edges_pos, edges_neg):\n",
    "    \"\"\"\n",
    "    Test the model's prediction on the positive and negative edges.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary of tensors containing the data.\n",
    "        model (nn.Module): Model to be tested.\n",
    "        edges_pos (Tensor): Positive edges to be predicted.\n",
    "        edges_neg (Tensor): Negative edges to be predicted.\n",
    "\n",
    "    Returns:\n",
    "        tuple(float, float): Area under ROC curve, and average precision metrics.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get the ground truth and predictions\n",
    "    y, pred = predict(data, model, edges_pos, edges_neg)\n",
    "    print(np.unique(pred, return_counts=True))\n",
    "    \n",
    "    # Compute the metrics\n",
    "    roc_score = roc_auc_score(y, pred)\n",
    "    ap_score = average_precision_score(y, pred)\n",
    "    return roc_score, ap_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_roc_curve(data, model, edges_pos, edges_neg):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve given the positive and negative edges.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary of tensors containing the data.\n",
    "        model (nn.Module): Model to be tested.\n",
    "        edges_pos (Tensor): Positive edges to be predicted.\n",
    "        edges_neg (Tensor): Negative edges to be predicted.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The ROC curve figure.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get the ground truth and predictions\n",
    "    y, pred = predict(data, model, edges_pos, edges_neg)\n",
    "\n",
    "    # Compute the false positive rate and true positive rate\n",
    "    fpr, tpr, _ = roc_curve(y, pred)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(fpr, tpr, label='ROC curve\\n(with threshold values)')\n",
    "    ax.set_xlim(0., 1.)\n",
    "    ax.set_ylim(0., 1.)\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('ROC curve')\n",
    "    ax.grid()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a30ccb-3256-46de-a542-311b1ce9fd20",
   "metadata": {},
   "source": [
    "### 4.2. Complete training\n",
    "We will now combine everything to train and evaluate our model.\n",
    "\n",
    "The main steps are:\n",
    "1. Prepare and preprocess the data (already done above)\n",
    "2. Define the model\n",
    "3. Define the training loss and optimizer\n",
    "4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e02444-86a3-4569-bd38-ec85e9870435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters (feel free to change these)\n",
    "learning_rate = 1e-2\n",
    "epochs = 400\n",
    "conv_dims = [256, 128, 64]\n",
    "activation = nn.LeakyReLU()\n",
    "dropout = 0.05\n",
    "\n",
    "# 1. Data\n",
    "# We already loaded and prepared the data above\n",
    "\n",
    "# 2. Model\n",
    "model = GNN(num_features=D_NODES, conv_dims=conv_dims, activation=activation, dropout=dropout)\n",
    "#print(model)\n",
    "\n",
    "# 3. Initialize the loss function and optimizer\n",
    "# We weight the samples to compensate for the class imbalance (there are very few edges compared to non-edges)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "history = {\"epoch\": 0, \"loss\": [], \"acc-pos\": [], \"acc-neg\": [], \"val-roc\": [], \"val-ap\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ee38ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/400: loss: 211.7282 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch   2/400: loss: 33.4684 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch   3/400: loss: 4.9410 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch   4/400: loss: 1.4570 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch   5/400: loss: 1.3538 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch   6/400: loss: 1.3521 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch   7/400: loss: 1.3519 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch   8/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch   9/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  10/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  11/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  12/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  13/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  14/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  15/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  16/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  17/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  18/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  19/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  20/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  21/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  22/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  23/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  24/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  25/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  26/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  27/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  28/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  29/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  30/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  31/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  32/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  33/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  34/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  35/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  36/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  37/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  38/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  39/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  40/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  41/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  42/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  43/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  44/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  45/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  46/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  47/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  48/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  49/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  50/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  51/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  52/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  53/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  54/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  55/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  56/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  57/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  58/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  59/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  60/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  61/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  62/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  63/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  64/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  65/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  66/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  67/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  68/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  69/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  70/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  71/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  72/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  73/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  74/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  75/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  76/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  77/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  78/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  79/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  80/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  81/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  82/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  83/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  84/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  85/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  86/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  87/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  88/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  89/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  90/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  91/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  92/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  93/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  94/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  95/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  96/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  97/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  98/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  99/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  100/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  101/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  102/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  103/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  104/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  105/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  106/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  107/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  108/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  109/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  110/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  111/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  112/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  113/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  114/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  115/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  116/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  117/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  118/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  119/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  120/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  121/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  122/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  123/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  124/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  125/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  126/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  127/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  128/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  129/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  130/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  131/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  132/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  133/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  134/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  135/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  136/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  137/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  138/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  139/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  140/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  141/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  142/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  143/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  144/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  145/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  146/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  147/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  148/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  149/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  150/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  151/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  152/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  153/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  154/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  155/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  156/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  157/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  158/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  159/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  160/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  161/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  162/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  163/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  164/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  165/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  166/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  167/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  168/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  169/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  170/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  171/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  172/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  173/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  174/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  175/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  176/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  177/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  178/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  179/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  180/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  181/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  182/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  183/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  184/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  185/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  186/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  187/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  188/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  189/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  190/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  191/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  192/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  193/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  194/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  195/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  196/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  197/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  198/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  199/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  200/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  201/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  202/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  203/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  204/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  205/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  206/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  207/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  208/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  209/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  210/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  211/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  212/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  213/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  214/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  215/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  216/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  217/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  218/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  219/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  220/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  221/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  222/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  223/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  224/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  225/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  226/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  227/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  228/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.2s/epoch)\n",
      "Epoch  229/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  230/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  231/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  232/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  233/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  234/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  235/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  236/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  237/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  238/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  239/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  240/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  241/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  242/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  243/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  244/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  245/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  246/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  247/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  248/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  249/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  250/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  251/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  252/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  253/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  254/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  255/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  256/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  257/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  258/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  259/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  260/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  261/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  262/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  263/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  264/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  265/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  266/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  267/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  268/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  269/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  270/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  271/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  272/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  273/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  274/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  275/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  276/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  277/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  278/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  279/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  280/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  281/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  282/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  283/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  284/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  285/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  286/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  287/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  288/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  289/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  290/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  291/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  292/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  293/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  294/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  295/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  296/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  297/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  298/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  299/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  300/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  301/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  302/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  303/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  304/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  305/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  306/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  307/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  308/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  309/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  310/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  311/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  312/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  313/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  314/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  315/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  316/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  317/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  318/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  319/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  320/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  321/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  322/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  323/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  324/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  325/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  326/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  327/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  328/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  329/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  330/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  331/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  332/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  333/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  334/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  335/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  336/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  337/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  338/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  339/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  340/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  341/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  342/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  343/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  344/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  345/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  346/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  347/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  348/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  349/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  350/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  351/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  352/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  353/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  354/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  355/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  356/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  357/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  358/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  359/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  360/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  361/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  362/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  363/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  364/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  365/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  366/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  367/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  368/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  369/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  370/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  371/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  372/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  373/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  374/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  375/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  376/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  377/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  378/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  379/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  380/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  381/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  382/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  383/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  384/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  385/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  386/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  387/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  388/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  389/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  390/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  391/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  392/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  393/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  394/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  395/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  396/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  397/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  398/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  399/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Epoch  400/400: loss: 1.3518 - acc-pos: 100.0% - acc-neg: 0.0%(array([1]), array([1290]))\n",
      " - val-roc: 0.5000 - val-ap: 0.5000 (0.1s/epoch)\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABRIAAAF0CAYAAABfQ8XmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB2TUlEQVR4nO3de3zO9f/H8edldmSb8w4MUzOEMCVDCNMcSiqKpFAktK/8RIqRthJaEcVXDjl+v1+H9E1szoRiiCKpMLE1xOa4sX1+f/juymWbnTfb53G/3a5brs/n/fl8Xu/t2qvrel3v9+dtMQzDEAAAAAAAAADcQamiDgAAAAAAAADA3Y9CIgAAAAAAAIAsUUgEAAAAAAAAkCUKiQAAAAAAAACyRCERAAAAAAAAQJYoJAIAAAAAAADIEoVEAAAAAAAAAFmikAgAAAAAAAAgSxQSAQAAAAAAAGSJQiJksViy9di8eXOerhMaGiqLxZKrYzdv3pwvMRS3awPIGHkLgBk88cQTcnZ21oULFzJt07t3b9nb2+vPP//M9nktFotCQ0Otz3OSr1544QXVrFkz29e61YwZMzRv3rx0248fPy6LxZLhvoKWlufTHvb29qpevbpeeuklxcXFZXjM5cuX9d5776lx48YqW7asypQpo0aNGiksLEyXL1/O8JikpCRNnz5dLVu2VPny5eXg4KCqVauqR48e2rJlS45iHj58uCwWi7p06ZLh/rTf53/+858M9w8ZMiTD/7flZ4wo/sg/hSurv2szq1mzpk2eLlu2rJo1a6YFCxYUeiy5fb3k5bV7Nypd1AGg6O3cudPm+TvvvKNNmzZp48aNNtvr1auXp+sMGDBAjz76aK6ObdKkiXbu3JnnGACUDOQtAGbQv39/rVq1SosXL9bgwYPT7U9ISNDKlSvVpUsXeXh45Po6hZWvZsyYoUqVKumFF16w2e7l5aWdO3fqnnvuKdDr38natWvl7u6uS5cuKTIyUlOmTNGOHTu0f/9+2dvbW9v9+eefat++vX777TcNGzZMkyZNkiRt3LhREydO1JIlS7R+/Xqb38fZs2f16KOP6sCBA+rXr5/+7//+TxUqVNCpU6f05Zdfql27doqOjtb999+fZZzXr1/XwoULrTGfOnVKVatWzXP/8zNGlAzkn8JTUH/XJUmLFi00efJkSdIff/yhyZMnq2/fvrp8+bJeeeWVQosjt6+Xt99+W6+99loBRVX4KCRCDz30kM3zypUrq1SpUum23+7KlStycXHJ9nWqVaumatWq5SpGNze3LOMBYB7krZLl6tWrcnJyyvXoT6CkCg4Olre3tz7//PMMP8gvWbJEV69eVf/+/fN0naLOV46OjkWeLwMCAlSpUiVJUvv27XX27FnNnTtX27dvV9u2ba3tnn/+ef3888/atGmTWrZsad3eoUMHde7cWW3btlXfvn21du1am2N++OEHrVu3To888ojNdZ955hkNHz5c5cuXz1acX375pc6cOaPOnTvr66+/1vz58/Xmm2/mpev5HiNKBvJP4Smov+ucSElJ0Y0bN+To6Fio182ucuXK2fye2rdvrxo1amjq1KmZFhILok+5fb0UZaG6IDC1GdnSpk0b1a9fX1u3blVgYKBcXFzUr18/SdKyZcsUFBQkLy8vOTs7q27duho1alS6qR0ZTRGsWbOmunTporVr16pJkyZydnZWnTp19Pnnn9u0y2jI+wsvvKCyZcvq119/VadOnVS2bFn5+Pjo9ddfV1JSks3xf/zxh5566im5urqqXLly6t27t3bv3p2nYeyrV69W8+bN5eLiIldXV3Xo0CHdKKkzZ87o5Zdflo+PjxwdHVW5cmW1aNFC69evt7bZt2+funTpoipVqsjR0VHe3t7q3Lmz/vjjj1zFBeAmM+etM2fOaPDgwapXr57Kli2rKlWq6JFHHtG2bdvStU1KStKECRNUt25dOTk5qWLFimrbtq127NhhbZOamqpp06apUaNGcnZ2tr6ZW716tbXN7VOVbv153frt/7x582SxWBQZGal+/fqpcuXKcnFxUVJSkn799Ve9+OKL8vPzk4uLi6pWraquXbvq4MGD6c574cIFvf7666pVq5YcHR1VpUoVderUST///LMMw5Cfn586duyY7rhLly7J3d1dr7766h1/hsDdwM7OTn379lV0dHSGfwdz586Vl5eXgoODc/R3f7vMphbOmzdP/v7+cnR0VN26dTOdRjZ+/Hg1a9ZMFSpUkJubm5o0aaI5c+bIMAxrm5o1a+qnn37Sli1brNPT0qZ5ZTZVbPv27WrXrp1cXV3l4uKiwMBAff311+litFgs2rRpk1555RVVqlRJFStWVPfu3XX69Oks+56Zpk2bSpLNlM09e/YoMjJS/fv3tykipmnZsqX69eundevWKTo6WpIUHR2tb775Rv37909XoEvzwAMPqHr16tmKa86cOXJwcNDcuXPl4+OjuXPn2vyccyO/Y0TJQP4pvPxzp7/rM2fOyMHBQW+//Xa6437++WdZLBZ9/PHH1m1xcXEaOHCgqlWrJgcHB/n6+mr8+PG6ceOGtU1anydNmqSJEyfK19dXjo6O2rRpk65du6bXX39djRo1kru7uypUqKDmzZvryy+/THf9CxcuqH///qpQoYLKli2rzp076/fff8/wPeHRo0fVq1cv62feunXr6pNPPsn2z+h25cqVk7+/v06cOJFln6Sb+fuxxx5ThQoV5OTkpMaNG+tf//pXuvOeOnXK+vndwcFB3t7eeuqpp6z/L8jo9ZKdz/wZTW2+du2aRo8eLV9fX+utJF599dV0txPI7mePwsSIRGRbbGysnnvuOY0cOVJhYWEqVepmHfro0aPq1KmTQkJCVKZMGf388896//339f3336ebZpiRH374Qa+//rpGjRolDw8P/fOf/1T//v1177336uGHH77jsdevX9djjz2m/v376/XXX9fWrVv1zjvvyN3dXWPHjpV08z42bdu21V9//aX3339f9957r9auXauePXvm+mexePFi9e7dW0FBQVqyZImSkpI0adIktWnTRhs2bLC+uezTp4/27t2rd999V7Vr19aFCxe0d+9enTt3zhpbhw4d5Ovrq08++UQeHh6Ki4vTpk2bdPHixVzHB+Ams+atv/76S5I0btw4eXp66tKlS1q5cqU1R7Vp00aSdOPGDQUHB2vbtm0KCQnRI488ohs3bmjXrl2KiYlRYGCgpJtvfhYuXKj+/ftrwoQJcnBw0N69e3X8+PFsxZORfv36qXPnzvriiy90+fJl2dvb6/Tp06pYsaLee+89Va5cWX/99Zfmz5+vZs2aad++ffL395ckXbx4US1bttTx48f1xhtvqFmzZrp06ZK2bt2q2NhY1alTR0OHDlVISIiOHj0qPz8/63UXLFigxMRECokoNvr166f33ntPn3/+uT788EPr9kOHDun777/XqFGjZGdnl+2/++yaN2+eXnzxRT3++OOaMmWKEhISFBoaqqSkJGsuTXP8+HENHDjQWmjatWuXhg4dqlOnTlnz2sqVK/XUU0/J3d1dM2bMkKQ7jhLZsmWLOnTooIYNG2rOnDlydHTUjBkz1LVrVy1ZsiRdPhwwYIA6d+6sxYsX6+TJk/q///s/Pffcc9nK6Rk5duyYJKl27drWbVFRUZKkbt26ZXpct27dNGvWLEVFRSkgIECRkZFZHpNdf/zxhyIjI/Xkk0+qcuXK6tu3ryZOnKitW7eqdevWuT5vfsaIkoX8U/D5J6u/68qVK6tLly6aP3++xo8fb9P/uXPnysHBQb1795Z0s4j44IMPqlSpUho7dqzuuece7dy5UxMnTtTx48c1d+5cm2t//PHHql27tiZPniw3Nzf5+fkpKSlJf/31l0aMGKGqVasqOTlZ69evV/fu3TV37lw9//zzkm5+ydy1a1ft2bNHoaGh1inqGd0S6NChQwoMDFT16tU1ZcoUeXp6at26dRo2bJjOnj2rcePGZflzut3169d14sQJVa5cOcs+bdq0SY8++qiaNWumTz/9VO7u7lq6dKl69uypK1euWL/wPnXqlB544AFdv35db775pho2bKhz585p3bp1On/+fKZT+LP6zJ8RwzDUrVs3bdiwQaNHj1arVq104MABjRs3Tjt37tTOnTttXqN5+exRIAzgNn379jXKlCljs61169aGJGPDhg13PDY1NdW4fv26sWXLFkOS8cMPP1j3jRs3zrj9JVejRg3DycnJOHHihHXb1atXjQoVKhgDBw60btu0aZMhydi0aZNNnJKMf/3rXzbn7NSpk+Hv7299/sknnxiSjG+++cam3cCBAw1Jxty5c+/Yp9uvnZKSYnh7exsNGjQwUlJSrO0uXrxoVKlSxQgMDLRuK1u2rBESEpLpuffs2WNIMlatWnXHGADcGXnrzm7cuGFcv37daNeunfHEE09Yty9YsMCQZMyePTvTY7du3WpIMsaMGXPHa0gyxo0bl257jRo1jL59+1qfz50715BkPP/889mKOzk52fDz8zP+8Y9/WLdPmDDBkGRERUVlemxiYqLh6upqvPbaazbb69WrZ7Rt2zbLawN3k9atWxuVKlUykpOTrdtef/11Q5Lxyy+/ZHhMZn/3hpH+7zWz9zpNmjQxUlNTre2OHz9u2NvbGzVq1Mg01pSUFOP69evGhAkTjIoVK9ocf9999xmtW7dOd8yxY8fS5baHHnrIqFKlinHx4kWbPtWvX9+oVq2a9bxpOWXw4ME255w0aZIhyYiNjc00VsP4O8/HxcUZ169fN86fP2/861//MsqUKWM8++yzNm0HDRpkSDJ+/vnnTM93+PBhQ5LxyiuvZPuY7ErLfWvXrjUMwzB+//13w2KxGH369LFpl/b7/Pe//53heV599VWb/7flZ4woecg/f/cpv/OPYWTv73r16tWGJCMyMtImHm9vb+PJJ5+0bhs4cKBRtmxZm/eohmEYkydPNiQZP/30k02f77nnHpvfa0bSfpf9+/c3GjdubN3+9ddfG5KMmTNn2rQPDw9P9zvu2LGjUa1aNSMhIcGm7ZAhQwwnJyfjr7/+umMMNWrUMDp16mRcv37duH79unHs2DHre+r/+7//y7JPderUMRo3bmxcv37dZnuXLl0MLy8v62f6fv36Gfb29sahQ4cyjSWj10tWn/kN4+ZngFtfu2vXrjUkGZMmTbJpt2zZMkOSMWvWLJv+Z+ezR2FiajOyrXz58hlOd/j999/Vq1cveXp6ys7OTvb29tZvRQ8fPpzleRs1amQzVcLJyUm1a9e2DlO+E4vFoq5du9psa9iwoc2xW7Zskaura7pvR5599tksz5+RI0eO6PTp0+rTp4/NN0Jly5bVk08+qV27dunKlSuSpAcffFDz5s3TxIkTtWvXLl2/ft3mXPfee6/Kly+vN954Q59++qkOHTqUq5gAZMzMeevTTz9VkyZN5OTkpNKlS8ve3l4bNmyw6d8333wjJycn65TvjHzzzTeSlO8j+J588sl0227cuKGwsDDVq1dPDg4OKl26tBwcHHT06NF0cdeuXVvt27fP9Pyurq568cUXNW/ePOuU9Y0bN+rQoUMaMmRIvvYFKGj9+/fX2bNnrbcTuHHjhhYuXKhWrVrZjLjNzt99dqS91+nVq5fN7R1q1KhhHal8q40bN6p9+/Zyd3e35tSxY8fq3Llzio+Pz3F/L1++rO+++05PPfWUypYta91uZ2enPn366I8//tCRI0dsjnnsscdsnjds2FCSspWXJcnT01P29vYqX768evTooYCAAM2fPz/HsRv/m46Y23u+3rhxw+aRdj7DMKzTHjt06CBJ8vX1VZs2bbR8+XIlJibm6npAVsg/NxVE/snu33VwcLA8PT1tRhSuW7dOp0+ftnkP99///ldt27aVt7e3TR4JDg6WpHSrrz/22GM2i0ml+fe//60WLVqobNmy1t/lnDlzbH6Xaefq0aOHzbG3v1e9du2aNmzYoCeeeEIuLi42cXXq1EnXrl3Trl277vhzkqQ1a9bI3t5e9vb28vX11b/+9S8NHTpUEydOvGOffv31V/3888/WUZu3Xz82Ntb6+/zmm2/Utm1b1a1bN8t4bpXVZ/6MpI1WvX3xn6efflplypTRhg0bbLbn5bNHQaCQiGzz8vJKt+3SpUtq1aqVvvvuO02cOFGbN2/W7t27tWLFCkk3b6CflYoVK6bb5ujomK1jXVxc5OTklO7Ya9euWZ+fO3cuw2HIuV1dLG2IckY/D29vb6Wmpur8+fOSbt6HrW/fvvrnP/+p5s2bq0KFCnr++ecVFxcnSXJ3d9eWLVvUqFEjvfnmm7rvvvvk7e2tcePGZSsBAbgzs+attBtPN2vWTMuXL9euXbu0e/duPfroozYxnjlzRt7e3ummCd3qzJkzsrOzk6enZ7aunV0Z/W6GDx+ut99+W926ddNXX32l7777Trt379b999+fLu7sLIIzdOhQXbx4UYsWLZIkTZ8+XdWqVdPjjz+efx0BCkHalLy0D5Fr1qzRn3/+abPIQXb/7rMj7b1ORn/3t2/7/vvvFRQUJEmaPXu2vv32W+3evVtjxoyRlL2cervz58/LMIxM32vdGmOa2/Ny2pSw7F5//fr12r17t9atW6cnn3xSW7du1dChQ23apH2IS5v2nJG0Wz74+Phk+5hbpX1QTnukFTM3btyoY8eO6emnn1ZiYqIuXLigCxcuqEePHrpy5YqWLFliPUfp0jfvXpWSkpLhNW7cuGFtk5sYYS7kn7/ld/7J7t916dKl1adPH61cudJ6/7x58+bJy8vL5n7Qf/75p7766qt0eeS+++6TdHN19ltl1McVK1aoR48eqlq1qhYuXKidO3dq9+7d6tevX7r3qqVLl1aFChVsjr/9veq5c+d048YNTZs2LV1cnTp1yjCujLRs2VK7d+/Wnj17dOjQIV24cEEff/yxHBwc7tintHsbjhgxIt310xYRSrt+dt9f3i6rz/wZSfv53T4122KxyNPTM8vXmJT9zx4FgXskItsy+mZ148aNOn36tDZv3mxzb5bbbxBalCpWrKjvv/8+3fY7/WFndT7p5r3Xbnf69GmVKlXKuqpdpUqVFBERoYiICMXExGj16tUaNWqU4uPjrav5NWjQQEuXLpVhGDpw4IDmzZunCRMmyNnZWaNGjcpVjABuMmveWrhwodq0aaOZM2fabL/93quVK1fW9u3blZqammkxsXLlykpJSVFcXFyGbzjTODo6plswRkr/ZjtNRr+bhQsX6vnnn1dYWJjN9rNnz6pcuXI2MWVnQap7771XwcHB+uSTTxQcHKzVq1dr/PjxsrOzy/JY4G7i7OysZ599VrNnz1ZsbKw+//xzubq66umnn7a2ye7ffXakvdfJKOfcvm3p0qWyt7fXf//7X5svSVatWpXj66YpX768SpUqlel7LUnWFZbzy/333289Z4cOHdSxY0fNmjVL/fv31wMPPGDd/uabb2rVqlUZ3gdM+rvfaaOLOnbsmOUxt9q9e7fNc19fX0k3F2OQbhZspk6dmu64OXPmaODAgZL+/iB/6tSpDK9x6tQpmw/7OY0R5kL++Vt+55+c/F2/+OKL+uCDD6z39lu9erVCQkJs3tNUqlRJDRs21Lvvvpvh9dIKoWkyey/m6+urZcuW2ey//T1exYoVdePGDf311182xcTbf0fly5e3jubMbHZLWp67E3d3d+siWHdye5/SflejR49W9+7dMzwm7R7c2X1/ebvsfOa/XdrP78yZMzbFRMMwFBcXZ/3/zt2KEYnIk7Q/1NtvVvvZZ58VRTgZat26tS5evGidnpdm6dKluTqfv7+/qlatqsWLF9usBnb58mUtX77cupLz7apXr64hQ4aoQ4cO2rt3b7r9FotF999/vz788EOVK1cuwzYA8s4MectisaTr34EDB9KtLB8cHKxr167dcRXotOkwt384uF3NmjV14MABm20bN27UpUuXshVzZnF//fXX6T4MBwcH65dffsnWTcxfe+01HThwQH379pWdnZ1eeumlbMcD3E369++vlJQUffDBB1qzZo2eeeYZm/cb2f27zw5/f395eXlpyZIlNu91Tpw4YbOie9p1S5cubfNh9urVq/riiy/SnTe7oyfKlCmjZs2aacWKFTbtU1NTtXDhQlWrVs1mEZT8ZrFY9Mknn8jOzk5vvfWWdXvTpk0VFBSkOXPm6Ntvv0133Pbt2/X555/r0UcfVUBAgCSpSZMmCg4O1pw5czLNWXv27FFMTIz1Grc+KlasqPPnz2vlypVq0aKFNm3alO7Ru3dv7d69Wz/++KMkyc/PTzVq1NC///3vdCs6nzlzRps2bbK5NUROY4T5kH/yP//k9O+6bt26atasmebOnavFixcrKSlJL774os05u3Tpoh9//FH33HNPulzStGnTdIXEjFgsFjk4ONgU5OLi4tKt2pz2ZfyyZctstt/+XtXFxUVt27bVvn371LBhwwzjymi0XX7x9/eXn5+ffvjhhwyv3bRpU7m6ukq6+f5y06ZN6aau50RWn/nTtGvXTtLNwu2tli9frsuXL1v3360YkYg8CQwMVPny5TVo0CCNGzdO9vb2WrRokX744YeiDs2qb9+++vDDD/Xcc89p4sSJuvfee/XNN99o3bp1knTHKX0ZKVWqlCZNmqTevXurS5cuGjhwoJKSkvTBBx/owoULeu+99yRJCQkJatu2rXr16qU6derI1dVVu3fv1tq1a63fhvz3v//VjBkz1K1bN9WqVUuGYWjFihW6cOGC9ZtsAPnLDHmrS5cueueddzRu3Di1bt1aR44c0YQJE+Tr66sbN25Y2z377LOaO3euBg0apCNHjqht27ZKTU3Vd999p7p16+qZZ55Rq1at1KdPH02cOFF//vmnunTpIkdHR+3bt08uLi7WqX99+vTR22+/rbFjx6p169Y6dOiQpk+fLnd392z3u0uXLpo3b57q1Kmjhg0bKjo6Wh988EG6aSYhISFatmyZHn/8cY0aNUoPPvigrl69qi1btqhLly5q27attW2HDh1Ur149bdq0Sc8995yqVKmS7XiAu0nTpk3VsGFDRUREyDAMm2mFUvb/7rOjVKlSeueddzRgwAA98cQTeumll3ThwgWFhoamm1rYuXNnTZ06Vb169dLLL7+sc+fOafLkyRmuiJo2C2PZsmWqVauWnJyc1KBBgwxjCA8PV4cOHdS2bVuNGDFCDg4OmjFjhn788UctWbIk1/cgzC4/Pz+9/PLLmjFjhrZv366WLVtKurnye/v27RUUFKRhw4ZZP+xt3LhRH330kerUqZPuy5kFCxbo0UcfVXBwsPr166fg4GCVL19esbGx+uqrr7RkyRJFR0fb3P/qVosWLdK1a9c0bNiwDFe/rVixohYtWqQ5c+ZYV9adPHmyevTooXbt2umll16Sp6enjh49qvfee08ODg56++238zVGlGzkn/zPP7n5u+7Xr58GDhyo06dPKzAw0DqSLs2ECRMUFRWlwMBADRs2TP7+/rp27ZqOHz+uNWvW6NNPP81y6m6XLl20YsUKDR48WE899ZROnjypd955R15eXjp69Ki13aOPPqoWLVro9ddfV2JiogICArRz504tWLBAku171Y8++kgtW7ZUq1at9Morr6hmzZq6ePGifv31V3311VfZ+mI4Lz777DMFBwerY8eOeuGFF1S1alX99ddfOnz4sPbu3at///vfkm7+/L755hs9/PDDevPNN9WgQQNduHBBa9eu1fDhw1WnTp10587OZ/6MpI18f+ONN5SYmKgWLVpYV21u3Lix+vTpU2A/j3xRBAu84C6X2eqn9913X4btd+zYYTRv3txwcXExKleubAwYMMDYu3dvutWMMlv9tHPnzunO2bp1a5tVtTJb/fT2ODO7TkxMjNG9e3ejbNmyhqurq/Hkk08aa9asMSQZX375ZWY/ikyvbRiGsWrVKqNZs2aGk5OTUaZMGaNdu3bGt99+a91/7do1Y9CgQUbDhg0NNzc3w9nZ2fD39zfGjRtnXL582TAMw/j555+NZ5991rjnnnsMZ2dnw93d3XjwwQeNefPm3TEmALbIW7aSkpKMESNGGFWrVjWcnJyMJk2aGKtWrUq3Ypxh3Fz1bezYsYafn5/h4OBgVKxY0XjkkUeMHTt2WNukpKQYH374oVG/fn3DwcHBcHd3N5o3b2589dVXNtccOXKk4ePjYzg7OxutW7c29u/fn+mqzbt3704X9/nz543+/fsbVapUMVxcXIyWLVsa27ZtS/ezTWv72muvGdWrVzfs7e2NKlWqGJ07d85w1dHQ0FBDkrFr1647/tyAu91HH31kSDLq1auXbl9O/u6Vxaqpaf75z39ac0Pt2rWNzz//PMPzff7554a/v7/h6Oho1KpVywgPDzfmzJljSDKOHTtmbXf8+HEjKCjIcHV1NSRZz5PRKpiGYRjbtm0zHnnkEaNMmTKGs7Oz8dBDD9nkHcPIPKdk1qfbpeXfM2fOpNv3559/GmXLlk230vulS5eMsLAwo1GjRoaLi4vh4uJiNGzY0Jg4caJx6dKlDK9z9epV4+OPPzaaN29uuLm5GaVLlza8vb2N7t27G19//fUdY2zUqJFRpUoVIykpKdM2Dz30kFGpUiWbNuvXrzeCgoKMcuXKGaVLlza8vLyM5557zjh69Gi+x4iSj/yTv/knN3/XCQkJhrOzsyHJmD17dobHnDlzxhg2bJjh6+tr2NvbGxUqVDACAgKMMWPGWPNTWp8/+OCDDM/x3nvvGTVr1jQcHR2NunXrGrNnz87wvepff/1lvPjii0a5cuUMFxcXo0OHDsauXbsMScZHH31k0/bYsWNGv379jKpVqxr29vZG5cqVjcDAQGPixImZ9j9NZu+9bz//nfr0ww8/GD169DCqVKli2NvbG56ensYjjzxifPrppzbtTp48afTr18/w9PQ07O3tDW9vb6NHjx7Gn3/+aXOdtNdLdj7zG0b6VZsN42bOfeONN4waNWoY9vb2hpeXl/HKK68Y58+fz1b/M3p/XFgshnHbeHfAJMLCwvTWW28pJiYmVzdVBYDCRt7KnaZNm8pisaS79xgAAADyz+LFi9W7d299++23Ga6yjZKBqc0whenTp0uS6tSpo+vXr2vjxo36+OOP9dxzz/FhHMBdibyVN4mJifrxxx/13//+V9HR0Vq5cmVRhwQAAFBiLFmyRKdOnVKDBg1UqlQp7dq1Sx988IEefvhhioglHIVEmIKLi4s+/PBDHT9+XElJSapevbreeOMNmxtoA8DdhLyVN3v37lXbtm1VsWJFjRs3Tt26dSvqkAAAAEoMV1dXLV26VBMnTtTly5fl5eWlF154QRMnTizq0FDAmNoMAAAAAAAAIEs5W64WAAAAAAAAgClRSAQAAAAAAACQJQqJAAAAAAAAALJULBdbSU1N1enTp+Xq6iqLxVLU4QAoBgzD0MWLF+Xt7a1SpUrOdyjkQwA5QS4EgJvIhwCQu1xYLAuJp0+flo+PT1GHAaAYOnnypKpVq1bUYeQb8iGA3CAXAsBN5EMAyFkuLJaFRFdXV0k3O+rm5lbE0QAoDhITE+Xj42PNHyUF+RBATpALAeAm8iEA5C4XFstCYtoQbTc3N5IjgBwpaVM8yIcAcoNcCAA3kQ8BIGe5sOTcDAIAAAAAAABAgaGQCAAAAAAAACBLFBIBAAAAAAAAZIlCIgAAAAAAAIAsUUgEAAAAAAAAkCUKiQAAAAAAAACyRCERAAAAAAAAQJYoJAJALmzdulVdu3aVt7e3LBaLVq1aZbPfMAyFhobK29tbzs7OatOmjX766SebNklJSRo6dKgqVaqkMmXK6LHHHtMff/xRiL0AAAAAACD7KCQCQC5cvnxZ999/v6ZPn57h/kmTJmnq1KmaPn26du/eLU9PT3Xo0EEXL160tgkJCdHKlSu1dOlSbd++XZcuXVKXLl2UkpJSWN0AAAAAACDbShd1AABQHAUHBys4ODjDfYZhKCIiQmPGjFH37t0lSfPnz5eHh4cWL16sgQMHKiEhQXPmzNEXX3yh9u3bS5IWLlwoHx8frV+/Xh07dsz3mA3D0NXrFCmBksjZ3k4Wi6WowwAAAEAJZ4pC4uofTmvBjuNq5VdZr7X3K+pwAJRwx44dU1xcnIKCgqzbHB0d1bp1a+3YsUMDBw5UdHS0rl+/btPG29tb9evX144dOzItJCYlJSkpKcn6PDExMdtxXb2eonpj1+WiRwDudocmdJSLgyne1gEAAKAImWJqc3ziNe05cV6/n71U1KEAMIG4uDhJkoeHh812Dw8P6764uDg5ODiofPnymbbJSHh4uNzd3a0PHx+ffI4eAAAAAICM8dU1ABSQ26cZGoaR5dTDrNqMHj1aw4cPtz5PTEzMdjHR2d5Ohybk/5RpAEXP2d6uqEMAAACACZiqkGgYRR0BADPw9PSUdHPUoZeXl3V7fHy8dZSip6enkpOTdf78eZtRifHx8QoMDMz03I6OjnJ0dMxVXBaLhamPAAAAAIBcM8XU5rTRPdQRARQGX19feXp6KioqyrotOTlZW7ZssRYJAwICZG9vb9MmNjZWP/744x0LiQAAAAAAFBVTDE1hDUMA+e3SpUv69ddfrc+PHTum/fv3q0KFCqpevbpCQkIUFhYmPz8/+fn5KSwsTC4uLurVq5ckyd3dXf3799frr7+uihUrqkKFChoxYoQaNGhgXcUZAAAAAIC7iSkKiWkM5jYDyCd79uxR27Ztrc/T7lvYt29fzZs3TyNHjtTVq1c1ePBgnT9/Xs2aNVNkZKRcXV2tx3z44YcqXbq0evTooatXr6pdu3aaN2+e7Oy41xkAAAAA4O5jikJi2roFlBEB5Jc2bdrc8csJi8Wi0NBQhYaGZtrGyclJ06ZN07Rp0wogQgAAAAAA8pc57pFY1AEAAAAAAAAAxZwpColWDEkEAAAAAAAAcsUUhcS/V22mkggAAAAAAADkhkkKiUUdAQAAAAAAAFC85aiQGB4ergceeECurq6qUqWKunXrpiNHjti0MQxDoaGh8vb2lrOzs9q0aaOffvrJpk1SUpKGDh2qSpUqqUyZMnrsscf0xx9/5L03WWDRZgAAAAAAACB3clRI3LJli1599VXt2rVLUVFRunHjhoKCgnT58mVrm0mTJmnq1KmaPn26du/eLU9PT3Xo0EEXL160tgkJCdHKlSu1dOlSbd++XZcuXVKXLl2UkpKSfz27RdqARAqJAAAAAAAAQO6UzknjtWvX2jyfO3euqlSpoujoaD388MMyDEMREREaM2aMunfvLkmaP3++PDw8tHjxYg0cOFAJCQmaM2eOvvjiC7Vv316StHDhQvn4+Gj9+vXq2LFjPnXtFsxtBgAAAAAAAPIkT/dITEhIkCRVqFBBknTs2DHFxcUpKCjI2sbR0VGtW7fWjh07JEnR0dG6fv26TRtvb2/Vr1/f2qagsNgKAAAAAAAAkDs5GpF4K8MwNHz4cLVs2VL169eXJMXFxUmSPDw8bNp6eHjoxIkT1jYODg4qX758ujZpx98uKSlJSUlJ1ueJiYk5ipWpzQAAAAAAAEDe5HpE4pAhQ3TgwAEtWbIk3T7LbVOJDcNIt+12d2oTHh4ud3d368PHxydHsTKzGQAAAAAAAMibXBUShw4dqtWrV2vTpk2qVq2adbunp6ckpRtZGB8fbx2l6OnpqeTkZJ0/fz7TNrcbPXq0EhISrI+TJ0/mJmwmNgMAAAAAAAC5lKNComEYGjJkiFasWKGNGzfK19fXZr+vr688PT0VFRVl3ZacnKwtW7YoMDBQkhQQECB7e3ubNrGxsfrxxx+tbW7n6OgoNzc3m0dOWP43uZmpzQAAAAAAAEDu5Ogeia+++qoWL16sL7/8Uq6urtaRh+7u7nJ2dpbFYlFISIjCwsLk5+cnPz8/hYWFycXFRb169bK27d+/v15//XVVrFhRFSpU0IgRI9SgQQPrKs75janNAAAAAAAAQN7kqJA4c+ZMSVKbNm1sts+dO1cvvPCCJGnkyJG6evWqBg8erPPnz6tZs2aKjIyUq6urtf2HH36o0qVLq0ePHrp69aratWunefPmyc7OLm+9yRJDEgEAAAAAAIDcyFEh0cjG3GCLxaLQ0FCFhoZm2sbJyUnTpk3TtGnTcnL5XGPVZgAAAAAAACBvcr1qc3HC1GYAAAAAAAAgb0xRSEzDgEQAAAAAAAAgd0xRSPx71WZKiQAAAHkxY8YM+fr6ysnJSQEBAdq2bVumbTdv3iyLxZLu8fPPP1vbzJ49W61atVL58uVVvnx5tW/fXt9//31hdAUA8oR8CMCMTFFIFFObAQAA8mzZsmUKCQnRmDFjtG/fPrVq1UrBwcGKiYm543FHjhxRbGys9eHn52fdt3nzZj377LPatGmTdu7cqerVqysoKEinTp0q6O4AQK6RDwGYlTkKif/DeEQAAIDcmzp1qvr3768BAwaobt26ioiIkI+Pj2bOnHnH46pUqSJPT0/rw87Ozrpv0aJFGjx4sBo1aqQ6depo9uzZSk1N1YYNGwq6OwCQa+RDAGZlikIiqzYDAADkTXJysqKjoxUUFGSzPSgoSDt27LjjsY0bN5aXl5fatWunTZs23bHtlStXdP36dVWoUCHTNklJSUpMTLR5AEBhIR8CMDNzFBJZthkAACBPzp49q5SUFHl4eNhs9/DwUFxcXIbHeHl5adasWVq+fLlWrFghf39/tWvXTlu3bs30OqNGjVLVqlXVvn37TNuEh4fL3d3d+vDx8cldpwAgF8iHAMysdFEHUJgYkAgAAJA3t39BaxhGpl/a+vv7y9/f3/q8efPmOnnypCZPnqyHH344XftJkyZpyZIl2rx5s5ycnDKNYfTo0Ro+fLj1eWJiIh+eARQ68iEAMzLHiMT//ZdVmwEAAHKnUqVKsrOzSzfaJj4+Pt2onDt56KGHdPTo0XTbJ0+erLCwMEVGRqphw4Z3PIejo6Pc3NxsHgBQWMiHAMzMHIVEZjYDAADkiYODgwICAhQVFWWzPSoqSoGBgdk+z759++Tl5WWz7YMPPtA777yjtWvXqmnTpvkSLwAUFPIhADMz1dRmAAAA5N7w4cPVp08fNW3aVM2bN9esWbMUExOjQYMGSbo5xe7UqVNasGCBJCkiIkI1a9bUfffdp+TkZC1cuFDLly/X8uXLreecNGmS3n77bS1evFg1a9a0jvApW7asypYtW/idBIBsIB8CMCtTFBLTRiQysxkAACD3evbsqXPnzmnChAmKjY1V/fr1tWbNGtWoUUOSFBsbq5iYGGv75ORkjRgxQqdOnZKzs7Puu+8+ff311+rUqZO1zYwZM5ScnKynnnrK5lrjxo1TaGhoofQLAHKKfAjArCxGMbxxYGJiotzd3ZWQkJCte0Cs2ndKIcv2q+W9lbRwQLNCiBDA3SaneaO4KKn9AlAwSmrOKKn9AlBwSmreKKn9AlAwcpMzTHGPxDQG6zYDAAAAAAAAuWKKQiJTmwEAAAAAAIC8MUUhEQAAAAAAAEDemKqQyIhEAAAAAAAAIHdMUUi0/G9uM/dIBAAAAAAAAHLHHIXEog4AAAAAAAAAKOZMUUhMw9RmAAAAAAAAIHdMUUi0rtpctGEAAAAAAAAAxZY5ColMbgYAAAAAAADyxBSFRCuGJAIAAAAAAAC5YopC4t9Tm6kkAgAAAAAAALlhjkJiUQcAAAAAAAAAFHOmKCSmYdVmAAAAAAAAIHdMUUi0MCQRAAAAAAAAyBNTFBLTMCARAAAAAAAAyB2TFBJvDkk0mNsMAAAAAAAA5IopColMbQYAAAAAAADyxhSFxDSMRwRQWG7cuKG33npLvr6+cnZ2Vq1atTRhwgSlpqZa2xiGodDQUHl7e8vZ2Vlt2rTRTz/9VIRRAwAAAACQOVMUEtMGJDKzGUBhef/99/Xpp59q+vTpOnz4sCZNmqQPPvhA06ZNs7aZNGmSpk6dqunTp2v37t3y9PRUhw4ddPHixSKMHAAAAACAjJmjkMjcZgCFbOfOnXr88cfVuXNn1axZU0899ZSCgoK0Z88eSTdHI0ZERGjMmDHq3r276tevr/nz5+vKlStavHhxEUcPAAAAAEB6pigkpmFAIoDC0rJlS23YsEG//PKLJOmHH37Q9u3b1alTJ0nSsWPHFBcXp6CgIOsxjo6Oat26tXbs2JHpeZOSkpSYmGjzAAAAAACgMJQu6gAKg3U8InObARSSN954QwkJCapTp47s7OyUkpKid999V88++6wkKS4uTpLk4eFhc5yHh4dOnDiR6XnDw8M1fvz4ggscAAAAAIBMmGJEIjObARS2ZcuWaeHChVq8eLH27t2r+fPna/LkyZo/f75Nu9tvvWAYxh1vxzB69GglJCRYHydPniyQ+AEAAAAAuJ0pRiSmYTwigMLyf//3fxo1apSeeeYZSVKDBg104sQJhYeHq2/fvvL09JR0c2Sil5eX9bj4+Ph0oxRv5ejoKEdHx4INHgAAAACADJhqRCIzmwEUlitXrqhUKdsUa2dnp9TUVEmSr6+vPD09FRUVZd2fnJysLVu2KDAwsFBjBQAAAAAgO0wxItEi5jYDKFxdu3bVu+++q+rVq+u+++7Tvn37NHXqVPXr10/SzSnNISEhCgsLk5+fn/z8/BQWFiYXFxf16tWriKMHAAAAACA9UxQS0xhMbgZQSKZNm6a3335bgwcPVnx8vLy9vTVw4ECNHTvW2mbkyJG6evWqBg8erPPnz6tZs2aKjIyUq6trEUYOAAAAAEDGzFFIZGozgELm6uqqiIgIRUREZNrGYrEoNDRUoaGhhRYXAAAAAAC5ZY57JBZ1AAAAAAAAAEAxZ4pCYhpGJAIAAAAAAAC5Y4pCouV/yzZTRwQAAAAAAAByxxyFxKIOAAAAAAAAACjmTFFITGMwtxkAAAAAAADIFVMUEi0MSQQAAAAAAADyxByFRCY3AwAAAAAAAHliikJiGmY2AwAAAAAAALljikJi2tRmg3WbAQAAAAAAgFwxRyGxqAMAAAAAAAAAijlTFBLTMLUZAAAAAAAAyB1zFBKtU5sBAAAAAAAA5IYpComs2gwAAAAAAADkjSkKiWkM5jYDAAAAAAAAuWKKQqKFqc0AAAAAAABAnuS4kLh161Z17dpV3t7eslgsWrVqlc3+F154QRaLxebx0EMP2bRJSkrS0KFDValSJZUpU0aPPfaY/vjjjzx15E6Y2AwAAAAAAADkTY4LiZcvX9b999+v6dOnZ9rm0UcfVWxsrPWxZs0am/0hISFauXKlli5dqu3bt+vSpUvq0qWLUlJSct6DnGBIIgAAAAAAAJArOS4kBgcHa+LEierevXumbRwdHeXp6Wl9VKhQwbovISFBc+bM0ZQpU9S+fXs1btxYCxcu1MGDB7V+/frc9SILlv/NbaaOCAAAkDczZsyQr6+vnJycFBAQoG3btmXadvPmzelmqlgsFv3888/WNj/99JOefPJJ1axZUxaLRREREYXQCwDIO/IhADMqkHskbt68WVWqVFHt2rX10ksvKT4+3rovOjpa169fV1BQkHWbt7e36tevrx07dmR4vqSkJCUmJto8csLC3GYAAIA8W7ZsmUJCQjRmzBjt27dPrVq1UnBwsGJiYu543JEjR2xmq/j5+Vn3XblyRbVq1dJ7770nT0/Pgu4CAOQL8iEAs8r3QmJwcLAWLVqkjRs3asqUKdq9e7ceeeQRJSUlSZLi4uLk4OCg8uXL2xzn4eGhuLi4DM8ZHh4ud3d368PHxydXsbFqMwAAQO5NnTpV/fv314ABA1S3bl1FRETIx8dHM2fOvONxVapUsZmtYmdnZ933wAMP6IMPPtAzzzwjR0fHgu4CAOQL8iEAs8r3QmLPnj3VuXNn1a9fX127dtU333yjX375RV9//fUdjzMMwzoF+XajR49WQkKC9XHy5MkcxZR2VsqIAAAAuZOcnKzo6GibWSWSFBQUlOmskjSNGzeWl5eX2rVrp02bNuU5lrzOVgGAvCAfAjCzApnafCsvLy/VqFFDR48elSR5enoqOTlZ58+ft2kXHx8vDw+PDM/h6OgoNzc3m0dOMLUZAAAgb86ePauUlJR079fuNKvEy8tLs2bN0vLly7VixQr5+/urXbt22rp1a55iya/ZKgCQG+RDAGZWuqAvcO7cOZ08eVJeXl6SpICAANnb2ysqKko9evSQJMXGxurHH3/UpEmTCjQWZjYDAADkze0zSO40q8Tf31/+/v7W582bN9fJkyc1efJkPfzww7mOYfTo0Ro+fLj1eWJiIh+eARQ68iEAM8pxIfHSpUv69ddfrc+PHTum/fv3q0KFCqpQoYJCQ0P15JNPysvLS8ePH9ebb76pSpUq6YknnpAkubu7q3///nr99ddVsWJFVahQQSNGjFCDBg3Uvn37/OuZjbRVm6kkAgAA5EalSpVkZ2eXbrTNnWaVZOShhx7SwoUL8xSLo6Mj9w8DUGTIhwDMLMdTm/fs2aPGjRurcePGkqThw4ercePGGjt2rOzs7HTw4EE9/vjjql27tvr27avatWtr586dcnV1tZ7jww8/VLdu3dSjRw+1aNFCLi4u+uqrr2xuNJufmNoMAACQNw4ODgoICFBUVJTN9qioKAUGBmb7PPv27bPOVAGA4oh8CMDMcjwisU2bNndc/XjdunVZnsPJyUnTpk3TtGnTcnr5PGFqMwAAQO4NHz5cffr0UdOmTdW8eXPNmjVLMTExGjRokKSbU+xOnTqlBQsWSJIiIiJUs2ZN3XfffUpOTtbChQu1fPlyLV++3HrO5ORkHTp0yPrvU6dOaf/+/Spbtqzuvffewu8kAGQD+RCAWRX4PRLvBtZVmykkAgAA5FrPnj117tw5TZgwQbGxsapfv77WrFmjGjVqSLp53+uYmBhr++TkZI0YMUKnTp2Ss7Oz7rvvPn399dfq1KmTtc3p06etM10kafLkyZo8ebJat26tzZs3F1rfACAnyIcAzMpi3Gl44V0qMTFR7u7uSkhIyNYKzvtPXlC3T75V1XLO+nbUI4UQIYC7TU7zRnFRUvsFoGCU1JxRUvsFoOCU1LxRUvsFoGDkJmfk+B6JAAAAAAAAAMzHFIXEv6c2F7vBlwAAAAAAAMBdwRyFRFZtBgAAAAAAAPLEFIXENIxHBAAAAAAAAHLHFIVEy/8mNzOzGQAAAAAAAMgdcxQSmdoMAAAAAAAA5IkpColpDCY3AwAAAAAAALlirkIidUQAAAAAAAAgV0xRSGRqMwAAAAAAAJA3pigkpmFAIgAAAAAAAJA7pigksmozAAAAAAAAkDfmKCQytRkAAAAAAADIE1MUEv/GkEQAAAAAAAAgN0xRSEwbkcjUZgAAAAAAACB3zFFIFHObAQAAAAAAgLwwRSExDQMSAQAAAAAAgNwxRSHx76nNlBIBAAAAAACA3DBHIbGoAwBgSqdOndJzzz2nihUrysXFRY0aNVJ0dLR1v2EYCg0Nlbe3t5ydndWmTRv99NNPRRgxAAAAAACZM0UhMQ3jEQEUlvPnz6tFixayt7fXN998o0OHDmnKlCkqV66ctc2kSZM0depUTZ8+Xbt375anp6c6dOigixcvFl3gAAAAAABkonRRB1AYWLUZQGF7//335ePjo7lz51q31axZ0/pvwzAUERGhMWPGqHv37pKk+fPny8PDQ4sXL9bAgQMLO2QAAAAAAO7IJCMSmdwMoHCtXr1aTZs21dNPP60qVaqocePGmj17tnX/sWPHFBcXp6CgIOs2R0dHtW7dWjt27Mj0vElJSUpMTLR5AAAAAABQGExSSLyJxVYAFJbff/9dM2fOlJ+fn9atW6dBgwZp2LBhWrBggSQpLi5OkuTh4WFznIeHh3VfRsLDw+Xu7m59+Pj4FFwnAAAAAAC4hSkKidapzUUbBgATSU1NVZMmTRQWFqbGjRtr4MCBeumllzRz5kybdhaL7YhpwzDSbbvV6NGjlZCQYH2cPHmyQOIHAAAAAOB25igkFnUAAEzHy8tL9erVs9lWt25dxcTESJI8PT0lKd3ow/j4+HSjFG/l6OgoNzc3mwcAAAAAAIXBFIVEK4YkAigkLVq00JEjR2y2/fLLL6pRo4YkydfXV56enoqKirLuT05O1pYtWxQYGFiosQIAAAAAkB0mWbX55phE6ogACss//vEPBQYGKiwsTD169ND333+vWbNmadasWZJu5qWQkBCFhYXJz89Pfn5+CgsLk4uLi3r16lXE0aOgpKSk6Pr160UdBko4BwcHlSplru+KAQAAUDjMUUgs6gAAmM4DDzyglStXavTo0ZowYYJ8fX0VERGh3r17W9uMHDlSV69e1eDBg3X+/Hk1a9ZMkZGRcnV1LcLIURAMw1BcXJwuXLhQ1KHABEqVKiVfX185ODgUdSgAAAAoYUxRSEzDqs0AClOXLl3UpUuXTPdbLBaFhoYqNDS08IJCkUgrIlapUkUuLi53XFAHyIvU1FSdPn1asbGxql69Oq81AAAA5CtTFBJZtRkAUFRSUlKsRcSKFSsWdTgwgcqVK+v06dO6ceOG7O3tizocAAAAlCCmuIGOhcnNAIAiknZPRBcXlyKOBGaRNqU5JSWliCMBAABASWOKQmIaZjYDAIoKU0xRWHitAQAAoKCYopD499RmKokAAAAAAABAbpiikAgAAO4+mzdvlsViyXI165o1ayoiIqJQYsqJefPmqVy5ckUdBgAAAFBoTFVIZGozAAB3j8DAQMXGxsrd3V1S5oW53bt36+WXXy7k6AAAAADcjlWbAQBAkXBwcJCnp2eW7SpXrlwI0QAAAADIiilGJHLTcQAAcq5NmzYaMmSIhgwZonLlyqlixYp66623ZNwyxP/8+fN6/vnnVb58ebm4uCg4OFhHjx617j9x4oS6du2q8uXLq0yZMrrvvvu0Zs0aSbZTmzdv3qwXX3xRCQkJslgsslgsCg0NlWQ7tfnZZ5/VM888YxPn9evXValSJc2dO1eSZBiGJk2apFq1asnZ2Vn333+//vOf/9yxr8nJyRo5cqSqVq2qMmXKqFmzZtq8ebNNm3nz5ql69epycXHRE088oXPnzqU7z8SJE1WlShW5urpqwIABGjVqlBo1amTTZu7cuapbt66cnJxUp04dzZgxwyaOIUOGyMvLS05OTqpZs6bCw8PvGDsAAABQWEwxItGKIYkAgLuAYRi6ej2lSK7tbG+Xoy/Y5s+fr/79++u7777Tnj179PLLL6tGjRp66aWXJEkvvPCCjh49qtWrV8vNzU1vvPGGOnXqpEOHDsne3l6vvvqqkpOTtXXrVpUpU0aHDh1S2bJl010nMDBQERERGjt2rI4cOSJJGbbr3bu3evTooUuXLln3r1u3TpcvX9aTTz4pSXrrrbe0YsUKzZw5U35+ftq6dauee+45Va5cWa1bt86wny+++KKOHz+upUuXytvbWytXrtSjjz6qgwcPys/PT99995369eunsLAwde/eXWvXrtW4ceNszrFo0SK9++67mjFjhlq0aKGlS5dqypQp8vX1tbaZPXu2xo0bp+nTp6tx48bat2+fXnrpJZUpU0Z9+/bVxx9/rNWrV+tf//qXqlevrpMnT+rkyZPZ/n0BAAAABckUhcS0j0us2gwAuBtcvZ6iemPXFcm1D03oKBeH7P/v38fHRx9++KEsFov8/f118OBBffjhh3rppZesBcRvv/1WgYGBkm4W03x8fLRq1So9/fTTiomJ0ZNPPqkGDRpIkmrVqpXhdRwcHOTu7i6LxXLH6c4dO3ZUmTJltHLlSvXp00eStHjxYnXt2lVubm66fPmypk6dqo0bN6p58+bWa27fvl2fffZZhoXE3377TUuWLNEff/whb29vSdKIESO0du1azZ07V2FhYfroo4/UsWNHjRo1SpJUu3Zt7dixQ2vXrrWeZ9q0aerfv79efPFFSdLYsWMVGRmpS5cuWdu88847mjJlirp37y5J8vX11aFDh/TZZ5+pb9++iomJkZ+fn1q2bCmLxaIaNWpk47cEAAAAFA6TTG0u6ggAACieHnroIZsRjM2bN9fRo0eVkpKiw4cPq3Tp0mrWrJl1f8WKFeXv76/Dhw9LkoYNG6aJEyeqRYsWGjdunA4cOJCneOzt7fX0009r0aJFkqTLly/ryy+/VO/evSVJhw4d0rVr19ShQweVLVvW+liwYIF+++23DM+5d+9eGYah2rVr2xyzZcsW6zGHDx+2FiZv/Vnc6siRI3rwwQdttt36/MyZMzp58qT69+9vc52JEydar/PCCy9o//798vf317BhwxQZGZmHnxYAAACQv0wxIjENqzYDAO4GzvZ2OjShY5FdO78YmfyP1TAMa/FxwIAB6tixo77++mtFRkYqPDxcU6ZM0dChQ3N93d69e6t169aKj49XVFSUnJycFBwcLElKTU2VJH399deqWrWqzXGOjo4Zni81NVV2dnaKjo6WnZ3tzydt+nRmfb3d7dPGbz0uLbbZs2fbFF8lWa/bpEkTHTt2TN98843Wr1+vHj16qH379lne4xEAAAAoDKYoJFr+N7mZOiIA4G5gsVhyNL24KO3atSvdcz8/P9nZ2alevXq6ceOGvvvuO+vU5nPnzumXX35R3bp1rcf4+Pho0KBBGjRokEaPHq3Zs2dnWEh0cHBQSkrW944MDAyUj4+Pli1bpm+++UZPP/20HBwcJEn16tWTo6OjYmJiMr0f4u0aN26slJQUxcfHq1WrVhm2qVevXoY/i1v5+/vr+++/t065lqQ9e/ZY/+3h4aGqVavq999/t46gzIibm5t69uypnj176qmnntKjjz6qv/76SxUqVMhWfwAAAICCUjw+xeQRU5sBAMidkydPavjw4Ro4cKD27t2radOmacqUKZIkPz8/Pf7443rppZf02WefydXVVaNGjVLVqlX1+OOPS5JCQkIUHBys2rVr6/z589q4caNNkfFWNWvW1KVLl7Rhwwbdf//9cnFxkYuLS7p2FotFvXr10qeffqpffvlFmzZtsu5zdXXViBEj9I9//EOpqalq2bKlEhMTtWPHDpUtW1Z9+/ZNd77atWurd+/eev755zVlyhQ1btxYZ8+e1caNG9WgQQN16tRJw4YNU2BgoCZNmqRu3bopMjLS5v6IkjR06FC99NJLatq0qQIDA7Vs2TIdOHDA5r6QoaGhGjZsmNzc3BQcHKykpCTt2bNH58+f1/Dhw/Xhhx/Ky8tLjRo1UqlSpfTvf/9bnp6eKleuXI5/dwAAAEB+M8U9EtNkd1oSAAC46fnnn9fVq1f14IMP6tVXX9XQoUP18ssvW/fPnTtXAQEB6tKli5o3by7DMLRmzRrZ29tLklJSUvTqq6+qbt26evTRR+Xv768ZM2ZkeK3AwEANGjRIPXv2VOXKlTVp0qRM4+rdu7cOHTqkqlWrqkWLFjb73nnnHY0dO1bh4eGqW7euOnbsqK+++spm9eTbzZ07V88//7xef/11+fv767HHHtN3330nHx8fSTfvFfnPf/5T06ZNU6NGjRQZGam33norXUyjR4/WiBEjrFOUX3jhBTk5OVnbDBgwQP/85z81b948NWjQQK1bt9a8efOssZUtW1bvv/++mjZtqgceeEDHjx/XmjVrVKqUqd6yAQAA4C5lMYphdS0xMVHu7u5KSEiQm5tblu3jE6/pwbANslikY+GdCyFCAHebnOaN4qKk9qskuXbtmo4dOyZfX1+bglJx0KZNGzVq1EgRERFFHUqx1aFDB3l6euqLL74otGve6TVXUnNGSe0XgIJTUvNGSe0XgIKRm5xhiqnNYmozAAAoYFeuXNGnn36qjh07ys7OTkuWLNH69esVFRVV1KEBAAAA+cJU82SK39hLAABQXFgsFq1Zs0atWrVSQECAvvrqKy1fvlzt27cv6tDy1YwZM6yjHQMCArRt27ZM227evFkWiyXd4+eff7Zpt3z5cutCOfXq1dPKlSsLuhsAkGfkQwBmZIoRiRaGJAIAkGObN28u6hCKFWdnZ61fv76owyhQy5YtU0hIiGbMmKEWLVros88+U3BwsA4dOqTq1atnetyRI0dspstUrlzZ+u+dO3eqZ8+eeuedd/TEE09o5cqV6tGjh7Zv365mzZoVaH8AILfIhwDMyhT3SDx7KUlNJ958Y3/8Pe6RCJhRSb1fTEntV0lSnO+RiOKpIO+R2KxZMzVp0kQzZ860bqtbt666deum8PDwdO03b96stm3b6vz585muPN2zZ08lJibqm2++sW579NFHVb58eS1ZsiRbceW0X4Zh6Or1lGydG0Dx4mxvJ4sl64Ek5ENyIVCSFWQuNMWIxFsZhpGtHyYAAAD+lpycrOjoaI0aNcpme1BQkHbs2HHHYxs3bqxr166pXr16euutt9S2bVvrvp07d+of//iHTfuOHTvecZGfpKQkJSUlWZ8nJibmoCfS1espqjd2XY6OAVA8HJrQUS4OBfsxt6TkQ3IhUHIVZC40xT0SKRsCAADkzdmzZ5WSkiIPDw+b7R4eHoqLi8vwGC8vL82aNUvLly/XihUr5O/vr3bt2mnr1q3WNnFxcTk6pySFh4fL3d3d+vDx8clDzwAgZ8iHAMzMhCMSJQYkAgAA5M7tMzvuNNvD399f/v7+1ufNmzfXyZMnNXnyZD388MO5OqckjR49WsOHD7c+T0xMzNGHZ2d7Ox2a0DHb7QEUH872doV2reKeD8mFQMlVkLnQFIXEWxNvsbshJAAAwF2gUqVKsrOzSzcyJj4+Pt0Imjt56KGHtHDhQutzT0/PHJ/T0dFRjo6O2b7m7SwWS4FPfQRQcpWUfEguBJAbTG0GAABAlhwcHBQQEKCoqCib7VFRUQoMDMz2efbt2ycvLy/r8+bNm6c7Z2RkZI7OCQCFiXwIwMxM9/XDzUWqKS0CAFDchYaGatWqVdq/f39Rh5KOxWLRypUr1a1bt6IOJV8NHz5cffr0UdOmTdW8eXPNmjVLMTExGjRokKSbU+xOnTqlBQsWSJIiIiJUs2ZN3XfffUpOTtbChQu1fPlyLV++3HrO1157TQ8//LDef/99Pf744/ryyy+1fv16bd++vUj6CADZQT4EYFY5HpG4detWde3aVd7e3rJYLFq1apXNfsMwFBoaKm9vbzk7O6tNmzb66aefbNokJSVp6NChqlSpksqUKaPHHntMf/zxR546cie33lKCqc0AABQ/Gb3nGDFihDZs2FA0AZlUz549FRERoQkTJqhRo0baunWr1qxZoxo1akiSYmNjFRMTY22fnJysESNGqGHDhmrVqpW2b9+ur7/+Wt27d7e2CQwM1NKlSzV37lw1bNhQ8+bN07Jly9SsWbNC7x8AZBf5EIBZWYybQ/Sy7ZtvvtG3336rJk2a6Mknn0z3bfv777+vd999V/PmzVPt2rU1ceJEbd26VUeOHJGrq6sk6ZVXXtFXX32lefPmqWLFinr99df1119/KTo6WnZ2Wd8QMjExUe7u7kpISJCbm1uW7ROuXNf9EyIlSUffDZa9nSlmdAO4RU7zRnFRUvtVkly7dk3Hjh2Tr6+vnJycijqcYqu4jfArynjv9JorqTmjpPYLQMEpqXmjpPYLQMHITc7IcUUtODhYEydOtPnmJI1hGIqIiNCYMWPUvXt31a9fX/Pnz9eVK1e0ePFiSVJCQoLmzJmjKVOmqH379mrcuLEWLlyogwcPav369TkNJ8dyVjYFAMC82rRpo2HDhmnkyJGqUKGCPD09FRoaatMmISFBL7/8sqpUqSI3Nzc98sgj+uGHH2zaTJw4UVWqVJGrq6sGDBigUaNGqVGjRtb9u3fvVocOHVSpUiW5u7urdevW2rt3r3V/zZo1JUlPPPGELBaL9XloaKj1POvWrZOTk5MuXLhgc+1hw4apdevW1uc7duzQww8/LGdnZ/n4+GjYsGG6fPnyHX8OX331lQICAuTk5KRatWpp/PjxunHjhnX/0aNH9fDDD8vJyUn16tVLd3+rtOs2atRITk5Oatq0qVatWiWLxWIzLfvQoUPq1KmTypYtKw8PD/Xp00dnz5617v/Pf/6jBg0ayNnZWRUrVlT79u2zjB0AAADIT/k6NO/YsWOKi4tTUFCQdZujo6Nat26tHTt2SJKio6N1/fp1mzbe3t6qX7++tc3tkpKSlJiYaPPIEZupzVQSAQBFzDCk5MtF88jhN2rz589XmTJl9N1332nSpEmaMGGCtVBmGIY6d+6suLg4rVmzRtHR0WrSpInatWunv/76S5K0aNEivfvuu3r//fcVHR2t6tWra+bMmTbXuHjxovr27att27Zp165d8vPzU6dOnXTx4kVJNwuNkjR37lzFxsZan9+qffv2KleunM29plJSUvSvf/1LvXv3liQdPHhQHTt2VPfu3XXgwAEtW7ZM27dv15AhQzLt/7p16/Tcc89p2LBhOnTokD777DPNmzdP7777riQpNTVV3bt3l52dnXbt2qVPP/1Ub7zxRrr+de3aVQ0aNNDevXv1zjvvpGsTGxur1q1bq1GjRtqzZ4/Wrl2rP//8Uz169LDuf/bZZ9WvXz8dPnxYmzdvVvfu3ZXDiSUAAABAnuTrYitpS9Xfvjy9h4eHTpw4YW3j4OCg8uXLp2tz+1L3acLDwzV+/Phcx2VhbRUAwN3k+hUpzLtorv3macmhTLabN2zYUOPGjZMk+fn5afr06dqwYYM6dOigTZs26eDBg4qPj5ejo6MkafLkyVq1apX+85//6OWXX9a0adPUv39/vfjii5KksWPHKjIyUpcuXbJe45FHHrG55meffaby5ctry5Yt6tKliypXrixJKleunDw9PTOM087OTj179tTixYvVv39/SdKGDRt0/vx5Pf3005KkDz74QL169VJISIi1Px9//LFat26tmTNnZjj1/N1339WoUaPUt29fSVKtWrX0zjvvaOTIkRo3bpzWr1+vw4cP6/jx46pWrZokKSwsTMHBwdZzLFq0SBaLRbNnz7aOWjx16pReeukla5uZM2eqSZMmCgsLs277/PPP5ePjo19++UWXLl3SjRs31L17d+v9txo0aJD5Lw4AAAAoAAVys0DLbZU7wzDSbbvdndqMHj1aCQkJ1sfJkydzHRtf3AMAkH0NGza0ee7l5aX4+HhJN2cZXLp0SRUrVlTZsmWtj2PHjum3336TJB05ckQPPvigzTlufx4fH69Bgwapdu3acnd3l7u7uy5dumRzk/rs6N27tzZv3qzTp09LulnA69Spk/XLy+joaM2bN88m1o4dOyo1NVXHjh3L8JzR0dGaMGGCzTEvvfSSYmNjdeXKFR0+fFjVq1e3FhElqXnz5jbnOHLkiBo2bGhTqLz9ZxAdHa1NmzbZXKdOnTqSpN9++03333+/2rVrpwYNGujpp5/W7Nmzdf78+Rz9fAAAAIC8ytcRiWmjBOLi4uTl5WXdHh8fbx2l6OnpqeTkZJ0/f95mVGJ8fLwCAwMzPK+jo6N1pENuMCARAHBXsXe5OTKwqK6dk+b29jbPLRaLUlNTJd2c1uvl5aXNmzenO65cuXI2x9zq9um4L7zwgs6cOaOIiAjVqFFDjo6Oat68uZKTk3MU64MPPqh77rlHS5cu1SuvvKKVK1dq7ty51v2pqakaOHCghg0blu7Y6tWrZ3jO1NRUjR8/PsN7Qzs5OWU4tTg7X6jeflxqaqq6du2q999/P935vLy8ZGdnp6ioKO3YsUORkZGaNm2axowZo++++06+vr4Zxg4AAADkt3wtJPr6+srT01NRUVFq3LixpJvL3G/ZssX6xjggIED29vaKioqyue/Pjz/+qEmTJuVnOFZZjYYEAKBQWSw5ml58t2rSpIni4uJUunRp6wIot/P399f333+vPn36WLft2bPHps22bds0Y8YMderUSZJ08uRJm0VGpJsFzZSUlCxj6tWrlxYtWqRq1aqpVKlS6ty5s028P/30k+69997sdlFNmjTRkSNHMj2mXr16iomJ0enTp+XtfXO6+s6dO23a1KlTR4sWLVJSUpL1i9HbfwZNmjTR8uXLVbNmTZUunfHbM4vFohYtWqhFixYaO3asatSooZUrV2r48OHZ7g8AAACQFzme2nzp0iXt37/fusrgsWPHtH//fsXExMhisSgkJERhYWFauXKlfvzxR73wwgtycXFRr169JEnu7u7q37+/Xn/9dW3YsEH79u3Tc889pwYNGqh9+/b52rmMMLUZAID80b59ezVv3lzdunXTunXrdPz4ce3YsUNvvfWWtVA2dOhQzZkzR/Pnz9fRo0c1ceJEHThwwOZLvnvvvVdffPGFDh8+rO+++069e/eWs7OzzbVq1qypDRs2KC4u7o5Tenv37q29e/fq3Xff1VNPPWUznfiNN97Qzp079eqrr2r//v06evSoVq9eraFDh2Z6vrFjx2rBggUKDQ3VTz/9pMOHD2vZsmV66623rD8Df39/Pf/88/rhhx+0bds2jRkzxuYcvXr1Umpqql5++WUdPnxY69at0+TJkyX9/WXnq6++qr/++kvPPvusvv/+e/3++++KjIxUv379lJKSou+++05hYWHas2ePYmJitGLFCp05c0Z169bNzq8KAAAAyBc5LiTu2bNHjRs3to44HD58uBo3bqyxY8dKkkaOHKmQkBANHjxYTZs21alTpxQZGSlXV1frOT788EN169ZNPXr0UIsWLeTi4qKvvvpKdnZ2+dQtW7eOR2TVZgAA8ofFYtGaNWv08MMPq1+/fqpdu7aeeeYZHT9+3HpLk969e2v06NEaMWKEmjRpomPHjumFF16wKfB9/vnnOn/+vBo3bqw+ffpo2LBhqlKlis21pkyZoqioKPn4+Fjfg2TEz89PDzzwgA4cOGBdrTlNw4YNtWXLFh09elStWrVS48aN9fbbb9vcjuV2HTt21H//+19FRUXpgQce0EMPPaSpU6daFzwpVaqUVq5cqaSkJD344IMaMGCAdUXnNG5ubvrqq6+0f/9+NWrUSGPGjLG+b0r7OXh7e+vbb79VSkqKOnbsqPr16+u1116Tu7u7SpUqJTc3N23dulWdOnVS7dq19dZbb2nKlCk2i7oAAAAABc1iZHRzn7tcYmKi3N3dlZCQIDc3tyzbX0m+oXpj10mSDk3oKBeHfJ3RDaAYyGneKC5Kar9KkmvXrunYsWPy9fXNcFVgM+rQoYM8PT31xRdfFHUoRWbRokV68cUXlZCQkG70ZV7d6TVXUnNGSe0XgIJTUvNGSe0XgIKRm5xhuopa8SubAgBQfF25ckWffvqpOnbsKDs7Oy1ZskTr169XVFRUUYdWqBYsWKBatWqpatWq+uGHH/TGG2+oR48e+V5EBAAAAAqSKQqJllsmN1NHBACg8KRNf544caKSkpLk7++v5cuXF8p9ke8mcXFxGjt2rOLi4uTl5aWnn3463RRoAAAA4G5njkIiizYDAFAknJ2dtX79+qIOo8iNHDlSI0eOLOowAAAAgDzJ8WIrxV0xvCUkAAAAAAAAUOTMV0gs6gAAAAAAAACAYsgUhUSmNgMAilpqampRhwCTYPYFAAAACoop7pF4K95bAwAKk4ODg0qVKqXTp0+rcuXKcnBwkIVvuFBADMPQmTNnZLFYZG9vX9ThAAAAoIQxRSHx1lWbmdsMAChMpUqVkq+vr2JjY3X69OmiDgcmYLFYVK1aNdnZ2RV1KAAAAChhzFFIZOAHgCIWHh6uN998U6+99poiIiIk3Rw5NH78eM2aNUvnz59Xs2bN9Mknn+i+++4r2mCR7xwcHFS9enXduHFDKSkpRR0OSjh7e3uKiAAAACgQpigk3spgSCKAQrZ7927NmjVLDRs2tNk+adIkTZ06VfPmzVPt2rU1ceJEdejQQUeOHJGrq2sRRYuCkjbVlOmmAAAAAIorcyy2csu/uUcigMJ06dIl9e7dW7Nnz1b58uWt2w3DUEREhMaMGaPu3burfv36mj9/vq5cuaLFixcXYcQAAAAAAGTMHIVE5jYDKCKvvvqqOnfurPbt29tsP3bsmOLi4hQUFGTd5ujoqNatW2vHjh2Zni8pKUmJiYk2DwAAAAAACoMJpzYDQOFYunSp9u7dq927d6fbFxcXJ0ny8PCw2e7h4aETJ05kes7w8HCNHz8+fwMFAAAAACAbzDEi8ZZ/G8xtBlAITp48qddee00LFy6Uk5NTpu1uHzFtGMYdR1GPHj1aCQkJ1sfJkyfzLWYAAAAAAO7EFCMSmdkMoLBFR0crPj5eAQEB1m0pKSnaunWrpk+friNHjki6OTLRy8vL2iY+Pj7dKMVbOTo6ytHRseACBwAAAAAgE6YYkXgrxiMCKAzt2rXTwYMHtX//fuujadOm6t27t/bv369atWrJ09NTUVFR1mOSk5O1ZcsWBQYGFmHkAAAAAABkzCQjEv8eksjMZgCFwdXVVfXr17fZVqZMGVWsWNG6PSQkRGFhYfLz85Ofn5/CwsLk4uKiXr16FUXIAAAAAADckSkKiQBwNxo5cqSuXr2qwYMH6/z582rWrJkiIyPl6upa1KEBAAAAAJCO6QqJBpObARSRzZs32zy3WCwKDQ1VaGhokcQDAAAAAEBOmOYeidbZzdQRAQAAAAAAgBwzTyGxqAMAAAAAAAAAijHTFBLTMCARAAAAAAAAyDnTFBLTVm5m1WYAAAAAAAAg58xTSCzqAAAAAAAAAIBizDSFxDSs2gwAAAAAAADknGkKiWmrNjO1GQAAAAAAAMg58xQSmdwMAAAAAAAA5JppColpGJAIAAAAAAAA5Jx5ConWqc2UEgEAAAAAAICcMk0hkYnNAAAAAAAAQO6ZppCYhgGJAAAAAAAAQM6ZppBoYUgiAAAAAAAAkGvmKSQyuRkAAAAAAADINdMUEtMwtRkAAAAAAADIOdMUEtOmNhuikggAAAAAAADklHkKiUUdAAAAAAAAAFCMmaaQmIapzQAAALk3Y8YM+fr6ysnJSQEBAdq2bVu2jvv2229VunRpNWrUyGb79evXNWHCBN1zzz1ycnLS/fffr7Vr1xZA5ACQv8iHAMzINIVEy//mNlNHBAAAyJ1ly5YpJCREY8aM0b59+9SqVSsFBwcrJibmjsclJCTo+eefV7t27dLte+utt/TZZ59p2rRpOnTokAYNGqQnnnhC+/btK6huAECekQ8BmJV5ColFHQAAAEAxN3XqVPXv318DBgxQ3bp1FRERIR8fH82cOfOOxw0cOFC9evVS8+bN0+374osv9Oabb6pTp06qVauWXnnlFXXs2FFTpkwpqG4AQJ6RDwGYlWkKiWkM5jYDAADkWHJysqKjoxUUFGSzPSgoSDt27Mj0uLlz5+q3337TuHHjMtyflJQkJycnm23Ozs7avn17pudMSkpSYmKizQMACgv5EICZmaeQaF21GQAAADl19uxZpaSkyMPDw2a7h4eH4uLiMjzm6NGjGjVqlBYtWqTSpUtn2KZjx46aOnWqjh49qtTUVEVFRenLL79UbGxsprGEh4fL3d3d+vDx8cl9xwAgh8iHAMzMNIVEpjYDAADkXdp9p9MYhpFumySlpKSoV69eGj9+vGrXrp3p+T766CP5+fmpTp06cnBw0JAhQ/Tiiy/Kzs4u02NGjx6thIQE6+PkyZO57xAA5BL5EIAZZfxVSAnGzGYAAICcq1Spkuzs7NKNtomPj083KkeSLl68qD179mjfvn0aMmSIJCk1NVWGYah06dKKjIzUI488osqVK2vVqlW6du2azp07J29vb40aNUq+vr6ZxuLo6ChHR8f87SAAZBP5EICZmWdEovWbISqJAAAAOeXg4KCAgABFRUXZbI+KilJgYGC69m5ubjp48KD2799vfQwaNEj+/v7av3+/mjVrZtPeyclJVatW1Y0bN7R8+XI9/vjjBdofAMgt8iEAMzPNiMQMRpgDAAAgB4YPH64+ffqoadOmat68uWbNmqWYmBgNGjRI0s0pdqdOndKCBQtUqlQp1a9f3+b4KlWqyMnJyWb7d999p1OnTqlRo0Y6deqUQkNDlZqaqpEjRxZq3wAgJ8iHAMzKNIXENExtBgAAyJ2ePXvq3LlzmjBhgmJjY1W/fn2tWbNGNWrUkCTFxsYqJiYmR+e8du2a3nrrLf3+++8qW7asOnXqpC+++ELlypUrgB4AQP4gHwIwK4thFL/SWmJiotzd3ZWQkCA3N7dsHdN4QqTOX7muyH88rNoergUcIYC7TW7yRnFQUvsFoGCU1JxRUvsFoOCU1LxRUvsFoGDkJmeY8B6JAAAAAAAAAHLKNIXENMVv/CUAAAAAAABQ9ExTSPx7zWYqiQAAAAAAAEBOmaeQyMxmAAAAAAAAINdMU0hMw9RmAAAAAAAAIOdMVEi8OSSRQiIAAAAAAACQc6YpJDK1GQAAAAAAAMg90xQS07DYCgAAAAAAAJBz+V5IDA0NlcVisXl4enpa9xuGodDQUHl7e8vZ2Vlt2rTRTz/9lN9hpGNdtZk6IgAAAAAAAJBjBTIi8b777lNsbKz1cfDgQeu+SZMmaerUqZo+fbp2794tT09PdejQQRcvXiyIUKyY2gwAAAAAAADkXoEUEkuXLi1PT0/ro3LlypJujkaMiIjQmDFj1L17d9WvX1/z58/XlStXtHjx4oIIBQAAAAAAAEA+KJBC4tGjR+Xt7S1fX18988wz+v333yVJx44dU1xcnIKCgqxtHR0d1bp1a+3YsaMgQrGysGozAAAAAAAAkGul8/uEzZo104IFC1S7dm39+eefmjhxogIDA/XTTz8pLi5OkuTh4WFzjIeHh06cOJHpOZOSkpSUlGR9npiYmOO4mNoMAAAAAAAA5F6+FxKDg4Ot/27QoIGaN2+ue+65R/Pnz9dDDz0kSbLcVtUzDCPdtluFh4dr/Pjx+RIfqzYDAAAAAAAAOVcgU5tvVaZMGTVo0EBHjx61rt6cNjIxTXx8fLpRircaPXq0EhISrI+TJ0/mOA5WbQYAAAAAAAByr8ALiUlJSTp8+LC8vLzk6+srT09PRUVFWfcnJydry5YtCgwMzPQcjo6OcnNzs3nk1J1GPAIAAAAAAAC4s3yf2jxixAh17dpV1atXV3x8vCZOnKjExET17dtXFotFISEhCgsLk5+fn/z8/BQWFiYXFxf16tUrv0PJEAMSAQAAAAAAgJzL9xGJf/zxh5599ln5+/ure/fucnBw0K5du1SjRg1J0siRIxUSEqLBgweradOmOnXqlCIjI+Xq6prfoWTIYG4zgEIQHh6uBx54QK6urqpSpYq6deumI0eO2LQxDEOhoaHy9vaWs7Oz2rRpo59++qmIIgYAAAAA4M7yvZC4dOlSnT59WsnJyTp16pSWL1+uevXqWfdbLBaFhoYqNjZW165d05YtW1S/fv38DiMdZjYDKExbtmzRq6++ql27dikqKko3btxQUFCQLl++bG0zadIkTZ06VdOnT9fu3bvl6empDh066OLFi0UYOQAAAAAAGcv3qc13O8YjAigMa9eutXk+d+5cValSRdHR0Xr44YdlGIYiIiI0ZswYde/eXZI0f/58eXh4aPHixRo4cGBRhA0AAAAAQKYKfLGVu0XaiERmNgMoCgkJCZKkChUqSJKOHTumuLg4BQUFWds4OjqqdevW2rFjR6bnSUpKUmJios0DAAAAAIDCYJ5CopjbDKBoGIah4cOHq2XLltZbOcTFxUmSPDw8bNp6eHhY92UkPDxc7u7u1oePj0/BBQ4AAAAAwC1MU0j8G0MSARSuIUOG6MCBA1qyZEm6fZbbbuBqGEa6bbcaPXq0EhISrI+TJ0/me7wAAAAAAGTENPdIZGozgKIwdOhQrV69Wlu3blW1atWs2z09PSXdHJno5eVl3R4fH59ulOKtHB0d5ejoWHABAwAAAACQCdOMSGRiM4DCZBiGhgwZohUrVmjjxo3y9fW12e/r6ytPT09FRUVZtyUnJ2vLli0KDAws7HABAAAAAMiSaUYkpmFAIoDC8Oqrr2rx4sX68ssv5erqar3vobu7u5ydnWWxWBQSEqKwsDD5+fnJz89PYWFhcnFxUa9evYo4egAAAAAA0jNNITHtnmNMbQZQGGbOnClJatOmjc32uXPn6oUXXpAkjRw5UlevXtXgwYN1/vx5NWvWTJGRkXJ1dS3kaAEAAAAAyJppCokAUJiMbHxrYbFYFBoaqtDQ0IIPCAAAAACAPDLdPRKz8+EeAAAAAAAAgC3TFBLTKomUEQEAAAAAAICcM08hEQAAAAAAAECumaaQ+PfU5iINAwAAAAAAACiWzFNItFiybgQAAAAAAAAgQ6YpJKYxuEsiAAAAAAAAkGOmKSRaxyNSRwQAAAAAAAByzDSFRLtSN0uJN1KpJAIAAAAAAAA5ZZpCor3dza6mUEgEAAAAAAAAcsw0hURGJAIAAAAAAAC5Z5pCYun/FRJTUlOLOBIAAAAAAACg+DFNIZERiQAAAAAAAEDumaaQWNoubUQihUQAAAAAAAAgp0xTSLQrdbOrN1IoJAIAAAAAAAA5ZZpC4t/3SKSQCAAAAAAAAOSUaQqJafdIvM5iKwAAAAAAAECOmaaQaM89EgEAAAAAAIBcM00hkXskAgAA5N2MGTPk6+srJycnBQQEaNu2bdk67ttvv1Xp0qXVqFGjdPsiIiLk7+8vZ2dn+fj46B//+IeuXbuWz5EDQP4iHwIwI9MUErlHIgAAQN4sW7ZMISEhGjNmjPbt26dWrVopODhYMTExdzwuISFBzz//vNq1a5du36JFizRq1CiNGzdOhw8f1pw5c7Rs2TKNHj26oLoBAHlGPgRgVqYpJKbdI/EGhUQAAIBcmTp1qvr3768BAwaobt26ioiIkI+Pj2bOnHnH4wYOHKhevXqpefPm6fbt3LlTLVq0UK9evVSzZk0FBQXp2Wef1Z49ewqqGwCQZ+RDAGZlmkLi3yMSWWwFAAAgp5KTkxUdHa2goCCb7UFBQdqxY0emx82dO1e//fabxo0bl+H+li1bKjo6Wt9//70k6ffff9eaNWvUuXPnTM+ZlJSkxMREmwcAFBbyIQAzK13UARQWRiQCAADk3tmzZ5WSkiIPDw+b7R4eHoqLi8vwmKNHj2rUqFHatm2bSpfO+G3nM888ozNnzqhly5YyDEM3btzQK6+8olGjRmUaS3h4uMaPH5/7zgBAHpAPAZiZCUckUkgEAADILYvFYvPcMIx02yQpJSVFvXr10vjx41W7du1Mz7d582a9++67mjFjhvbu3asVK1bov//9r955551Mjxk9erQSEhKsj5MnT+a+QwCQS+RDAGZkmhGJpe3+t2ozhUQAAIAcq1Spkuzs7NKNtomPj083KkeSLl68qD179mjfvn0aMmSIJCk1NVWGYah06dKKjIzUI488orffflt9+vTRgAEDJEkNGjTQ5cuX9fLLL2vMmDEqVSr9996Ojo5ydHQsgF4CQNbIhwDMzHQjEm+kcI9EAACAnHJwcFBAQICioqJstkdFRSkwMDBdezc3Nx08eFD79++3PgYNGiR/f3/t379fzZo1kyRduXIl3YdjOzs7GYYhw+ALYAB3H/IhADMzzYhE7pEIAACQN8OHD1efPn3UtGlTNW/eXLNmzVJMTIwGDRok6eYUu1OnTmnBggUqVaqU6tevb3N8lSpV5OTkZLO9a9eumjp1qho3bqxmzZrp119/1dtvv63HHntMdnZ2hdo/AMgu8iEAszJNIZF7JAIAAORNz549de7cOU2YMEGxsbGqX7++1qxZoxo1akiSYmNjFRMTk6NzvvXWW7JYLHrrrbd06tQpVa5cWV27dtW7775bEF0AgHxBPgRgVhajGI6RTkxMlLu7uxISEuTm5patYz5af1Qfrv9FvZpVV9gTDQo4QgB3m9zkjeKgpPYLQMEoqTmjpPYLQMEpqXmjpPYLQMHITc4wzz0S7f43IjGl2NVNAQAAAAAAgCJnmkIi90gEAAAAAAAAcs80hcS/75HIqs0AAAAAAABATpmukMiIRAAAAAAAACDnTFNItLO72VVWbQYAAAAAAAByzjSFxLQRiddZbAUAAAAAAADIMdMUEu24RyIAAAAAAACQa6YpJHKPRAAAAAAAACD3TFNI/HtEIoVEAAAAAAAAIKdMU0gsXepmVxmRCAAAAAAAAOSceQqJdoxIBAAAAAAAAHLLPIVE7pEIAAAAAAAA5JppComs2gwAAAAAAADknmkKidZ7JKYwIhEAAAAAAADIKdMUEu2Y2gwAAAAAAADkmmkKiSy2AgAAAAAAAOSeaQqJDnY3u3o1OaWIIwEAAAAAAACKH9MUEr3LOUuS4i9eU/INFlwBcPeYMWOGfH195eTkpICAAG3btq2oQwIAAAAAIB3TFBIrlXWQs72dUg3p9IWrRR0OAEiSli1bppCQEI0ZM0b79u1Tq1atFBwcrJiYmKIODQAAAAAAG6WL8uIzZszQBx98oNjYWN13332KiIhQq1atCuRaFotFPhWc9cuflxTz1xXVrFSmQK4DADkxdepU9e/fXwMGDJAkRUREaN26dZo5c6bCw8Pz92KGIV2/kr/nBHB3sHeRLJaijgIAAAAlXJEVEtNG4cyYMUMtWrTQZ599puDgYB06dEjVq1cvkGtWr+CiX/68pOc//15Na5QvkGsAKDwBNcprdKe6RR1GriUnJys6OlqjRo2y2R4UFKQdO3ZkeExSUpKSkpKszxMTE7N/wetXpDDvXMUK4C735mnJgS9JAQAAULCKrJBYqKNw/ueBmhW0/nC8JGnPifMFcg0Ahcfd2b6oQ8iTs2fPKiUlRR4eHjbbPTw8FBcXl+Ex4eHhGj9+fGGEBwAAAACAjSIpJOZ0FE6eRuDc4uWHa6lZrYqKS7iWq+Pzh1GE1wZKlsqujkUdQr6w3DYd0TCMdNvSjB49WsOHD7c+T0xMlI+PT/YuZO9yc9QSgJLH3qWoIwAAAIAJFEkhMaejcPJrBI7FYlEjn3JSNj9zA0BBqlSpkuzs7NLlvfj4+HT5MY2jo6McHXNZQLVYmPoIAAAAAMi1Il21ObujcEaPHq2EhATr4+TJk4UVIgAUGAcHBwUEBCgqKspme1RUlAIDA4soKgAAAAAAMlYkIxJzOgonTyNwAOAuNnz4cPXp00dNmzZV8+bNNWvWLMXExGjQoEFFHRoAAAAAADaKpJB46yicJ554wro9KipKjz/+eFGEBABFomfPnjp37pwmTJig2NhY1a9fX2vWrFGNGjWKOjQAAAAAAGwU2arNjMIBgJsGDx6swYMHF3UYAAAAAADcUZEVEhmFAwAAAAAAABQfRVZIlBiFAwAAAAAAABQXRbpqMwAAAAAAAIDigUIiAAAAAAAAgCxRSAQAAAAAAACQJQqJAAAAAAAAALJUpIut5JZhGJKkxMTEIo4EQHGRli/S8kdJQT4EkBPkQgC4iXwIALnLhcWykHjx4kVJko+PTxFHAqC4uXjxotzd3Ys6jHxDPgSQG+RCALiJfAgAOcuFFqMYfgWTmpqq06dPy9XVVRaLJVvHJCYmysfHRydPnpSbm1sBR3h3oM/m6LNkzn7ntM+GYejixYvy9vZWqVIl564OOc2HvFbM0WfJnP02Y5+lnPWbXPg3M75e6LM5+iyZs9+8N7yJ94ZZM2OfJXP224x9lgr+vWGxHJFYqlQpVatWLVfHurm5meoFJNFnMzFjv3PS55L0bXOa3OZDXivmYcZ+m7HPUvb7TS60ZcbXC302DzP2m/eGvDfMLjP2WTJnv83YZ6ng3huWnK9eAAAAAAAAABQYCokAAAAAAAAAsmSaQqKjo6PGjRsnR0fHog6l0NBn8zBjv83Y5/xgxp+bGfssmbPfZuyzZN5+55UZf2702TzM2G8z9jk/mPHnZsY+S+bstxn7LBV8v4vlYisAAAAAAAAACpdpRiQCAAAAAAAAyD0KiQAAAAAAAACyRCERAAAAAAAAQJYoJAIAAAAAAADIkikKiTNmzJCvr6+cnJwUEBCgbdu2FXVIubZ161Z17dpV3t7eslgsWrVqlc1+wzAUGhoqb29vOTs7q02bNvrpp59s2iQlJWno0KGqVKmSypQpo8cee0x//PFHIfYiZ8LDw/XAAw/I1dVVVapUUbdu3XTkyBGbNiWx3zNnzlTDhg3l5uYmNzc3NW/eXN988411f0ns8+3Cw8NlsVgUEhJi3WaGfhck8mHxfq2YMR+SC8mFBYFcWLxfK2bMhRL5UCIf5reSlAsl8qFZ8iG58C7IhUYJt3TpUsPe3t6YPXu2cejQIeO1114zypQpY5w4caKoQ8uVNWvWGGPGjDGWL19uSDJWrlxps/+9994zXF1djeXLlxsHDx40evbsaXh5eRmJiYnWNoMGDTKqVq1qREVFGXv37jXatm1r3H///caNGzcKuTfZ07FjR2Pu3LnGjz/+aOzfv9/o3LmzUb16dePSpUvWNiWx36tXrza+/vpr48iRI8aRI0eMN99807C3tzd+/PFHwzBKZp9v9f333xs1a9Y0GjZsaLz22mvW7SW93wWJfFj8XytmzIfkQnJhfiMXFv/XihlzoWGQD8mH+auk5ULDIB+aJR+SC4s+F5b4QuKDDz5oDBo0yGZbnTp1jFGjRhVRRPnn9uSYmppqeHp6Gu+9955127Vr1wx3d3fj008/NQzDMC5cuGDY29sbS5cutbY5deqUUapUKWPt2rWFFntexMfHG5KMLVu2GIZhnn4bhmGUL1/e+Oc//1ni+3zx4kXDz8/PiIqKMlq3bm1NkCW93wWNfFjyXitmzYfkwpLd74JGLix5rxWz5kLDIB+W9H4XpJKcCw2DfGi2fEguLNx+l+ipzcnJyYqOjlZQUJDN9qCgIO3YsaOIoio4x44dU1xcnE1/HR0d1bp1a2t/o6Ojdf36dZs23t7eql+/frH5mSQkJEiSKlSoIMkc/U5JSdHSpUt1+fJlNW/evMT3+dVXX1Xnzp3Vvn17m+0lvd8FiXxYMl8rZsuH5MKbSnq/CxK5sGS+VsyWCyXyYZqS3u+CYrZcKJnntWK2fEguvKmw+106D3246509e1YpKSny8PCw2e7h4aG4uLgiiqrgpPUpo/6eOHHC2sbBwUHly5dP16Y4/EwMw9Dw4cPVsmVL1a9fX1LJ7vfBgwfVvHlzXbt2TWXLltXKlStVr1496x96Sezz0qVLtXfvXu3evTvdvpL8uy5o5ENZn5eU14qZ8iG50FZJ/T0XBnKhrM9LymvFTLlQIh/eriT/rguS2XKhZI7XipnyIbnQVmH/nkt0ITGNxWKxeW4YRrptJUlu+ltcfiZDhgzRgQMHtH379nT7SmK//f39tX//fl24cEHLly9X3759tWXLFuv+ktbnkydP6rXXXlNkZKScnJwybVfS+l2YyIcl57VipnxILsxYSet3YSIXlpzXiplyoUQ+zExJ63dhMVsulEr2a8VM+ZBcmLHC6neJntpcqVIl2dnZpauuxsfHp6vUlgSenp6SdMf+enp6Kjk5WefPn8+0zd1q6NChWr16tTZt2qRq1apZt5fkfjs4OOjee+9V06ZNFR4ervvvv18fffRRie1zdHS04uPjFRAQoNKlS6t06dLasmWLPv74Y5UuXdoad0nrd2EgH95UUl4rZsuH5EJyYX4hF95UUl4rZsuFEvmQfJg/zJYLpZKdFyTz5UNyYdHmwhJdSHRwcFBAQICioqJstkdFRSkwMLCIoio4vr6+8vT0tOlvcnKytmzZYu1vQECA7O3tbdrExsbqxx9/vGt/JoZhaMiQIVqxYoU2btwoX19fm/0ltd8ZMQxDSUlJJbbP7dq108GDB7V//37ro2nTpurdu7f279+vWrVqlch+FwbyYcl4rZAPbyIXkgtzi1xYMl4r5MK/kQ/Jh7lhtlwoldy8QD68iVxYyLkwR0uzFENpy9rPmTPHOHTokBESEmKUKVPGOH78eFGHlisXL1409u3bZ+zbt8+QZEydOtXYt2+fceLECcMwbi757e7ubqxYscI4ePCg8eyzz2a45He1atWM9evXG3v37jUeeeSRu3qp81deecVwd3c3Nm/ebMTGxlofV65csbYpif0ePXq0sXXrVuPYsWPGgQMHjDfffNMoVaqUERkZaRhGyexzRm5djcowzNPvgkA+LP6vFTPmQ3LhTeTC/EMuLP6vFTPmQsMgH6YhH+aPkpYLDYN8aJZ8SC68qShzYYkvJBqGYXzyySdGjRo1DAcHB6NJkybWpdCLo02bNhmS0j369u1rGMbNZb/HjRtneHp6Go6OjsbDDz9sHDx40OYcV69eNYYMGWJUqFDBcHZ2Nrp06WLExMQUQW+yJ6P+SjLmzp1rbVMS+92vXz/r67Zy5cpGu3btrMnRMEpmnzNye4I0S78LCvmweL9WzJgPyYU3kQvzF7mweL9WzJgLDYN8mIZ8mH9KUi40DPKhWfIhufCmosyFFsMwjJyNYQQAAAAAAABgNiX6HokAAAAAAAAA8geFRAAAAAAAAABZopAIAAAAAAAAIEsUEgEAAAAAAABkiUIiAAAAAAAAgCxRSAQAAAAAAACQJQqJAAAAAAAAALJEIREAAAAAAABAligkAgAAAAAAAMgShUQAAAAAAAAAWaKQCAAAAAAAACBLFBIBAAAAAAAAZOn/AZL4mNDksLBZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â 4. Main training loop\n",
    "def train_loop(history, data, model, loss_fn, optimizer, epochs):\n",
    "    \"\"\"Full training loop over the dataset for several epochs.\"\"\"\n",
    "    for t in range(epochs):\n",
    "        start_epoch = time.time()  # time the epoch\n",
    "        print(f\"Epoch {t+1: 3d}/{epochs}:\", end=\"\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss = train_step(data, model, loss_fn, optimizer)\n",
    "        print(f\" loss: {train_loss:.4f}\", end=\"\")\n",
    "\n",
    "        # Trainin accuracy\n",
    "        acc_pos, acc_neg = get_train_accuracy(data, model)\n",
    "        print(f\" - acc-pos: {acc_pos:.1f}% - acc-neg: {acc_neg:.1f}%\", end=\"\")\n",
    "\n",
    "        # Validation\n",
    "        valid_roc, valid_ap = test(data, model, data['valid_edges'], data['valid_edges_neg'])\n",
    "        print(f\" - val-roc: {valid_roc:.4f} - val-ap: {valid_ap:.4f}\", end=\"\")\n",
    "            \n",
    "        # Log\n",
    "        history['epoch'] += 1\n",
    "        history['loss'].append(train_loss)\n",
    "        history['acc-pos'].append(acc_pos)\n",
    "        history['acc-neg'].append(acc_neg)\n",
    "        history['val-roc'].append(valid_roc)\n",
    "        history['val-ap'].append(valid_ap)\n",
    "        print(f\" ({time.time() - start_epoch:.1f}s/epoch)\")\n",
    "    print(\"Done!\")\n",
    "    return history\n",
    "\n",
    "history = train_loop(history, data, model, loss_fn, optimizer, epochs)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4,  figsize=(16, 4))\n",
    "axs[0].plot(history['loss'])\n",
    "axs[0].set_title('Training loss')\n",
    "axs[1].plot(history['acc-pos'], label='positive edges')\n",
    "axs[1].plot(history['acc-neg'], label='negative edges')\n",
    "axs[1].set_title('Training accuracy')\n",
    "axs[1].legend()\n",
    "axs[2].plot(history['val-roc'])\n",
    "axs[2].set_title('Validation ROC-AUC')\n",
    "axs[3].plot(history['val-ap'])\n",
    "axs[3].set_title('Validation Average Precision');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade573d7-7657-4638-8dea-b9a26b42636f",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "Once we have our best model based on the performance on the *validation* set, we can do a final evaluation on a held-out set to get an idea of its predicting performance on new unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b726ffb-8a31-44f9-a5d6-770f1d332dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_roc, test_ap = test(...)  # TODO\n",
    "print(f\"Test ROC-AUC: {test_roc:.4f}\")\n",
    "print(f\"Test AP: {test_ap:.4f}\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "fig = plot_roc_curve(...)  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b509fd65",
   "metadata": {},
   "source": [
    "Let's try to visualize the prediction of our network on subsets of the graph.\n",
    "\n",
    "You can play with `threshold`, the value above which we consider an edge to be predicted. By lowering it, we recover more edges (True Positives), at the risk of more False Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # number of node to keep for the visualization, keep it small for better visualization\n",
    "threshold = 0.9  # threshold for edge prediction\n",
    "\n",
    "def plot_pred_subgraph(graph, N, data, model, threshold=0.5):\n",
    "    \"\"\"Plot a subgraph with true and predicted edges.\"\"\"\n",
    "    # Ground-truth\n",
    "    nodelist = np.random.permutation(graph.nodes())[:N]\n",
    "    subgraph = graph.subgraph(nodelist).copy()\n",
    "    subgraph.remove_edges_from(list(nx.selfloop_edges(subgraph)))  # remove self-loops for visualization\n",
    "    subgraph.remove_nodes_from(list(nx.isolates(subgraph)))  # remove isolated nodes for visualization\n",
    "    print(subgraph)\n",
    "    pos = nx.spring_layout(subgraph, iterations=25)\n",
    "\n",
    "    # Predict edges\n",
    "    model.eval()\n",
    "    pred_adj = model(data['x_train'], data['adj_train']).detach()\n",
    "    pred_adj = torch.sigmoid(pred_adj) > threshold\n",
    "    pred_edges = get_edge_index(pred_adj.detach().numpy())\n",
    "    # Build a subgraph from the predicted edges\n",
    "    subgraph_pred = nx.Graph()\n",
    "    subgraph_pred.add_nodes_from(subgraph.nodes())\n",
    "    nodes, subnodes = list(graph.nodes), set(subgraph.nodes())\n",
    "    subgraph_pred.add_edges_from([(nodes[pred_edges[0][i]], nodes[pred_edges[1][i]]) for i in range(pred_edges.shape[1]) \n",
    "                                if nodes[pred_edges[0][i]] in subnodes and nodes[pred_edges[1][i]] in subnodes])\n",
    "    subgraph_pred.remove_edges_from(list(nx.selfloop_edges(subgraph_pred)))  # remove self-loops for visualization\n",
    "    print(f\"{len(subgraph_pred.edges)} predicted edges\")\n",
    "\n",
    "    # Plot the subgraphs with GT and predicted edges\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    fig.suptitle('Sub-graph of the yeast protein-protein interaction network')\n",
    "    axs[0].set_title('Ground truth')\n",
    "    axs[1].set_title('Prediction')\n",
    "    for edge_set, color in [(train_edges, 'C0'), (valid_edges, 'C1'), (test_edges, 'C2')]:\n",
    "        edgelist = subgraph.edges(np.array(graph.nodes())[edge_set[0]])\n",
    "        nx.draw_networkx(subgraph, pos, edgelist=edgelist, ax=axs[0], with_labels=False, node_size=5, node_color='black', edge_color=color, width=1., alpha=0.5)\n",
    "    nx.draw_networkx(subgraph_pred, pos, ax=axs[1], with_labels=False, node_size=5, node_color='black', edge_color='C3', width=1., alpha=0.5)\n",
    "    axs[0].legend([Line2D([], [], color=c, linewidth=1., alpha=0.5) for c in ('C0', 'C1', 'C2')], ['train', 'validation', 'test'])\n",
    "    axs[1].legend([Line2D([], [], color='C3', linewidth=1., alpha=0.5)], ['prediction']);\n",
    "    return fig\n",
    "\n",
    "fig = plot_pred_subgraph(graph, N, data, model, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b082c2",
   "metadata": {},
   "source": [
    "## 6. GraphSAGE\n",
    "\n",
    "Let's now try to implement GraphSAGE, which generalizes the architecture above to other types of neighbors aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650877fb",
   "metadata": {},
   "source": [
    "### 6.1. Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6151d890",
   "metadata": {},
   "source": [
    "Start by the graph convolutional layer below. It should take as argument an arbitrary aggregation method.\n",
    "\n",
    "The equation for the GraphSAGE layer is:\n",
    "$$\n",
    "\\mathbf{h}_v^{(l+1)} = \\sigma\\left( \\mathbf{W}_l \\cdot \\mathrm{CONCAT} \\left[\\mathbf{h}_v^{(l)}, \\mathrm{AGG} \\left(\\left\\{\\mathbf{h}_u^{(l)}, \\forall u\\in N(v) \\right\\}\\right) \\right] \\right),\n",
    "$$\n",
    "where $v$ index the node, $l$ the layer, $\\mathbf{h}$ are the node embeddings, $\\sigma$ is a non-linearity, $N(v)$ is the set of neighbor of node $v$, and $\\mathbf{W}$ is the trainable weight matrix of the layer. $\\mathrm{CONCAT}$ is the concatenation operation, while $\\mathrm{AGG}$ is an arbitrary aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEConv(nn.Module):\n",
    "    \"\"\"GraphSAGE convolutional layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, aggregation, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the GraphSAGE convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            out_features (int): number of output node features.\n",
    "            aggregation (nn.Module or callable): aggregation function to apply, as x_agg = aggegration(x, adj).\n",
    "            activation (nn.Module or callable): activation function to apply. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...  # TODO\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, typically sparse, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        ...  # TODO\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449b3c8",
   "metadata": {},
   "source": [
    "Now, implement some aggregation functions for the GraphSAGE layers:\n",
    "1. `MeanAggregation`: average the neighbors, same as above,\n",
    "2. `SumAggregation`: sum the neighbors,\n",
    "3. `SqrtDegAggregation`: divide the sum of the neighbors by the square-root of the node degrees:  $$\\mathrm{AGG} \\left(\\left\\{\\mathbf{h}_u^{(l)}, \\forall u\\in N(v) \\right\\}\\right) = \\sum_{u\\in N(v)} \\frac{\\mathbf{h}_u^{(l)}}{\\sqrt{\\mathrm{deg}(v)}\\sqrt{\\mathrm{deg}(u)}},$$\n",
    "4. `MaxPoolAggregation`: (optional) element-wise maximum over the neighbors. This is complex to code in pure PyTorch, so in this exercise we will implement it by simply looping over the nodes (even if it is slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: all these aggregations take as input the node embedding x, adjacency matrix adj,\n",
    "# and return the aggregated node embedding x_agg.\n",
    "\n",
    "class MeanAggregation(nn.Module):\n",
    "    \"\"\"Aggregate node features by averaging over the neighborhood.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        ...  # TODO\n",
    "        return ...\n",
    "    \n",
    "class SumAggregation(nn.Module):\n",
    "    \"\"\"Aggregate node features by summing over the neighborhood.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        ...  # TODO\n",
    "        return ...\n",
    "\n",
    "class SqrtDegAggregation(nn.Module):\n",
    "    \"\"\"Aggregate node features by summing over the neighborhood and normalizing by the degrees.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        ...  # TODO\n",
    "        return ...\n",
    "    \n",
    "class MaxPoolAggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregate node features by taking the maximum over the transformed neighborhood.\n",
    "\n",
    "    Note: this is complicated to implement in pure PyTorch, so we will do a naive loop.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        ...  # TODO\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8a192",
   "metadata": {},
   "source": [
    "Finally, build the whole graph network. Similarly as for the above GNN, it will have graphSAGE convolution layers, followed by a prediction head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd54cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"GraphSAGE Neural Network for edge prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, num_features, conv_dims, aggregation, activation, dropout=0.):\n",
    "        \"\"\"\n",
    "        Initialize the GraphSAGE model for edge prediction.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Number of input node features.\n",
    "            conv_dims (list of int): Number of hidden features in the convolution layers.\n",
    "            aggregation (nn.Module or callable): Aggregation function to apply.\n",
    "            activation (nn.Module or callable): Activation function to apply.\n",
    "            dropout (float): Dropout probability. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...  # TODO\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform forward pass for edge prediction.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, num_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, typically sparse, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted edge probabilities for each pair of nodes, shape (num_nodes, num_nodes).\n",
    "        \"\"\"\n",
    "        ...  # TODO\n",
    "        return .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b63d53",
   "metadata": {},
   "source": [
    "### 6.2. Training and evaluation\n",
    "Similarly to above, let's train this model and evaluate its performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74153968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters (feel free to change these)\n",
    "learning_rate = 1e-2\n",
    "epochs = 400\n",
    "conv_dims = [256, 128]\n",
    "aggregation = MeanAggregation()\n",
    "activation = nn.LeakyReLU()\n",
    "dropout = 0.1\n",
    "\n",
    "# 1. Data\n",
    "# We already loaded and prepared the data above\n",
    "\n",
    "# 2. Model\n",
    "modelSAGE = GraphSAGE(num_features=D_NODES, conv_dims=conv_dims, aggregation=aggregation, activation=activation, dropout=dropout)\n",
    "#print(modelSAGE)\n",
    "\n",
    "# 3. Initialize the loss function and optimizer\n",
    "# We weight the samples to compensate for the class imbalance (there are very few edges compared to non-edges)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "optimizer = torch.optim.Adam(...)  # TODO\n",
    "\n",
    "history = {\"epoch\": 0, \"loss\": [], \"acc-pos\": [], \"acc-neg\": [], \"val-roc\": [], \"val-ap\": []}\n",
    "#Â 4. Main training loop\n",
    "history = train_loop(history, data, modelSAGE, loss_fn, optimizer, epochs)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4,  figsize=(16, 4))\n",
    "axs[0].plot(history['loss'])\n",
    "axs[0].set_title('Training loss')\n",
    "axs[1].plot(history['acc-pos'], label='positive edges')\n",
    "axs[1].plot(history['acc-neg'], label='negative edges')\n",
    "axs[1].set_title('Training accuracy')\n",
    "axs[1].legend()\n",
    "axs[2].plot(history['val-roc'])\n",
    "axs[2].set_title('Validation ROC-AUC')\n",
    "axs[3].plot(history['val-ap'])\n",
    "axs[3].set_title('Validation Average Precision');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_roc, test_ap = test(...)  # TODO\n",
    "print(f\"Test ROC-AUC: {test_roc:.4f}\")\n",
    "print(f\"Test AP: {test_ap:.4f}\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "fig = plot_roc_curve(...)  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # number of node to keep for the visualization, keep it small for better visualization\n",
    "threshold = 0.9  # threshold for edge prediction\n",
    "\n",
    "fig = plot_pred_subgraph(graph, N, data, modelSAGE, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834e172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
