\documentclass[10pt,twocolumn]{article}

% Page layout
\usepackage[a4paper,margin=1in]{geometry}

% Font settings (choose any font package you prefer)
\usepackage{mathptmx}  % Times New Roman font
\usepackage{fontspec}

% Hyperlinks
\usepackage{hyperref}

% Colors
\usepackage{xcolor}

% Title and Author information
\title{\vspace{-2cm}Graph ML for Chemical Compound Analysis}
\author{Ludek Cizinsky (ludek.cizinsky@epfl.ch)}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}
This report investigates the use of Graph Neural Networks
for mutagenicity prediction of chemical compounds. 
The mutagenicity of a chemical compound is a binary property
indicating whether the compound is likely to cause mutations
in living organisms. The prediction of mutagenicity is an
important task in drug discovery, as mutagenic compounds
are likely to be carcinogenic. The dataset used for this task is the 
\textit{MUTAG} dataset \cite{mutag}, which contains 188 chemical compounds.
The report first focuses on building optimal Graph Neural Network using
only node features. Then, the report investigates the use of edge features
as a way to improve the performance of the model. All experiments are
with associated code are available at \href{https://github.com/ludekcizinsky/epfl-cs502/tree/main/coursework/homeworks/hw2}{\textcolor{blue}{Github}}. 

\section{Dataset}\label{sec:dataset}
The \textit{MUTAG} dataset \cite{mutag} contains 188 chemical compounds with
binary labels indicating whether the compound is mutagenic or not. In total,
there is 63 mutagenic and 125 non-mutagenic compounds. Each compound is represented
as a graph. Each node and edge has is associated with one hot encoded feature vector
indicating type of the node (atom) and type of the edge (bond). It is important to note
that the labels are unvevenly distributed, with two thirds of the compounds being mutagenic.
The skew in the labels is favorable for the downstream task, however, the trained
models might be as a result more biased towards predicting mutagenic compounds.


The feature vectors were mapped to double precision floating point numbers to
increase the numerical stability of the model. In addition, based on the provided
edges indices, the adjacency matrix was constructed.


% TODO: maybe define what is overall the goal --> build nice features
% TODO: add figure showing the whole pipeline
% TODO: add the explanation of global softmaxing
\section{Methodology}\label{sec:methodology}
On a high level, GNN's goal is to transform the given
graph into a vector representation, which can then be used
for the downstream task, in this case binary classification.

The first part of the experiment focuses on building a GNN purely
based on transformation of the initial node features. The experiment compares the performance of the
three types of convolutional layers: Normal Graph Convolution (\texttt{NormConv}), 
GraphSAGE (\texttt{SAGEConv}) and Graph Attention (\texttt{GATConv}).

\texttt{GCNConv} first computes representation of each node by aggregating
over the features of its neighbors. Then, the representation is transformed
using a linear layer, and linearly transformed representation of the node is added.
The \texttt{SAGEConv} extends the \texttt{GCNConv} by allowing the user to 
choose the aggregation function, i.e., one can for instance use \texttt{max pooling}
strategy to obtain the aggregate instead of mean. Finally, the \texttt{GATConv} aggregates
the neighbors using attention weights, which indicate the importance of each neighbor 
for the representation of the node. The attention weights are learned during training.

In the second part of the experiment, the edge features are used to improve the performance
of the model. In general, every node convolution layer is now followed by en edge convolution.
This way the model can learn to incorporate the edge features into the node representation.
Two types of edge convolutions are used: Graph Edge Sum \texttt{ESUMConv} and Graph Edge Attention
(\texttt{EATTConv}).

The idea behind \texttt{ESUMConv} is that each node's representation should be updated 
by sum of the edge features that are associated with the given node. Thus, the node's
representation no longer depends on the type of its neighbors, but also in what way 
is the node connected to its neighbors. The \texttt{EATTConv} extends the \texttt{ESUMConv}
idea by using attention weights to indicate the importance of each edge feature for the

\section{Experiments}\label{sec:experiments}


\section{Results}

\section{Discussion}

\section{Conclusion}

\newpage
\section{References}
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}