{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 3 - Ludek Cizinsky, 377297"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this homework, we are going to work with the transformer. There are three parts of this homework.\n",
        "\n",
        "- In the first part, we are going to implement **positional encoding** and **self-attention**  and test them on a simple text dataset which contains around 100 sentences. We will use a small transformer in this task.\n",
        "\n",
        "- In the second part, we will detect promoters from the DNA sequences. The main difference compared to the previous task is to tokenize the DNA sequence. Thus, our task here is to build the **tokenizer** to tokenize the DNA sequence. For the model, we will continue using the small transformer.\n",
        "\n",
        "- In the third part, we will use a **foundation model** DNABERT to perform promoter detection. In this part, you do not need to train the transformer. Instead, you need to find and load the correct pre-trained model and then use it to get the embedding of the DNA sequence. Then, you will build a simple classifier to perform promoter detection based on the DNA embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Initialization\n",
        "\n",
        "Import the packages you are going to use here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from types import SimpleNamespace\n",
        "from utils import data, evaluation, models, visualization, text_exercise\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from bhtsne import tsne\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seeds\n",
        "seed = 128\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Positional Encoding and Self-Attention (7 pts)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Sinusoidal Positional Encoding (1 pt)\n",
        "\n",
        "In this section, you are going to implement the sinusoidal positional encoding. The formula is as the following:\n",
        "\n",
        "<div>\n",
        "<img src=\"./imgs/positional embedding.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "where $t$ is the desired position in the input and $\\mathsf{\\omega}_k$ follows:\n",
        "\n",
        "<div>\n",
        "<img src=\"./imgs/omega.png\" width=\"200\"/>\n",
        "</div>\n",
        "\n",
        "To see the details of sinusoidal positional encoding, you can check this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, max_position_embeddings, hidden_size, device):\n",
        "        super().__init__()\n",
        "\n",
        "        '''Construct the positional embedding.\n",
        "\n",
        "        Args:\n",
        "            max_position_embeddings (int): maximum length of the input - related to t in the previous formula\n",
        "            hidden_size (int): encoding dimension - d in the previous formula \n",
        "        '''\n",
        "\n",
        "        # Account for odd hidden size dimension by assuming first we have even dimension\n",
        "        # In the last step, we will just drop the last column\n",
        "        is_odd = hidden_size % 2\n",
        "        if is_odd:\n",
        "            hidden_size += 1 \n",
        "\n",
        "        # Compute weights for the positional embedding\n",
        "        k = torch.arange(hidden_size//2, dtype=torch.float32, device=device)\n",
        "        w = 1 / (10000 ** (2*k/hidden_size))\n",
        "\n",
        "        # Compute the positional embedding\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        sin = torch.sin(torch.outer(t, w)) # n x d/2\n",
        "        cos = torch.cos(torch.outer(t, w))  # n x d/2\n",
        "\n",
        "        # Put the sin and cos together\n",
        "        self.positional_embedding = torch.zeros((max_position_embeddings, hidden_size), device=device)\n",
        "        self.positional_embedding[:, 0::2] = sin\n",
        "        self.positional_embedding[:, 1::2] = cos\n",
        "\n",
        "        # Drop the last column\n",
        "        if is_odd:\n",
        "            self.positional_embedding = self.positional_embedding[:, :-1]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.positional_embedding\n",
        "    \n",
        "    def embedding(self):\n",
        "        return self.positional_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, you can visualize your positional encoding. If you implement everything correctly, you can get a figure that is similar to Figure 2 in this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76e2ba7ef4e646298c7822e0f81cd132",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Dropdown(description='Sequence Length', index=2, options=(100, 500, 1000, 5000), style=Descriptâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88fb59ded0894b2194a5b17dadf81083",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_embedding, dimension_selector, max_len_selector = visualization.display_positional_encoding(PositionalEmbedding)\n",
        "ui = widgets.HBox([max_len_selector, dimension_selector])  \n",
        "out = widgets.interactive_output( visualize_embedding, {'max_len': max_len_selector, 'dimension': dimension_selector})\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Self-Attention Mechanism (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, you are going to implement the self-attention mechanism. Please check the section 'Self-Attention in Detail' in this [link](https://jalammar.github.io/illustrated-transformer/) for the details of self-attention mechanism. (We encourage you to carefully go through the link since it is a very good tutorial for transformer.)\n",
        "\n",
        "The specific steps will be provided in the comments of the following code. (The steps are only for reference. You do need to follow the steps if you have a better way to implement it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "            )\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): input sequence (shape (batch_size, sequence_length, hidden_size))\n",
        "            attention_mask (torch.Tensor): attention mask\n",
        "            head_mask (torch.Tensor): head mask\n",
        "            encoder_hidden_states (torch.Tensor): encoder hidden states of shape\n",
        "            encoder_attention_mask (torch.Tensor): encoder attention mask of shape\n",
        "        \n",
        "        Returns:\n",
        "            torch.Tensor: context layer of shape (batch_size, sequence_length, hidden_size)\n",
        "            torch.Tensor: attention probabilities (optinal)\n",
        "        \"\"\"\n",
        "\n",
        "        # Resulting shape: (batch_size, sequence_length, all_head_size)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        # Resulting shape: (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Resulting shape: (batch_size, num_attention_heads, sequence_length, sequence_length)\n",
        "        # For each input token, we have unnormalized scores for all other tokens in the input sequence\n",
        "        attention_scores = query_layer @ key_layer.transpose(-1, -2) / math.sqrt(self.attention_head_size)\n",
        "        \n",
        "        # Explanation of attention_mask: https://lukesalamone.github.io/posts/what-are-attention-masks/\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # (optional) Apply head mask if provided\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        # Reweight each token embeddings by the attention scores\n",
        "        # Shape: (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        context_layer = attention_probs @ value_layer\n",
        "\n",
        "        # Concatenate all the attention heads together\n",
        "        head_outputs = torch.split(context_layer, 1, dim=1)\n",
        "        context_layer = torch.cat(head_outputs, dim=-1).squeeze(1)\n",
        "\n",
        "        # Get the output\n",
        "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test your implementation using simple text data! First, let's load the data.\n",
        "\n",
        "We use a small dataset in this homework for a shorter training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChatGPT generated text data about BERT\n",
        "text = text_exercise.get()\n",
        "sentences_df, vocab = data.to_sentence_df(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After loading the data, you can train your model. Here we train our model using masked token prediction.\n",
        "\n",
        "Hint: The final model accuracy should be higher than 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable model parameters: 64910\n",
            "Epoch: 0020 loss = 4.239664\n",
            "Epoch: 0040 loss = 3.614159\n",
            "Epoch: 0060 loss = 3.016250\n",
            "Epoch: 0080 loss = 2.497268\n",
            "Epoch: 0100 loss = 2.059983\n",
            "Epoch: 0120 loss = 1.624396\n",
            "Epoch: 0140 loss = 1.239146\n",
            "Epoch: 0160 loss = 0.875031\n",
            "Epoch: 0180 loss = 0.581011\n",
            "Epoch: 0200 loss = 0.371008\n",
            "Final model accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "text_max_len = 11\n",
        "\n",
        "text_config = SimpleNamespace(\n",
        "        vocab_size=len(vocab),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=text_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=1,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=1,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=False,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=\"cpu\"\n",
        "    )\n",
        "\n",
        "tokenizer = data.TextTokenizer(vocab)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(sentences_df, tokenizer, k=1, max_len=text_max_len, noise_rate=0.4)\n",
        "\n",
        "model = models.BertForMaskedLM(config=text_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    loss, outputs, attentions = model(\n",
        "        input_ids=input_ids,\n",
        "        token_type_ids=segment_ids,\n",
        "        masked_lm_labels=masked_lm_labels,\n",
        "        attention_mask=attention_masks\n",
        "    )\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"Final model accuracy: {evaluation.masked_label_accuracy(labels, labels_idx, outputs.data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Visualize Attention (1 pt)\n",
        "\n",
        "Here, you can visualize the self-attention. \n",
        "\n",
        "Question: Can you interpret the visualization of the self-attention?\n",
        "\n",
        "**Answer:** In each row, we have weights that denote the importance of each token in the input sequence for the current token (ith token). The darker the color, the more important the token is. Generally, we would expect the model to pay attention to tokens close to the current token, making the diagonal of the matrix darker. However, we observe that our model focuses attention on a few specific words rather arbitrarily (the column(s) appear dark). The most likely explanation for such behavior is the small size of the dataset. The model has likely memorized the training data and does not generalize well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fad0a23e2bc48e9829374c81d2f996b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='Sample:', options=(5, 8, 11, 13, 18, 24, 26, 29, 31, 35, 37, 38, 6â€¦"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "visualize_attention, sample_id_selector = visualization.display_attantion(attentions=attentions, input_ids=input_ids, tokenizer=tokenizer)\n",
        "widgets.interactive(visualize_attention, sample_id=sample_id_selector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Train on small Wikitext Dataset\n",
        "\n",
        "Here, you can **optionally** test your model on the smallest wikitext dataset. You should get an test accuracy around 0.4 after training 50 epochs.\n",
        "\n",
        "This part is only for you to test your code. You can choose to run it or not. It takes around 1 hour to train the model for 50 epochs on the smallest wikitext dataset with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text_exercise.train_wikitext(device, positional_embedding=PositionalEmbedding, attention=BertSelfAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Promoter detection (7 pts)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we detect promoter in DNA sequence.\n",
        "\n",
        "A promoter is a region of DNA upstream of a gene where relevant proteins (such as RNA polymerase and transcription factors) bind to initiate transcription of that gene. Promoter detection is to identify if there are promoter regions in the given DNA sequence. We have covered this in the lecture. (If you are interested in the promoter, you can check this [link](https://www.genome.gov/genetics-glossary/Promoter) for more details.)\n",
        "\n",
        "Here, we use a transformer and a classifier. The transformer first embeds the DNA sequences into features, and then the classifier detects the promoter based on the features.\n",
        "\n",
        "The main difference between text and DNA sequence is how to tokenize the sequence. Thus, you need to implement a tokenizer for the DNA sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. DNA Tokenizer (1 pts)\n",
        "\n",
        "Here, you will implement the DNA tokenizer the same as in DNABERT. Please check this [paper](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680) for implementation details. Also, you need to check the data type and shape for both input and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DNATokenizer(data.Tokenizer):\n",
        "    def __init__(self, k, vocab, unknown=\"[UNK]\"):\n",
        "        \"\"\"\n",
        "        K-mer tokenizer for DNA sequences.\n",
        "        Args:\n",
        "            k (int): k-mer size\n",
        "            vocab (list): list of tokens\n",
        "            unknown (str): unknown token\n",
        "        \"\"\"\n",
        "        super().__init__(vocab, unknown)\n",
        "        self.k = k\n",
        "\n",
        "    def _parse_text(self, text):\n",
        "        \"\"\"Splits the text into k-mers and adds special tokens to the start and end of the sequence.\n",
        "        Args:\n",
        "            text (str): text to be tokenized \n",
        "        Returns:\n",
        "            list: list of tokens \n",
        "        Notes:\n",
        "        [CLS] is denoting a special token that can be used later for classification, the idea behind\n",
        "        is that it should represent the whole sequence [SEP] is denoting the end of the sequence.\n",
        "        \"\"\"\n",
        "        n = len(text)\n",
        "        return ['[CLS]'] + [text[i:i + self.k] for i in range(n - self.k + 1)] + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Test BERT on DNA Sequence\n",
        "\n",
        "In this section, you will train BERT on DNA sequence to learn the embedding of DNA sequence. The code is provided below and you do not need to write anything.\n",
        "\n",
        "Hint: the final evaluation accuracy should be higher than 0.2.\n",
        "\n",
        "**Note for TA:** I put the training/eval code into functions so there is no code redundancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embedding_eval_loop(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the embedding model on the test data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): model\n",
        "        test_loader (DataLoader): test data loader\n",
        "        device (torch.device): device to use for the evaluation\n",
        "    \n",
        "    Returns:\n",
        "        float: average evaluation loss across the batches\n",
        "    \"\"\"\n",
        "    # Set the model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate data\n",
        "    total_eval_loss = 0\n",
        "    for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in test_loader:\n",
        "\n",
        "        # Evalate the batch\n",
        "        with torch.no_grad():\n",
        "            loss, _, _, _ = model(\n",
        "            input_ids=batch_input_ids.to(device),\n",
        "            token_type_ids=batch_segment_ids.to(device),\n",
        "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "            attention_mask=batch_attention_mask.to(device)\n",
        "            )\n",
        "            if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0 :\n",
        "                print(\"found patting\", batch_attention_mask.sum())\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_eval_loss = total_eval_loss / len(test_loader)\n",
        "\n",
        "    return avg_eval_loss\n",
        "\n",
        "def embeddings_train_loop(model, optimizer, train_loader, val_loader, device, epochs, verbose=True):\n",
        "    \"\"\"\n",
        "    Train the embedding model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): model\n",
        "        optimizer (optim.Optimizer): optimizer\n",
        "        train_loader (DataLoader): train data loader\n",
        "        val_loader (DataLoader): validation data loader\n",
        "        device (torch.device): device to use for the training\n",
        "        epochs (int): number of epochs to train\n",
        "        verbose (bool): if True, print progress information\n",
        "    \"\"\"\n",
        "\n",
        "    # Train the model for the given numeb of epochs\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Set the model to train mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # Iterate over the training batches\n",
        "        for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in train_loader:\n",
        "\n",
        "            # Reset gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            loss, _, _, _ = model(\n",
        "                input_ids=batch_input_ids.to(device),\n",
        "                token_type_ids=batch_segment_ids.to(device),\n",
        "                masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "                attention_mask=batch_attention_mask.to(device)\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        \n",
        "        # Calculate the average loss over all of the batches\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # Report progress\n",
        "    if verbose:\n",
        "        avg_eval_loss = embedding_eval_loop(model, val_loader, device)\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'eval cost =', '{:.6f}'.format(avg_eval_loss))\n",
        "\n",
        "def get_custom_embedding_model(model_path, config, train_loader, val_loader, device, epochs=50, verbose=True):\n",
        "    \"\"\"\n",
        "    Get the custom embedding model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): path to the model\n",
        "        config (SimpleNamespace): model configuration\n",
        "        train_loader (DataLoader): train data loader\n",
        "        val_loader (DataLoader): validation data loader\n",
        "        device (torch.device): device to use for the training\n",
        "        epochs (int): number of epochs to train\n",
        "        verbose (bool): if True, print progress information\n",
        "    \n",
        "    Returns:\n",
        "        nn.Module: trained model\n",
        "    \"\"\"\n",
        "\n",
        "    # Train the model if it does not exist yet\n",
        "    if not os.path.exists(model_path):\n",
        "\n",
        "        # Define the model and optimizer based on the config\n",
        "        print(\"Training the model...\")\n",
        "        model = models.BertForMaskedLM(config=config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention).to(device)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "        print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "        # Train the model\n",
        "        embeddings_train_loop(model, optimizer, train_loader, val_loader, device, epochs, verbose)\n",
        "\n",
        "        # Save the trained model\n",
        "        print(\"Saving the model...\")\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        torch.save(model, model_path)\n",
        "\n",
        "    # Load the trained model\n",
        "    else:\n",
        "        print(\"Loading the trained model...\")\n",
        "        model = torch.load(model_path)\n",
        "        print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Number of trainable model parameters: 118869\n"
          ]
        }
      ],
      "source": [
        "# Load raw data\n",
        "raw_training_data = data.load_csv(\"./data/train.csv\")\n",
        "raw_test_data = data.load_csv(\"./data/test.csv\")\n",
        "\n",
        "# Define the tokenizer\n",
        "kmer = 3\n",
        "mask_length = kmer\n",
        "VOCAB_3MER = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"AAA\", \"AAT\", \"AAC\", \"AAG\", \"ATA\", \"ATT\", \"ATC\", \"ATG\", \"ACA\", \"ACT\", \"ACC\", \"ACG\", \"AGA\", \"AGT\", \"AGC\", \"AGG\", \"TAA\", \"TAT\", \"TAC\", \"TAG\", \"TTA\", \"TTT\", \"TTC\", \"TTG\", \"TCA\", \"TCT\", \"TCC\", \"TCG\", \"TGA\", \"TGT\", \"TGC\", \"TGG\", \"CAA\", \"CAT\", \"CAC\", \"CAG\", \"CTA\", \"CTT\", \"CTC\", \"CTG\", \"CCA\", \"CCT\", \"CCC\", \"CCG\", \"CGA\", \"CGT\", \"CGC\", \"CGG\", \"GAA\", \"GAT\", \"GAC\", \"GAG\", \"GTA\", \"GTT\", \"GTC\", \"GTG\", \"GCA\", \"GCT\", \"GCC\", \"GCG\", \"GGA\", \"GGT\", \"GGC\", \"GGG\" ]\n",
        "tokenizer = DNATokenizer(k=kmer, vocab=VOCAB_3MER)\n",
        "\n",
        "# Define tokenization and data parameters\n",
        "dna_max_len = 300 # NB: change to account for the two special tokens\n",
        "batch_size = 128\n",
        "max_dna_mask = 100\n",
        "dataset_size = 1000\n",
        "\n",
        "# Load the data\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(raw_training_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks = data.generate_masked_data(raw_test_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the embedder model\n",
        "num_layers = 3\n",
        "num_heads = 6\n",
        "dna_config = SimpleNamespace(\n",
        "        vocab_size=len(VOCAB_3MER),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=dna_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=num_heads,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=num_layers,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=device\n",
        ")\n",
        "\n",
        "# Define the model path\n",
        "model_path = \"models/dna_embedder.pth\"\n",
        "\n",
        "# Get the embedder model\n",
        "dna_embedder = get_custom_embedding_model(model_path, dna_config, train_loader, test_loader, device, epochs=50, verbose=True)\n",
        "\n",
        "# Get the trained embeddings model accuracy, also get the last attention layer\n",
        "average_train_acc, _ = evaluation.model_masked_label_accuracy(dna_embedder, train_loader, device)\n",
        "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(dna_embedder, test_loader, device)\n",
        "print('Train Acc =', '{:.6f}'.format(average_train_acc), 'Eval Acc =', '{:.6f}'.format(average_test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Visualize the Attentions (1 pt)\n",
        "\n",
        "Here, you can visualize the self-attention. \n",
        "\n",
        "Question: compare the visualization to Section 1.3, what can you find here? How do you explain it?\n",
        "\n",
        "**Answer:** The main difference now is that we can observe a clear diagonal pattern for the majority of heads in each layer, indicating that the model is paying attention to tokens close to the current token. This change can likely be attributed to the training dataset size, which is 10 times larger in the latter case, making overfitting more challenging. However, for certain layers and heads, we still notice that the model is for instance focusing on tokens that are far away from the current token. This behavior aligns with our expectations, as we intend for each head to learn different features and, consequently, focus on different parts of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd9d23fe4de94f5eb723c270ed1c9687",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Dropdown(description='Sample:', options=(4, 13, 21, 25, 28, 30, 37, 38, 42, 45, 47, 50, 62, 77,â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a53d8a4ffa24b9b8b869a0fffa00026",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_attention, sample_id_selector, layer_selector, head_selector = visualization.display_multi_attantion(attentions=last_test_attention, tokenizer=tokenizer, input_ids=input_ids,  layers=range(1, num_layers+1),  heads=range(1, num_heads+1))\n",
        "ui = widgets.HBox([sample_id_selector, layer_selector, head_selector])  \n",
        "out = widgets.interactive_output(visualize_attention, {'sample_id': sample_id_selector, 'layer': layer_selector, 'head': head_selector})\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Use your pretrained model for promoter detection (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You already have the embeddings for the DNA sequence. Now, you are going to build a classifier based on the DNA embeddings. The classifier is to perform promoter detection. Specifically, the DNA sequence will be classified into *'contains promoter'* or *'does not contain promoter'*.\n",
        "\n",
        "Hint: \n",
        "- We now want to annotate data (get the label for each sample), not predict masked data anymore!\n",
        "- You can reuse some parts of the code in the previous sections, e.g. dataloader and training pipeline in Section 2.2.\n",
        "- If you implement the previous section correctly (the Eval Acc > 0.2 in Section 2.2), you already have an pre-trained object named 'model' of class models.BertForMaskedLM. You can directly use it.\n",
        "- The evaluation accuracy of this task should be around 0.6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "def save_embeddings(loader, model, path):\n",
        "    # Save the embeddings and labels as pytorch tensors\n",
        "    labels, embeddings = [], []\n",
        "    for data in tqdm(loader):\n",
        "        # Get the input \n",
        "        input_ids, input_labels = data\n",
        "\n",
        "        # Get the output\n",
        "        hidden_states = model(input_ids)[0]\n",
        "        \n",
        "        # Aggregate the embedding usig mean pooling\n",
        "        embedding_mean = torch.mean(hidden_states[0], dim=0)\n",
        "\n",
        "        # Add the embedding and label\n",
        "        embeddings.append(embedding_mean)\n",
        "        labels.append(input_labels)\n",
        "\n",
        "    # Convert to tensors\n",
        "    embeddings = torch.stack(embeddings)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    # Save\n",
        "    torch.save(embeddings, os.path.join(path, f\"x.pth\"))\n",
        "    torch.save(labels, os.path.join(path, f\"y.pth\"))\n",
        "\n",
        "def load_embeddings(path):\n",
        "    # Load the embeddings and labels\n",
        "    embeddings = torch.load(os.path.join(path, f\"x.pth\"))\n",
        "    labels = torch.load(os.path.join(path, f\"y.pth\"))\n",
        "    return embeddings, labels\n",
        "\n",
        "# Define custom torch dataset\n",
        "class EmbeddingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.embeddings[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "class DnaBertHead(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dimensions, dropout_rate=0.05, activation=nn.ReLU()):\n",
        "        \"\"\"Classification head which takes embedded DNA sequences and outputs a probability distribution over classes.\n",
        "        Args:\n",
        "            input_size (int): the size of the input\n",
        "            output_size (int): the size of the output\n",
        "            hidden_dimensions (list): a list of integers denoting the size of each hidden layer\n",
        "            dropout_rate (float): the dropout rate to use\n",
        "            activation (torch.nn.functional): the activation function to use\n",
        "        \"\"\"\n",
        "        super(DnaBertHead, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        for out_features in hidden_dimensions:\n",
        "            layers.append(nn.Linear(in_features, out_features))\n",
        "            layers.append(nn.Dropout(p=dropout_rate))\n",
        "            layers.append(activation)\n",
        "            in_features = out_features\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dimensions[-1], output_size))\n",
        "\n",
        "        # Put the layers together \n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): the input tensor of shape (batch_size, input_size)\n",
        "        Returns:\n",
        "            torch.Tensor: the output tensor of shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "def eval_loop(clf, test_loader, criterion, device, verbose=True):\n",
        "\n",
        "    # Validation\n",
        "    total_test_loss = 0\n",
        "    preds = []\n",
        "    labs = []\n",
        "    clf.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_embeddings, batch_labels in test_loader:\n",
        "\n",
        "            # Preprocess the labels\n",
        "            batch_embeddings, batch_labels = batch_embeddings.to(device), batch_labels.squeeze(1).float().to(device)\n",
        "\n",
        "            # Get the output of the classifier\n",
        "            batch_output = clf(batch_embeddings)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "\n",
        "            # Add prediction and labels\n",
        "            batch_output = torch.sigmoid(batch_output)\n",
        "            bpred = (batch_output > 0.5).float().tolist()\n",
        "            blabs = batch_labels.tolist()\n",
        "            preds.extend(bpred)\n",
        "            labs.extend(blabs)\n",
        "\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "    # Evaluate the epoch\n",
        "    preds, labs = np.array(preds), np.array(labs)\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    test_acc = np.mean(preds == labs)\n",
        "\n",
        "    # Print the epoch results\n",
        "    if verbose:\n",
        "        print('Test cost =', '{:.6f}'.format(avg_test_loss), 'test acc =', '{:.6f}'.format(test_acc))\n",
        "\n",
        "    return avg_test_loss, test_acc\n",
        "\n",
        "def train_loop(clf, train_loader, val_loader, criterion, optimizer, device, epochs=100, patience=5, verbose=True):\n",
        "\n",
        "    # Initialize the early stopping\n",
        "    best_val_loss = np.inf\n",
        "    best_val_acc = 0\n",
        "    best_epoch = 0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.train()\n",
        "    for epoch in range(epochs):\n",
        "            \n",
        "            # Training\n",
        "            total_train_loss = 0\n",
        "            preds = []\n",
        "            labs = []\n",
        "            for batch_embeddings, batch_labels in train_loader:\n",
        "    \n",
        "                # Preprocess the labels\n",
        "                batch_embeddings, batch_labels = batch_embeddings.to(device), batch_labels.squeeze(1).float().to(device)\n",
        "    \n",
        "                # Reset the gradients\n",
        "                optimizer.zero_grad()\n",
        "    \n",
        "                # Get the output of the classifier\n",
        "                batch_output = clf(batch_embeddings)\n",
        "    \n",
        "                # Compute the loss\n",
        "                loss = criterion(batch_output, batch_labels)\n",
        "    \n",
        "                # Add prediction and labels\n",
        "                batch_output = torch.sigmoid(batch_output)\n",
        "                bpred = (batch_output > 0.5).float().tolist()\n",
        "                blabs = batch_labels.tolist()\n",
        "                preds.extend(bpred)\n",
        "                labs.extend(blabs)\n",
        "    \n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item()\n",
        "    \n",
        "            # Evaluate the epoch\n",
        "            preds, labs = np.array(preds), np.array(labs)\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "            train_acc = np.mean(preds == labs)\n",
        "    \n",
        "            # Validation\n",
        "            avg_val_loss, val_acc = eval_loop(clf, val_loader, criterion, device, verbose=False)\n",
        "    \n",
        "            # Print the epoch results\n",
        "            if verbose:\n",
        "                print('Epoch:', '%04d' % (epoch + 1), \n",
        "                      'train cost =', '{:.6f}'.format(avg_train_loss), 'train acc =', '{:.6f}'.format(train_acc), \n",
        "                      'val cost =', '{:.6f}'.format(avg_val_loss), 'val acc =', '{:.6f}'.format(val_acc))\n",
        "            \n",
        "            # Check if the validation loss has improved\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                best_val_acc = val_acc\n",
        "                best_epoch = epoch\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "            \n",
        "            # Check if we should stop the training\n",
        "            if epochs_without_improvement == patience:\n",
        "                break\n",
        "\n",
        "    # Print the best epoch results\n",
        "    if verbose:\n",
        "        print('Best epoch:', '%04d' % (best_epoch + 1), 'val cost =', '{:.6f}'.format(best_val_loss), 'val acc =', '{:.6f}'.format(best_val_acc))\n",
        "    \n",
        "    return best_val_loss, best_val_acc\n",
        "\n",
        "# Visualize the embeddings\n",
        "def visualize_embeddings(emb, lab, seed=seed):\n",
        "\n",
        "    # Apply Standard Scaling\n",
        "    emb = (emb - emb.mean(dim=0)) / emb.std(dim=0)\n",
        "\n",
        "    # Reduce the dimensionality of the embeddings\n",
        "    pca = PCA(n_components=100)\n",
        "    reduced_data = pca.fit_transform(emb)\n",
        "\n",
        "    # Apply t-SNE on the reduced data\n",
        "    embedded_data = tsne(reduced_data, rand_seed=seed)\n",
        "\n",
        "    # Visualize the result\n",
        "    plt.scatter(embedded_data[:, 0], embedded_data[:, 1], c=lab, cmap=plt.cm.get_cmap(\"jet\", 2), alpha=0.5)\n",
        "    plt.title(f't-SNE Visualization n={n}')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data (without masks indeed)\n",
        "input_ids, labels = data.generate_labeled_data(raw_training_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "test_input_ids, test_labels = data.generate_labeled_data(raw_test_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "\n",
        "# Convert labels to float\n",
        "labels = labels.float()\n",
        "test_labels = test_labels.float()\n",
        "\n",
        "# Define data loaders\n",
        "train_dataset = TensorDataset(input_ids, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model first\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "if not os.path.exists('models/promoter_clf.pth'):\n",
        "\n",
        "  print(\"Training the model...\")\n",
        "  # Freeze the weights of the embedder model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Define the classifier\n",
        "  clf = BinaryClassifier(input_size=60, hidden_size=30, output_size=1, embedder=model).to(device)\n",
        "  clf_optimizer = optim.AdamW(clf.parameters(), lr=1e-2)\n",
        "\n",
        "  # Train the classifier\n",
        "  clf.train()\n",
        "  for epoch in range(50):\n",
        "\n",
        "    # Training\n",
        "    total_train_loss = 0\n",
        "    preds = []\n",
        "    labs = []\n",
        "    for batch_input_ids, batch_labels in train_loader:\n",
        "\n",
        "      # Reset the gradients\n",
        "      clf_optimizer.zero_grad()\n",
        "\n",
        "      # Get the output of the classifier\n",
        "      batch_output = clf(batch_input_ids)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = criterion(batch_output.to(device), batch_labels.to(device))\n",
        "\n",
        "      # Add prediction and labels\n",
        "      batch_output = F.sigmoid(batch_output)\n",
        "      bpred = (batch_output > 0.5).float().tolist()\n",
        "      blabs = batch_labels.tolist()\n",
        "      preds.extend(bpred)\n",
        "      labs.extend(blabs)\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      clf_optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "    # Evaluate the epoch\n",
        "    preds, labs = np.array(preds), np.array(labs)\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_acc = np.mean(preds == labs)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'train acc =', '{:.6f}'.format(train_acc))\n",
        "\n",
        "  # Save the trained model\n",
        "  os.makedirs('models', exist_ok=True)\n",
        "  torch.save(clf, 'models/promoter_clf.pth')\n",
        "\n",
        "# Load the trained model\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  clf = torch.load('models/promoter_clf.pth')\n",
        "\n",
        "# Evaluation\n",
        "total_test_loss = 0\n",
        "preds = []\n",
        "labs = []\n",
        "clf.eval()\n",
        "for batch_input_ids, batch_labels in test_loader:\n",
        "\n",
        "  # Get the output of the classifier\n",
        "  batch_output = clf(batch_input_ids)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = criterion(batch_output.to(device), batch_labels.to(device))\n",
        "\n",
        "  # Add prediction and labels\n",
        "  batch_output = F.sigmoid(batch_output)\n",
        "  bpred = (batch_output > 0.5).float().tolist()\n",
        "  blabs = batch_labels.tolist()\n",
        "  preds.extend(bpred)\n",
        "  labs.extend(blabs)\n",
        "\n",
        "  total_test_loss += loss.item()\n",
        "\n",
        "preds, labs = np.array(preds), np.array(labs)\n",
        "avg_test_loss = total_test_loss / len(test_loader)\n",
        "test_acc = np.mean(preds == labs)\n",
        "print('Test cost =', '{:.6f}'.format(avg_test_loss), 'test acc =', '{:.6f}'.format(test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5. Additional question (1 pt)\n",
        "\n",
        "Now we change mask_length = 1 (already changed, you do not need to implement anything).\n",
        "Let's run the code below and check the accuracy.\n",
        "\n",
        "Question: What is the final masked token prediction accuracy? How do you explain this?\n",
        "\n",
        "**Answer**: The final prediction accuracy is high, reaching 92%. This robust performance is attributed to the simplicity of the prediction task when masking only one k-mer. To illustrate, consider the sequence ABCDEF, tokenized into 3-mers as [ABC, BCD, CDE, DEF]. If we mask a single k-mer, let's say BCD, we retain access to the surrounding k-mers, simplifying the prediction task. On the other hand, masking multiple k-mers, such as BCD and CDE, introduces additional complexity. In this scenario, each masked k-mer has access to only one direct neighbor, making the prediction task inherently more challenging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmer = 3\n",
        "mask_length = 1\n",
        "\n",
        "dna_max_len = 300\n",
        "batch_size = 128\n",
        "max_dna_mask = 100\n",
        "dataset_size = 1000\n",
        "num_layers = 3\n",
        "num_heads = 6\n",
        "dna_config = SimpleNamespace(\n",
        "        vocab_size=len(VOCAB_3MER),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=dna_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=num_heads,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=num_layers,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "tokenizer = DNATokenizer(k=3, vocab=VOCAB_3MER)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(raw_training_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks = data.generate_masked_data(raw_test_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "if not os.path.exists('models/bert_shortmask.pth'):\n",
        "  print(\"Training the model...\")\n",
        "  model = models.BertForMaskedLM(config=dna_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention).to(device)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "  for epoch in range(50):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss, outputs, hidden_states, _ = model(\n",
        "          input_ids=batch_input_ids.to(device),\n",
        "          token_type_ids=batch_segment_ids.to(device),\n",
        "          masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "          attention_mask=batch_attention_mask.to(device)\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      model.eval()\n",
        "      total_eval_loss = 0\n",
        "      for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in test_loader:\n",
        "        with torch.no_grad():\n",
        "          loss, outputs, hidden_states, _ = model(\n",
        "            input_ids=batch_input_ids.to(device),\n",
        "            token_type_ids=batch_segment_ids.to(device),\n",
        "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "            attention_mask=batch_attention_mask.to(device)\n",
        "          )\n",
        "          if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0 :\n",
        "            print(\"found patting\", batch_attention_mask.sum())\n",
        "          total_eval_loss += loss.item()\n",
        "      avg_eval_loss = total_eval_loss / len(test_loader)\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'eval cost =', '{:.6f}'.format(avg_eval_loss))\n",
        "\n",
        "  # Save the trained model\n",
        "  os.makedirs('models', exist_ok=True)\n",
        "  torch.save(model, 'models/bert_shortmask.pth')\n",
        "\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  model = torch.load('models/bert_shortmask.pth')\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "average_train_acc, _ = evaluation.model_masked_label_accuracy(model, train_loader, device)\n",
        "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(model, test_loader, device)\n",
        "print('Train Acc =', '{:.6f}'.format(average_train_acc), 'Eval Acc =', '{:.6f}'.format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using foundation model (5 pts)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Introduction\n",
        "\n",
        "In this section, we aim to use a foundation model, DNABERT, to perform promoter detection.\n",
        "A foundation model is a model pretrained on large datasets. Foundation models serve as the foundational building blocks upon which various applications can be constructed.\n",
        "\n",
        "Here, we use DNABERT as the foundation model. We first apply it on DNA sequence to get the embedding. Then, we train a classifier on the embedding as in Section 2. Please follow this [link](https://github.com/Zhihan1996/DNABERT_2) to load the foundation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Implementation\n",
        "\n",
        "**Consider this situation:** You get a dataset about promoter detection, and you build your model to perform the task as in Section 2. However, the performance is not good since the model is not strong enough. Suddenly, you think we can use a large pre-trained model to embed DNA sequences. Then, you search online and find the pre-trained model [DNABERT](https://github.com/Zhihan1996/DNABERT_2). Now, you want to perform promoter detection using the pre-trained DNABERT.\n",
        "\n",
        "There is no coding framework in this section. Just make things work (get good test accuracy) using the pre-trained model!\n",
        "\n",
        "Hint: \n",
        "- We encourage you to create a **new environment** following the instructions of Section 3 in this [link](https://github.com/Zhihan1996/DNABERT_2). (When you face the error \"The model class you are passing has a config_class attribute that is not consistent with the config class you passed ...\", creating a new environment can save you.)\n",
        "- Section 4 in this [link](https://github.com/Zhihan1996/DNABERT_2) shows you how to load and use the pre-trained foundation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a. Load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "raw_training_data = data.load_csv(\"./data/train.csv\")\n",
        "raw_test_data = data.load_csv(\"./data/test.csv\")\n",
        "\n",
        "# Define the tokenizer\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True) \n",
        "\n",
        "# Define training hyperparameters\n",
        "dna_max_len = 298\n",
        "dataset_size = 200\n",
        "\n",
        "# Load the data\n",
        "input_ids, labels = data.generate_labeled_data(raw_training_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "test_input_ids, test_labels = data.generate_labeled_data(raw_test_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "\n",
        "# Define data loaders\n",
        "train_dataset = TensorDataset(input_ids, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. Get the embeddings of the DNA sequences using pretrained model.\n",
        "\n",
        "Hint: \n",
        "- This step can take some time. Thus, you can start with a small sample size, and then increase it when you have made sure that everything works correctly.\n",
        "- After getting the embeddings, you can save them so that you can directly load them next time without running the foundation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the pretrained model\n",
        "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True).to(device)\n",
        "\n",
        "# Turn off the gradients\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Save the training embeddings\n",
        "train_path = os.path.join(\"embeddings\", \"dnabert\", \"train\")\n",
        "if not os.path.exists(train_path):\n",
        "    # First create the path\n",
        "    os.makedirs(train_path, exist_ok=True)\n",
        "\n",
        "    # Save the embeddings \n",
        "    print(\"Saving dnabert train embeddings\")\n",
        "    save_embeddings(train_loader, model, train_path)\n",
        "\n",
        "# Save the test embeddings\n",
        "test_path = os.path.join(\"embeddings\", \"dnabert\", \"test\")\n",
        "if not os.path.exists(test_path):\n",
        "    # First create the path\n",
        "    os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "    # Save the embeddings\n",
        "    print(\"Saving dnabert test embeddings\")\n",
        "    save_embeddings(test_loader, model, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the embeddings with labels\n",
        "train_embeddings, train_labels = load_embeddings(train_path)\n",
        "test_embeddings, test_labels = load_embeddings(test_path)\n",
        "\n",
        "# Split the train into train and validation\n",
        "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(train_embeddings, train_labels, test_size=0.5, random_state=seed)\n",
        "\n",
        "# Define the datasets\n",
        "train_dataset = EmbeddingDataset(train_embeddings, train_labels)\n",
        "val_dataset = EmbeddingDataset(val_embeddings, val_labels)\n",
        "test_dataset = EmbeddingDataset(test_embeddings, test_labels)\n",
        "\n",
        "# Define the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# TODO: visualize the embeddings\n",
        "n = min(len(train_embeddings), 1000)\n",
        "emb, lab = train_embeddings[:n], train_labels[:n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. Train a classifier.\n",
        "\n",
        "Hint: It is easy to overfit on the training set. Try to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model first\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "if True or not os.path.exists('models/promoter_clf2.pth'):\n",
        "\n",
        "  print(\"Training the model...\")\n",
        "\n",
        "  # Define the classifier\n",
        "  clf = DnaBertHead(input_size=768, output_size=1, hidden_dimensions=[30], dropout_rate=0.55, activation=nn.ReLU()).to(device)\n",
        "  clf_optimizer = optim.AdamW(clf.parameters(), lr=1e-1, weight_decay=0.05)\n",
        "\n",
        "  # Train the classifier\n",
        "  best_val_loss, best_val_acc = train_loop(clf, train_loader, val_loader, criterion, clf_optimizer, device, epochs=100, patience=5)\n",
        "\n",
        "  # Save the trained model\n",
        "  os.makedirs(\"models\", exist_ok=True)\n",
        "  torch.save(clf, 'models/promoter_clf2.pth')\n",
        "\n",
        "# Load the trained model\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  clf = torch.load('models/promoter_clf2.pth')\n",
        "\n",
        "# Evaluation\n",
        "avg_test_loss, test_acc = eval_loop(clf, test_loader, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
