{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 3 - Name, SCIPER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this homework, we are going to work with the transformer. There are three parts of this homework.\n",
        "\n",
        "- In the first part, we are going to implement **positional encoding** and **self-attention**  and test them on a simple text dataset which contains around 100 sentences. We will use a small transformer in this task.\n",
        "\n",
        "- In the second part, we will detect promoters from the DNA sequences. The main difference compared to the previous task is to tokenize the DNA sequence. Thus, our task here is to build the **tokenizer** to tokenize the DNA sequence. For the model, we will continue using the small transformer.\n",
        "\n",
        "- In the third part, we will use a **foundation model** DNABERT to perform promoter detection. In this part, you do not need to train the transformer. Instead, you need to find and load the correct pre-trained model and then use it to get the embedding of the DNA sequence. Then, you will build a simple classifier to perform promoter detection based on the DNA embedding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Initialization\n",
        "\n",
        "Import the packages you are going to use here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from types import SimpleNamespace\n",
        "from utils import data, evaluation, models, visualization, text_exercise\n",
        "\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seeds\n",
        "seed = 128\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Positional Encoding and Self-Attention (7 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Sinusoidal Positional Encoding (1 pt)\n",
        "\n",
        "In this section, you are going to implement the sinusoidal positional encoding. The formula is as the following:\n",
        "\n",
        "<div>\n",
        "<img src=\"./imgs/positional embedding.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "where $t$ is the desired position in the input and $\\mathsf{\\omega}_k$ follows:\n",
        "\n",
        "<div>\n",
        "<img src=\"./imgs/omega.png\" width=\"200\"/>\n",
        "</div>\n",
        "\n",
        "To see the details of sinusoidal positional encoding, you can check this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, max_position_embeddings, hidden_size, device):\n",
        "        super().__init__()\n",
        "\n",
        "        '''\n",
        "        max_position_embeddings: maximum length of the input - related to t in the previous formula\n",
        "        hidden_size: encoding dimension - d in the previous formula\n",
        "        '''\n",
        "\n",
        "        # Compute weights for the positional embedding\n",
        "        k = torch.arange(hidden_size//2, dtype=torch.float32, device=device)\n",
        "        w = 1 / (10000 ** (2*k/hidden_size))\n",
        "\n",
        "        # Compute the positional embedding\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        sin = torch.sin(torch.outer(t, w)) # n x d/2\n",
        "        cos = torch.cos(torch.outer(t, w))  # n x d/2\n",
        "\n",
        "        # Put the sin and cos together\n",
        "        self.positional_embedding = torch.zeros((max_position_embeddings, hidden_size), device=device)\n",
        "        self.positional_embedding[:, 0::2] = sin\n",
        "        self.positional_embedding[:, 1::2] = cos\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.positional_embedding\n",
        "    \n",
        "    def embedding(self):\n",
        "        return self.positional_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, you can visualize your positional encoding. If you implement everything correctly, you can get a figure that is similar to Figure 2 in this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fef2c25b243497b85f460a3de26e395",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Dropdown(description='Sequence Length', index=2, options=(100, 500, 1000, 5000), style=Descript…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1eaf851ac87040e69ef9ad3b25d65c77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_embedding, dimension_selector, max_len_selector = visualization.display_positional_encoding(PositionalEmbedding)\n",
        "ui = widgets.HBox([max_len_selector, dimension_selector])  \n",
        "out = widgets.interactive_output( visualize_embedding, {'max_len': max_len_selector, 'dimension': dimension_selector})\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Self-Attention Mechanism (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, you are going to implement the self-attention mechanism. Please check the section 'Self-Attention in Detail' in this [link](https://jalammar.github.io/illustrated-transformer/) for the details of self-attention mechanism. (We encourage you to carefully go through the link since it is a very good tutorial for transformer.)\n",
        "\n",
        "The specific steps will be provided in the comments of the following code. (The steps are only for reference. You do need to follow the steps if you have a better way to implement it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "            )\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "    ):\n",
        "\n",
        "        # Resulting shape: (batch_size, sequence_length, all_head_size)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        # Resulting shape: (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Resulting shape: (batch_size, num_attention_heads, sequence_length, sequence_length)\n",
        "        # For each input token, we have unnormalized scores for all other tokens in the input sequence\n",
        "        attention_scores = query_layer @ key_layer.transpose(-1, -2) / math.sqrt(self.attention_head_size)\n",
        "        \n",
        "        # Explanation of attention_mask: https://lukesalamone.github.io/posts/what-are-attention-masks/\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # You do not need to change this part.\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        # Reweight each token embeddings by the attention scores\n",
        "        # Shape: (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        context_layer = attention_probs @ value_layer\n",
        "\n",
        "        # Concatenate all the attention heads together\n",
        "        head_outputs = torch.split(context_layer, 1, dim=1)\n",
        "        context_layer = torch.cat(head_outputs, dim=-1).squeeze(1)\n",
        "\n",
        "        # Get the output\n",
        "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test your implementation using simple text data! First, let's load the data.\n",
        "\n",
        "We use a small dataset in this homework for a shorter training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChatGPT generated text data about BERT\n",
        "text = text_exercise.get()\n",
        "sentences_df, vocab = data.to_sentence_df(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After loading the data, you can train your model. Here we train our model using masked token prediction.\n",
        "\n",
        "Hint: The final model accuracy should be higher than 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable model parameters: 64910\n",
            "Epoch: 0020 loss = 4.297029\n",
            "Epoch: 0040 loss = 3.666570\n",
            "Epoch: 0060 loss = 3.096795\n",
            "Epoch: 0080 loss = 2.602032\n",
            "Epoch: 0100 loss = 2.147091\n",
            "Epoch: 0120 loss = 1.708678\n",
            "Epoch: 0140 loss = 1.317243\n",
            "Epoch: 0160 loss = 0.874174\n",
            "Epoch: 0180 loss = 0.551095\n",
            "Epoch: 0200 loss = 0.335653\n",
            "Final model accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "text_max_len = 11\n",
        "\n",
        "text_config = SimpleNamespace(\n",
        "        vocab_size=len(vocab),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=text_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=2,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=1,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=False,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=\"cpu\"\n",
        "    )\n",
        "\n",
        "tokenizer = data.TextTokenizer(vocab)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(sentences_df, tokenizer, k=1, max_len=text_max_len, noise_rate=0.4)\n",
        "\n",
        "model = models.BertForMaskedLM(config=text_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    loss, outputs, attentions = model(\n",
        "        input_ids=input_ids,\n",
        "        token_type_ids=segment_ids,\n",
        "        masked_lm_labels=masked_lm_labels,\n",
        "        attention_mask=attention_masks\n",
        "    )\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"Final model accuracy: {evaluation.masked_label_accuracy(labels, labels_idx, outputs.data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Visualize Attention (1 pt)\n",
        "\n",
        "Here, you can visualize the self-attention. \n",
        "\n",
        "Question: Can you interpret the visualization of the self-attention?\n",
        "\n",
        "**Answer:** In each row, we have weights denoting the importance of each token in the input sequence for the current token (ith token). The darker the color, the more important the token is. We can see that the model pays more attention to the tokens that are closer to the current token. For instance, given sample `11`, the most important tokens for word they are `they`, `one` and `at`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3b415442c6c48aa95a70c166dd29a88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='Sample:', options=(5, 8, 11, 13, 18, 24, 26, 29, 31, 35, 37, 38, 6…"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "visualize_attention, sample_id_selector = visualization.display_attantion(attentions=attentions, input_ids=input_ids, tokenizer=tokenizer)\n",
        "widgets.interactive(visualize_attention, sample_id=sample_id_selector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Train on small Wikitext Dataset\n",
        "\n",
        "Here, you can **optionally** test your model on the smallest wikitext dataset. You should get an test accuracy around 0.4 after training 50 epochs.\n",
        "\n",
        "This part is only for you to test your code. You can choose to run it or not. It takes around 1 hour to train the model for 50 epochs on the smallest wikitext dataset with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/ludekcizinsky/Dev/personal/epfl-cs502/coursework/homeworks/hw3/Homework3.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ludekcizinsky/Dev/personal/epfl-cs502/coursework/homeworks/hw3/Homework3.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text_exercise\u001b[39m.\u001b[39;49mtrain_wikitext(device, positional_embedding\u001b[39m=\u001b[39;49mPositionalEmbedding, attention\u001b[39m=\u001b[39;49mBertSelfAttention)\n",
            "File \u001b[0;32m~/Dev/personal/epfl-cs502/coursework/homeworks/hw3/utils/text_exercise.py:152\u001b[0m, in \u001b[0;36mtrain_wikitext\u001b[0;34m(device, positional_embedding, attention)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m    150\u001b[0m         \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids[idx]\u001b[39m.\u001b[39msqueeze(), \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_masks[idx]\u001b[39m.\u001b[39msqueeze()}\n\u001b[0;32m--> 152\u001b[0m train_dataset \u001b[39m=\u001b[39m TextDataset(train_text_data, tokenizer)\n\u001b[1;32m    153\u001b[0m validation_dataset \u001b[39m=\u001b[39m TextDataset(validation_text_data, tokenizer)\n\u001b[1;32m    155\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[39m=\u001b[39mtokenizer, mlm_probability\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m)\n",
            "File \u001b[0;32m~/Dev/personal/epfl-cs502/coursework/homeworks/hw3/utils/text_exercise.py:142\u001b[0m, in \u001b[0;36mtrain_wikitext.<locals>.TextDataset.__init__\u001b[0;34m(self, texts, tokenizer, max_length)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_masks \u001b[39m=\u001b[39m []\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts:\n\u001b[0;32m--> 142\u001b[0m     encoded_text \u001b[39m=\u001b[39m tokenizer(text, max_length\u001b[39m=\u001b[39;49mtext_max_len, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mappend(encoded_text\u001b[39m.\u001b[39minput_ids)\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_masks\u001b[39m.\u001b[39mappend(encoded_text\u001b[39m.\u001b[39mattention_mask)\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2548\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2549\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2635\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2636\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2651\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2652\u001b[0m     )\n\u001b[1;32m   2653\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2654\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2655\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2656\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2657\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2658\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2659\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2660\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2661\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2662\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2663\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2664\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2665\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2666\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2667\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2668\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2669\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2670\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2671\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2672\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2673\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2727\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2717\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2718\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2719\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2720\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2724\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2725\u001b[0m )\n\u001b[0;32m-> 2727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2728\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2729\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2730\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2731\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2732\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2733\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2734\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2735\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2736\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2737\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2738\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2739\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2740\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2741\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2742\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2743\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2744\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2745\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2746\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/transformers/tokenization_utils.py:652\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39;49msecond_ids,\n\u001b[1;32m    655\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    656\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    657\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    658\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    659\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    660\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    661\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    662\u001b[0m     prepend_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    663\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    664\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    665\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    666\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    667\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3217\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   3214\u001b[0m \u001b[39mif\u001b[39;00m return_length:\n\u001b[1;32m   3215\u001b[0m     encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 3217\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(\n\u001b[1;32m   3218\u001b[0m     encoded_inputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis\n\u001b[1;32m   3219\u001b[0m )\n\u001b[1;32m   3221\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    207\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:718\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    715\u001b[0m     value \u001b[39m=\u001b[39m [value]\n\u001b[1;32m    717\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 718\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    720\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    727\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m tensor\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "text_exercise.train_wikitext(device, positional_embedding=PositionalEmbedding, attention=BertSelfAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Promoter detection (7 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we detect promoter in DNA sequence.\n",
        "\n",
        "A promoter is a region of DNA upstream of a gene where relevant proteins (such as RNA polymerase and transcription factors) bind to initiate transcription of that gene. Promoter detection is to identify if there are promoter regions in the given DNA sequence. We have covered this in the lecture. (If you are interested in the promoter, you can check this [link](https://www.genome.gov/genetics-glossary/Promoter) for more details.)\n",
        "\n",
        "Here, we use a transformer and a classifier. The transformer first embeds the DNA sequences into features, and then the classifier detects the promoter based on the features.\n",
        "\n",
        "The main difference between text and DNA sequence is how to tokenize the sequence. Thus, you need to implement a tokenizer for the DNA sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. DNA Tokenizer (1 pts)\n",
        "\n",
        "Here, you will implement the DNA tokenizer the same as in DNABERT. Please check this [paper](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680) for implementation details. Also, you need to check the data type and shape for both input and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DNATokenizer(data.Tokenizer):\n",
        "    def __init__(self, k, vocab, unknown=\"[UNK]\"):\n",
        "        super().__init__(vocab, unknown)\n",
        "\n",
        "        # self.k is the k of k-mers\n",
        "        self.k = k\n",
        "\n",
        "    def _parse_text(self, text):\n",
        "        n = len(text)\n",
        "        return ['[CLS]'] + [text[i:i + self.k] for i in range(n - self.k + 1)] + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Test BERT on DNA Sequence\n",
        "\n",
        "In this section, you will train BERT on DNA sequence to learn the embedding of DNA sequence. The code is provided below and you do not need to write anything.\n",
        "\n",
        "Hint: the final evaluation accuracy should be higher than 0.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the trained model...\n",
            "Number of trainable model parameters: 118869\n",
            "Train Acc = 0.252133 Eval Acc = 0.242678\n"
          ]
        }
      ],
      "source": [
        "kmer = 3\n",
        "mask_length = kmer\n",
        "VOCAB_3MER = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"AAA\", \"AAT\", \"AAC\", \"AAG\", \"ATA\", \"ATT\", \"ATC\", \"ATG\", \"ACA\", \"ACT\", \"ACC\", \"ACG\", \"AGA\", \"AGT\", \"AGC\", \"AGG\", \"TAA\", \"TAT\", \"TAC\", \"TAG\", \"TTA\", \"TTT\", \"TTC\", \"TTG\", \"TCA\", \"TCT\", \"TCC\", \"TCG\", \"TGA\", \"TGT\", \"TGC\", \"TGG\", \"CAA\", \"CAT\", \"CAC\", \"CAG\", \"CTA\", \"CTT\", \"CTC\", \"CTG\", \"CCA\", \"CCT\", \"CCC\", \"CCG\", \"CGA\", \"CGT\", \"CGC\", \"CGG\", \"GAA\", \"GAT\", \"GAC\", \"GAG\", \"GTA\", \"GTT\", \"GTC\", \"GTG\", \"GCA\", \"GCT\", \"GCC\", \"GCG\", \"GGA\", \"GGT\", \"GGC\", \"GGG\" ]\n",
        "\n",
        "raw_training_data = data.load_csv(\"./data/train.csv\")\n",
        "raw_test_data = data.load_csv(\"./data/test.csv\")\n",
        "\n",
        "dna_max_len = 300\n",
        "batch_size = 128\n",
        "max_dna_mask = 100\n",
        "dataset_size = 1000\n",
        "num_layers = 3\n",
        "num_heads = 6\n",
        "dna_config = SimpleNamespace(\n",
        "        vocab_size=len(VOCAB_3MER),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=dna_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=num_heads,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=num_layers,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "tokenizer = DNATokenizer(k=kmer, vocab=VOCAB_3MER)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(raw_training_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks = data.generate_masked_data(raw_test_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "if not os.path.exists('models/bert_longmask.pth'):\n",
        "  print(\"Training the model...\")\n",
        "  model = models.BertForMaskedLM(config=dna_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention).to(device)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "  for epoch in range(50):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss, outputs, hidden_states, _ = model(\n",
        "          input_ids=batch_input_ids.to(device),\n",
        "          token_type_ids=batch_segment_ids.to(device),\n",
        "          masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "          attention_mask=batch_attention_mask.to(device)\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      model.eval()\n",
        "      total_eval_loss = 0\n",
        "      for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in test_loader:\n",
        "        with torch.no_grad():\n",
        "          loss, outputs, hidden_states, _ = model(\n",
        "            input_ids=batch_input_ids.to(device),\n",
        "            token_type_ids=batch_segment_ids.to(device),\n",
        "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "            attention_mask=batch_attention_mask.to(device)\n",
        "          )\n",
        "          if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0 :\n",
        "            print(\"found patting\", batch_attention_mask.sum())\n",
        "          total_eval_loss += loss.item()\n",
        "      avg_eval_loss = total_eval_loss / len(test_loader)\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'eval cost =', '{:.6f}'.format(avg_eval_loss))\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(model, 'models/bert_longmask.pth')\n",
        "\n",
        "# Load the trained model\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  model = torch.load('models/bert_longmask.pth')\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "average_train_acc, _ = evaluation.model_masked_label_accuracy(model, train_loader, device)\n",
        "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(model, test_loader, device)\n",
        "print('Train Acc =', '{:.6f}'.format(average_train_acc), 'Eval Acc =', '{:.6f}'.format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Visualize the Attentions (1 pt)\n",
        "\n",
        "Here, you can visualize the self-attention. \n",
        "\n",
        "Question: compare the visualization to Section 1.3, what can you find here? How do you explain it?\n",
        "\n",
        "**Answer:** We can see a clear diagonal pattern in the visualization. This demonstrates that each `k-mer`'s embedding is defined by the closest following `k-mers`. This is in contrast to 1.3 where the model also payed attention to previuous tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f10d7342204b4c6da6d4fa4a606c51d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Dropdown(description='Sample:', options=(4, 13, 21, 25, 28, 30, 37, 38, 42, 45, 47, 50, 62, 77,…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b8007943f904c27a4d06e990740c729",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_attention, sample_id_selector, layer_selector, head_selector = visualization.display_multi_attantion(attentions=last_test_attention, tokenizer=tokenizer, input_ids=input_ids,  layers=range(1, num_layers+1),  heads=range(1, num_heads+1))\n",
        "ui = widgets.HBox([sample_id_selector, layer_selector, head_selector])  \n",
        "out = widgets.interactive_output(visualize_attention, {'sample_id': sample_id_selector, 'layer': layer_selector, 'head': head_selector})\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Use your pretrained model for promoter detection (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You already have the embeddings for the DNA sequence. Now, you are going to build a classifier based on the DNA embeddings. The classifier is to perform promoter detection. Specifically, the DNA sequence will be classified into *'contains promoter'* or *'does not contain promoter'*.\n",
        "\n",
        "Hint: \n",
        "- We now want to annotate data (get the label for each sample), not predict masked data anymore!\n",
        "- You can reuse some parts of the code in the previous sections, e.g. dataloader and training pipeline in Section 2.2.\n",
        "- If you implement the previous section correctly (the Eval Acc > 0.2 in Section 2.2), you already have an pre-trained object named 'model' of class models.BertForMaskedLM. You can directly use it.\n",
        "- The evaluation accuracy of this task should be around 0.6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a binary classifier\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, embedder):\n",
        "        super().__init__()\n",
        "        self.embedder = embedder\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "      # Get the output embeddings of BertForMaskedLM\n",
        "      hidden_states = self.embedder(x)[1]\n",
        "\n",
        "      # Get the output embeddings\n",
        "      outputs = hidden_states[-1]\n",
        "\n",
        "      # Use the [CLS] token embedding\n",
        "      x = outputs[:, 0, :]\n",
        "\n",
        "      # Run the classifier\n",
        "      x = self.linear1(x)\n",
        "      x = self.linear2(x)\n",
        "      x = self.sigmoid(x)\n",
        "\n",
        "      return x.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data (without masks indeed)\n",
        "input_ids, labels = data.generate_labeled_data(raw_training_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "test_input_ids, test_labels = data.generate_labeled_data(raw_test_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "\n",
        "# Convert labels to float\n",
        "labels = labels.float()\n",
        "test_labels = test_labels.float()\n",
        "\n",
        "# Define data loaders\n",
        "train_dataset = TensorDataset(input_ids, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the trained model...\n",
            "Test cost = 308.532658 test acc = 0.586000\n"
          ]
        }
      ],
      "source": [
        "# Train the model first\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if not os.path.exists('models/promoter_clf.pth'):\n",
        "\n",
        "  print(\"Training the model...\")\n",
        "  # Freeze the weights of the embedder model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Define the classifier\n",
        "  clf = BinaryClassifier(input_size=60, hidden_size=30, output_size=1, embedder=model).to(device)\n",
        "  clf_optimizer = optim.AdamW(clf.parameters(), lr=1e-2)\n",
        "\n",
        "  # Train the classifier\n",
        "  clf.train()\n",
        "  for epoch in range(50):\n",
        "\n",
        "    # Training\n",
        "    total_train_loss = 0\n",
        "    preds = []\n",
        "    labs = []\n",
        "    for batch_input_ids, batch_labels in train_loader:\n",
        "\n",
        "      # Reset the gradients\n",
        "      clf_optimizer.zero_grad()\n",
        "\n",
        "      # Get the output of the classifier\n",
        "      batch_output = clf(batch_input_ids)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = criterion(batch_output.to(device), batch_labels.to(device))\n",
        "\n",
        "      # Add prediction and labels\n",
        "      bpred = (batch_output > 0.5).float().tolist()\n",
        "      blabs = batch_labels.tolist()\n",
        "      preds.extend(bpred)\n",
        "      labs.extend(blabs)\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      clf_optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "    # Evaluate the epoch\n",
        "    preds, labs = np.array(preds), np.array(labs)\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_acc = np.mean(preds == labs)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'train acc =', '{:.6f}'.format(train_acc))\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(clf, 'models/promoter_clf.pth')\n",
        "\n",
        "# Load the trained model\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  clf = torch.load('models/promoter_clf.pth')\n",
        "\n",
        "# Evaluation\n",
        "total_test_loss = 0\n",
        "preds = []\n",
        "labs = []\n",
        "clf.eval()\n",
        "for batch_input_ids, batch_labels in test_loader:\n",
        "\n",
        "  # Get the output of the classifier\n",
        "  batch_output = clf(batch_input_ids)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = criterion(batch_output.to(device), batch_labels.to(device))\n",
        "\n",
        "  # Add prediction and labels\n",
        "  bpred = (batch_output > 0.5).float().tolist()\n",
        "  blabs = batch_labels.tolist()\n",
        "  preds.extend(bpred)\n",
        "  labs.extend(blabs)\n",
        "\n",
        "  total_test_loss += loss.item()\n",
        "\n",
        "preds, labs = np.array(preds), np.array(labs)\n",
        "avg_test_loss = total_test_loss / len(test_loader)\n",
        "test_acc = np.mean(preds == labs)\n",
        "print('Test cost =', '{:.6f}'.format(avg_test_loss), 'test acc =', '{:.6f}'.format(test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5. Additional question (1 pt)\n",
        "\n",
        "Now we change mask_length = 1 (already changed, you do not need to implement anything).\n",
        "Let's run the code below and check the accuracy.\n",
        "\n",
        "Question: What is the final masked token prediction accuracy? How do you explain this?\n",
        "\n",
        "**Write down you answer here (1 pt):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the trained model...\n",
            "Number of trainable model parameters: 118869\n",
            "Train Acc = 0.921650 Eval Acc = 0.923077\n"
          ]
        }
      ],
      "source": [
        "kmer = 3\n",
        "mask_length = 1\n",
        "\n",
        "dna_max_len = 300\n",
        "batch_size = 128\n",
        "max_dna_mask = 100\n",
        "dataset_size = 1000\n",
        "num_layers = 3\n",
        "num_heads = 6\n",
        "dna_config = SimpleNamespace(\n",
        "        vocab_size=len(VOCAB_3MER),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=dna_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=num_heads,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=num_layers,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "tokenizer = DNATokenizer(k=3, vocab=VOCAB_3MER)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(raw_training_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks = data.generate_masked_data(raw_test_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "if not os.path.exists('models/bert_shortmask.pth'):\n",
        "  print(\"Training the model...\")\n",
        "  model = models.BertForMaskedLM(config=dna_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention).to(device)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "  for epoch in range(50):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss, outputs, hidden_states, _ = model(\n",
        "          input_ids=batch_input_ids.to(device),\n",
        "          token_type_ids=batch_segment_ids.to(device),\n",
        "          masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "          attention_mask=batch_attention_mask.to(device)\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      model.eval()\n",
        "      total_eval_loss = 0\n",
        "      for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in test_loader:\n",
        "        with torch.no_grad():\n",
        "          loss, outputs, hidden_states, _ = model(\n",
        "            input_ids=batch_input_ids.to(device),\n",
        "            token_type_ids=batch_segment_ids.to(device),\n",
        "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "            attention_mask=batch_attention_mask.to(device)\n",
        "          )\n",
        "          if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0 :\n",
        "            print(\"found patting\", batch_attention_mask.sum())\n",
        "          total_eval_loss += loss.item()\n",
        "      avg_eval_loss = total_eval_loss / len(test_loader)\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'eval cost =', '{:.6f}'.format(avg_eval_loss))\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(model, 'models/bert_shortmask.pth')\n",
        "\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  model = torch.load('models/bert_shortmask.pth')\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "average_train_acc, _ = evaluation.model_masked_label_accuracy(model, train_loader, device)\n",
        "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(model, test_loader, device)\n",
        "print('Train Acc =', '{:.6f}'.format(average_train_acc), 'Eval Acc =', '{:.6f}'.format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using foundation model (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Introduction\n",
        "\n",
        "In this section, we aim to use a foundation model, DNABERT, to perform promoter detection.\n",
        "A foundation model is a model pretrained on large datasets. Foundation models serve as the foundational building blocks upon which various applications can be constructed.\n",
        "\n",
        "Here, we use DNABERT as the foundation model. We first apply it on DNA sequence to get the embedding. Then, we train a classifier on the embedding as in Section 2. Please follow this [link](https://github.com/Zhihan1996/DNABERT_2) to load the foundation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Implementation\n",
        "\n",
        "**Consider this situation:** You get a dataset about promoter detection, and you build your model to perform the task as in Section 2. However, the performance is not good since the model is not strong enough. Suddenly, you think we can use a large pre-trained model to embed DNA sequences. Then, you search online and find the pre-trained model [DNABERT](https://github.com/Zhihan1996/DNABERT_2). Now, you want to perform promoter detection using the pre-trained DNABERT.\n",
        "\n",
        "There is no coding framework in this section. Just make things work (get good test accuracy) using the pre-trained model!\n",
        "\n",
        "Hint: \n",
        "- We encourage you to create a **new environment** following the instructions of Section 3 in this [link](https://github.com/Zhihan1996/DNABERT_2). (When you face the error \"The model class you are passing has a config_class attribute that is not consistent with the config class you passed ...\", creating a new environment can save you.)\n",
        "- Section 4 in this [link](https://github.com/Zhihan1996/DNABERT_2) shows you how to load and use the pre-trained foundation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a. Load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(input_ids, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. Get the embeddings of the DNA sequences using pretrained model.\n",
        "\n",
        "Hint: \n",
        "- This step can take some time. Thus, you can start with a small sample size, and then increase it when you have made sure that everything works correctly.\n",
        "- After getting the embeddings, you can save them so that you can directly load them next time without running the foundation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "194bb16f095543ea994cb9b85de72120",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/468M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ludekcizinsky/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained model\n",
        "pretokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "premodel = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Using tsne or umap to visualize the embedding space.\n",
        "# Hint: you can import other packages here for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. Train a classifier.\n",
        "\n",
        "Hint: It is easy to overfit on the training set. Try to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Define your classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Train your classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
