{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 3 - Name, SCIPER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this homework, we are going to work with the transformer. There are three parts of this homework.\n",
        "\n",
        "- In the first part, we are going to implement **positional encoding** and **self-attention**  and test them on a simple text dataset which contains around 100 sentences. We will use a small transformer in this task.\n",
        "\n",
        "- In the second part, we will detect promoters from the DNA sequences. The main difference compared to the previous task is to tokenize the DNA sequence. Thus, our task here is to build the **tokenizer** to tokenize the DNA sequence. For the model, we will continue using the small transformer.\n",
        "\n",
        "- In the third part, we will use a **foundation model** DNABERT to perform promoter detection. In this part, you do not need to train the transformer. Instead, you need to find and load the correct pre-trained model and then use it to get the embedding of the DNA sequence. Then, you will build a simple classifier to perform promoter detection based on the DNA embedding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Initialization\n",
        "\n",
        "Import the packages you are going to use here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from types import SimpleNamespace\n",
        "from utils import data, evaluation, models, visualization, text_exercise\n",
        "\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seeds\n",
        "seed = 128\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Positional Encoding and Self-Attention (7 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Sinusoidal Positional Encoding (1 pt)\n",
        "\n",
        "In this section, you are going to implement the sinusoidal positional encoding. The formula is as the following:\n",
        "\n",
        "<div>\n",
        "<img src=\"./imgs/positional embedding.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "where $t$ is the desired position in the input and $\\mathsf{\\omega}_k$ follows:\n",
        "\n",
        "<div>\n",
        "<img src=\"./imgs/omega.png\" width=\"200\"/>\n",
        "</div>\n",
        "\n",
        "To see the details of sinusoidal positional encoding, you can check this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, max_position_embeddings, hidden_size, device):\n",
        "        super().__init__()\n",
        "\n",
        "        '''\n",
        "        max_position_embeddings: maximum length of the input - related to t in the previous formula\n",
        "        hidden_size: encoding dimension - d in the previous formula\n",
        "        '''\n",
        "\n",
        "        # Compute weights for the positional embedding\n",
        "        k = torch.arange(hidden_size//2, dtype=torch.float32, device=device)\n",
        "        w = 1 / (10000 ** (2*k/hidden_size))\n",
        "\n",
        "        # Compute the positional embedding\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        sin = torch.sin(torch.outer(t, w)) # n x d/2\n",
        "        cos = torch.cos(torch.outer(t, w))  # n x d/2\n",
        "\n",
        "        # Put the sin and cos together\n",
        "        self.positional_embedding = torch.zeros((max_position_embeddings, hidden_size), device=device)\n",
        "        self.positional_embedding[:, 0::2] = sin\n",
        "        self.positional_embedding[:, 1::2] = cos\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.positional_embedding\n",
        "    \n",
        "    def embedding(self):\n",
        "        return self.positional_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, you can visualize your positional encoding. If you implement everything correctly, you can get a figure that is similar to Figure 2 in this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78a70ce8100343a9ab094942cf4bf4bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Dropdown(description='Sequence Length', index=2, options=(100, 500, 1000, 5000), style=Descript…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91fb92fb71f64a16b2b10c94ff68bb98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_embedding, dimension_selector, max_len_selector = visualization.display_positional_encoding(PositionalEmbedding)\n",
        "ui = widgets.HBox([max_len_selector, dimension_selector])  \n",
        "out = widgets.interactive_output( visualize_embedding, {'max_len': max_len_selector, 'dimension': dimension_selector})\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Self-Attention Mechanism (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, you are going to implement the self-attention mechanism. Please check the section 'Self-Attention in Detail' in this [link](https://jalammar.github.io/illustrated-transformer/) for the details of self-attention mechanism. (We encourage you to carefully go through the link since it is a very good tutorial for transformer.)\n",
        "\n",
        "The specific steps will be provided in the comments of the following code. (The steps are only for reference. You do need to follow the steps if you have a better way to implement it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "            )\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "    ):\n",
        "\n",
        "        # Resulting shape: (batch_size, sequence_length, all_head_size)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        # Resulting shape: (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Resulting shape: (batch_size, num_attention_heads, sequence_length, sequence_length)\n",
        "        # For each input token, we have unnormalized scores for all other tokens in the input sequence\n",
        "        attention_scores = query_layer @ key_layer.transpose(-1, -2) / math.sqrt(self.attention_head_size)\n",
        "        \n",
        "        # Explanation of attention_mask: https://lukesalamone.github.io/posts/what-are-attention-masks/\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # You do not need to change this part.\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        # Reweight each token embeddings by the attention scores\n",
        "        # Shape: (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        context_layer = attention_probs @ value_layer\n",
        "\n",
        "        # Concatenate all the attention heads together\n",
        "        head_outputs = torch.split(context_layer, 1, dim=1)\n",
        "        context_layer = torch.cat(head_outputs, dim=-1).squeeze(1)\n",
        "\n",
        "        # Get the output\n",
        "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test your implementation using simple text data! First, let's load the data.\n",
        "\n",
        "We use a small dataset in this homework for a shorter training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChatGPT generated text data about BERT\n",
        "text = text_exercise.get()\n",
        "sentences_df, vocab = data.to_sentence_df(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After loading the data, you can train your model. Here we train our model using masked token prediction.\n",
        "\n",
        "Hint: The final model accuracy should be higher than 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable model parameters: 64910\n",
            "Epoch: 0020 loss = 4.312112\n",
            "Epoch: 0040 loss = 3.652103\n",
            "Epoch: 0060 loss = 3.091904\n",
            "Epoch: 0080 loss = 2.636230\n",
            "Epoch: 0100 loss = 2.266909\n",
            "Epoch: 0120 loss = 1.880526\n",
            "Epoch: 0140 loss = 1.435074\n",
            "Epoch: 0160 loss = 1.024923\n",
            "Epoch: 0180 loss = 0.667369\n",
            "Epoch: 0200 loss = 0.413059\n",
            "Final model accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "text_max_len = 11\n",
        "\n",
        "text_config = SimpleNamespace(\n",
        "        vocab_size=len(vocab),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=text_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=2,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=1,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=False,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=\"cpu\"\n",
        "    )\n",
        "\n",
        "tokenizer = data.TextTokenizer(vocab)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(sentences_df, tokenizer, k=1, max_len=text_max_len, noise_rate=0.4)\n",
        "\n",
        "model = models.BertForMaskedLM(config=text_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    loss, outputs, attentions = model(\n",
        "        input_ids=input_ids,\n",
        "        token_type_ids=segment_ids,\n",
        "        masked_lm_labels=masked_lm_labels,\n",
        "        attention_mask=attention_masks\n",
        "    )\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"Final model accuracy: {evaluation.masked_label_accuracy(labels, labels_idx, outputs.data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Visualize Attention (1 pt)\n",
        "\n",
        "Here, you can visualize the self-attention. \n",
        "\n",
        "Question: Can you interpret the visualization of the self-attention?\n",
        "\n",
        "**Answer:** In each row, we have weights denoting the importance of each token in the input sequence for the current token (ith token). The darker the color, the more important the token is. We can see that the model pays more attention to the tokens that are closer to the current token. For instance, given sample `11`, the most important tokens for word they are `they`, `one` and `at`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c47a703be1a416f8b7704931b83575c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='Sample:', options=(5, 8, 11, 13, 18, 24, 26, 29, 31, 35, 37, 38, 6…"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "visualize_attention, sample_id_selector = visualization.display_attantion(attentions=attentions, input_ids=input_ids, tokenizer=tokenizer)\n",
        "widgets.interactive(visualize_attention, sample_id=sample_id_selector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Train on small Wikitext Dataset\n",
        "\n",
        "Here, you can **optionally** test your model on the smallest wikitext dataset. You should get an test accuracy around 0.4 after training 50 epochs.\n",
        "\n",
        "This part is only for you to test your code. You can choose to run it or not. It takes around 1 hour to train the model for 50 epochs on the smallest wikitext dataset with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/ludekcizinsky/Dev/personal/epfl-cs502/coursework/homeworks/hw3/Homework3.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ludekcizinsky/Dev/personal/epfl-cs502/coursework/homeworks/hw3/Homework3.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text_exercise\u001b[39m.\u001b[39;49mtrain_wikitext(device, positional_embedding\u001b[39m=\u001b[39;49mPositionalEmbedding, attention\u001b[39m=\u001b[39;49mBertSelfAttention)\n",
            "File \u001b[0;32m~/Dev/personal/epfl-cs502/coursework/homeworks/hw3/utils/text_exercise.py:130\u001b[0m, in \u001b[0;36mtrain_wikitext\u001b[0;34m(device, positional_embedding, attention)\u001b[0m\n\u001b[1;32m    127\u001b[0m train_wikitext_subset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mwikitext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwikitext-2-v1\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    128\u001b[0m train_text_data \u001b[39m=\u001b[39m train_wikitext_subset[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 130\u001b[0m validation_wikitext_subset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mwikitext\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mwikitext-2-v1\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    131\u001b[0m validation_text_data \u001b[39m=\u001b[39m validation_wikitext_subset[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    133\u001b[0m text_max_len \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2130\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   2131\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2135\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2139\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2141\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   2142\u001b[0m )\n\u001b[1;32m   2144\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1816\u001b[0m     path,\n\u001b[1;32m   1817\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1818\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1819\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1820\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1821\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1822\u001b[0m )\n\u001b[1;32m   1823\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/load.py:1481\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1480\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39min\u001b[39;00m [sibling\u001b[39m.\u001b[39mrfilename \u001b[39mfor\u001b[39;00m sibling \u001b[39min\u001b[39;00m dataset_info\u001b[39m.\u001b[39msiblings]:\n\u001b[0;32m-> 1481\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1482\u001b[0m         path,\n\u001b[1;32m   1483\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1484\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1485\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1486\u001b[0m         dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[1;32m   1487\u001b[0m     )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1488\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1489\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1490\u001b[0m         path,\n\u001b[1;32m   1491\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1495\u001b[0m         download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[1;32m   1496\u001b[0m     )\u001b[39m.\u001b[39mget_module()\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/load.py:1159\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithScript.__init__\u001b[0;34m(self, name, revision, download_config, download_mode, dynamic_modules_path)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_mode \u001b[39m=\u001b[39m download_mode\n\u001b[1;32m   1158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_modules_path \u001b[39m=\u001b[39m dynamic_modules_path\n\u001b[0;32m-> 1159\u001b[0m increase_load_count(name, resource_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/load.py:232\u001b[0m, in \u001b[0;36mincrease_load_count\u001b[0;34m(name, resource_type)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mHF_DATASETS_OFFLINE \u001b[39mand\u001b[39;00m config\u001b[39m.\u001b[39mHF_UPDATE_DOWNLOAD_COUNTS:\n\u001b[1;32m    231\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m         head_hf_s3(name, filename\u001b[39m=\u001b[39;49mname \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.py\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataset\u001b[39m=\u001b[39;49m(resource_type \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    233\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/utils/file_utils.py:96\u001b[0m, in \u001b[0;36mhead_hf_s3\u001b[0;34m(identifier, filename, use_cdn, dataset, max_retries)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhead_hf_s3\u001b[39m(\n\u001b[1;32m     94\u001b[0m     identifier: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, use_cdn\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dataset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_retries\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     95\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[requests\u001b[39m.\u001b[39mResponse, \u001b[39mException\u001b[39;00m]:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m http_head(\n\u001b[1;32m     97\u001b[0m         hf_bucket_url(identifier\u001b[39m=\u001b[39;49midentifier, filename\u001b[39m=\u001b[39;49mfilename, use_cdn\u001b[39m=\u001b[39;49muse_cdn, dataset\u001b[39m=\u001b[39;49mdataset),\n\u001b[1;32m     98\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m     99\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/utils/file_utils.py:429\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    427\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    428\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 429\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    430\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    431\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    432\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    433\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    434\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    435\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[1;32m    436\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    437\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    438\u001b[0m )\n\u001b[1;32m    439\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/datasets/utils/file_utils.py:328\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    326\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    329\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/urllib3/connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1096\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1099\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1100\u001b[0m         (\n\u001b[1;32m   1101\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mconn\u001b[39m.\u001b[39mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1107\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/urllib3/connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m is_time_off:\n\u001b[1;32m    634\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    635\u001b[0m         (\n\u001b[1;32m    636\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSystem time is way off (before \u001b[39m\u001b[39m{\u001b[39;00mRECENT_DATE\u001b[39m}\u001b[39;00m\u001b[39m). This will probably \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[1;32m    640\u001b[0m     )\n\u001b[0;32m--> 642\u001b[0m sock_and_verified \u001b[39m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[1;32m    643\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    644\u001b[0m     cert_reqs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_reqs,\n\u001b[1;32m    645\u001b[0m     ssl_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_version,\n\u001b[1;32m    646\u001b[0m     ssl_minimum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_minimum_version,\n\u001b[1;32m    647\u001b[0m     ssl_maximum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_maximum_version,\n\u001b[1;32m    648\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    649\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    650\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    651\u001b[0m     cert_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    652\u001b[0m     key_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    653\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    654\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    655\u001b[0m     ssl_context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_context,\n\u001b[1;32m    656\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    657\u001b[0m     assert_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_hostname,\n\u001b[1;32m    658\u001b[0m     assert_fingerprint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_fingerprint,\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    660\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock_and_verified\u001b[39m.\u001b[39msocket\n\u001b[1;32m    661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_verified \u001b[39m=\u001b[39m sock_and_verified\u001b[39m.\u001b[39mis_verified\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/urllib3/connection.py:782\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    780\u001b[0m         server_hostname \u001b[39m=\u001b[39m normalized\n\u001b[0;32m--> 782\u001b[0m ssl_sock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    783\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    784\u001b[0m     keyfile\u001b[39m=\u001b[39;49mkey_file,\n\u001b[1;32m    785\u001b[0m     certfile\u001b[39m=\u001b[39;49mcert_file,\n\u001b[1;32m    786\u001b[0m     key_password\u001b[39m=\u001b[39;49mkey_password,\n\u001b[1;32m    787\u001b[0m     ca_certs\u001b[39m=\u001b[39;49mca_certs,\n\u001b[1;32m    788\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49mca_cert_dir,\n\u001b[1;32m    789\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49mca_cert_data,\n\u001b[1;32m    790\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    791\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    792\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    793\u001b[0m )\n\u001b[1;32m    795\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[39mif\u001b[39;00m assert_fingerprint:\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/urllib3/util/ssl_.py:470\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:  \u001b[39m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[1;32m    471\u001b[0m \u001b[39mreturn\u001b[39;00m ssl_sock\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/site-packages/urllib3/util/ssl_.py:514\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    511\u001b[0m     SSLTransport\u001b[39m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    501\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    502\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    503\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    504\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    505\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    506\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    507\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    508\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/ssl.py:1073\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   1071\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1073\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1074\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/miniconda3/envs/dnabert/lib/python3.8/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "text_exercise.train_wikitext(device, positional_embedding=PositionalEmbedding, attention=BertSelfAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Promoter detection (7 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we detect promoter in DNA sequence.\n",
        "\n",
        "A promoter is a region of DNA upstream of a gene where relevant proteins (such as RNA polymerase and transcription factors) bind to initiate transcription of that gene. Promoter detection is to identify if there are promoter regions in the given DNA sequence. We have covered this in the lecture. (If you are interested in the promoter, you can check this [link](https://www.genome.gov/genetics-glossary/Promoter) for more details.)\n",
        "\n",
        "Here, we use a transformer and a classifier. The transformer first embeds the DNA sequences into features, and then the classifier detects the promoter based on the features.\n",
        "\n",
        "The main difference between text and DNA sequence is how to tokenize the sequence. Thus, you need to implement a tokenizer for the DNA sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. DNA Tokenizer (1 pts)\n",
        "\n",
        "Here, you will implement the DNA tokenizer the same as in DNABERT. Please check this [paper](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680) for implementation details. Also, you need to check the data type and shape for both input and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DNATokenizer(data.Tokenizer):\n",
        "    def __init__(self, k, vocab, unknown=\"[UNK]\"):\n",
        "        super().__init__(vocab, unknown)\n",
        "\n",
        "        # self.k is the k of k-mers\n",
        "        self.k = k\n",
        "\n",
        "    def _parse_text(self, text):\n",
        "        n = len(text)\n",
        "        return ['[CLS]'] + [text[i:i + self.k] for i in range(n - self.k + 1)] + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Test BERT on DNA Sequence\n",
        "\n",
        "In this section, you will train BERT on DNA sequence to learn the embedding of DNA sequence. The code is provided below and you do not need to write anything.\n",
        "\n",
        "Hint: the final evaluation accuracy should be higher than 0.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the trained model...\n",
            "Number of trainable model parameters: 118869\n",
            "Train Acc = 0.252133 Eval Acc = 0.242678\n"
          ]
        }
      ],
      "source": [
        "kmer = 3\n",
        "mask_length = kmer\n",
        "VOCAB_3MER = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"AAA\", \"AAT\", \"AAC\", \"AAG\", \"ATA\", \"ATT\", \"ATC\", \"ATG\", \"ACA\", \"ACT\", \"ACC\", \"ACG\", \"AGA\", \"AGT\", \"AGC\", \"AGG\", \"TAA\", \"TAT\", \"TAC\", \"TAG\", \"TTA\", \"TTT\", \"TTC\", \"TTG\", \"TCA\", \"TCT\", \"TCC\", \"TCG\", \"TGA\", \"TGT\", \"TGC\", \"TGG\", \"CAA\", \"CAT\", \"CAC\", \"CAG\", \"CTA\", \"CTT\", \"CTC\", \"CTG\", \"CCA\", \"CCT\", \"CCC\", \"CCG\", \"CGA\", \"CGT\", \"CGC\", \"CGG\", \"GAA\", \"GAT\", \"GAC\", \"GAG\", \"GTA\", \"GTT\", \"GTC\", \"GTG\", \"GCA\", \"GCT\", \"GCC\", \"GCG\", \"GGA\", \"GGT\", \"GGC\", \"GGG\" ]\n",
        "\n",
        "raw_training_data = data.load_csv(\"./data/train.csv\")\n",
        "raw_test_data = data.load_csv(\"./data/test.csv\")\n",
        "\n",
        "dna_max_len = 300\n",
        "batch_size = 128\n",
        "max_dna_mask = 100\n",
        "dataset_size = 1000\n",
        "num_layers = 3\n",
        "num_heads = 6\n",
        "dna_config = SimpleNamespace(\n",
        "        vocab_size=len(VOCAB_3MER),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=dna_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=num_heads,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=num_layers,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "tokenizer = DNATokenizer(k=kmer, vocab=VOCAB_3MER)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(raw_training_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks = data.generate_masked_data(raw_test_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "if not os.path.exists('models/bert_longmask.pth'):\n",
        "  print(\"Training the model...\")\n",
        "  model = models.BertForMaskedLM(config=dna_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention).to(device)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "  for epoch in range(50):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss, outputs, hidden_states, _ = model(\n",
        "          input_ids=batch_input_ids.to(device),\n",
        "          token_type_ids=batch_segment_ids.to(device),\n",
        "          masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "          attention_mask=batch_attention_mask.to(device)\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      model.eval()\n",
        "      total_eval_loss = 0\n",
        "      for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in test_loader:\n",
        "        with torch.no_grad():\n",
        "          loss, outputs, hidden_states, _ = model(\n",
        "            input_ids=batch_input_ids.to(device),\n",
        "            token_type_ids=batch_segment_ids.to(device),\n",
        "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "            attention_mask=batch_attention_mask.to(device)\n",
        "          )\n",
        "          if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0 :\n",
        "            print(\"found patting\", batch_attention_mask.sum())\n",
        "          total_eval_loss += loss.item()\n",
        "      avg_eval_loss = total_eval_loss / len(test_loader)\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'eval cost =', '{:.6f}'.format(avg_eval_loss))\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(model, 'models/bert_longmask.pth')\n",
        "\n",
        "# Load the trained model\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  model = torch.load('models/bert_longmask.pth')\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "average_train_acc, _ = evaluation.model_masked_label_accuracy(model, train_loader, device)\n",
        "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(model, test_loader, device)\n",
        "print('Train Acc =', '{:.6f}'.format(average_train_acc), 'Eval Acc =', '{:.6f}'.format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Visualize the Attentions (1 pt)\n",
        "\n",
        "Here, you can visualize the self-attention. \n",
        "\n",
        "Question: compare the visualization to Section 1.3, what can you find here? How do you explain it?\n",
        "\n",
        "**Answer:** We can see a clear diagonal pattern in the visualization. This demonstrates that each `k-mer`'s embedding is defined by the closest following `k-mers`. This is in contrast to 1.3 where the model also payed attention to previuous tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18a180f389af482b8edf86f5acc6ea37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Dropdown(description='Sample:', options=(4, 13, 21, 25, 28, 30, 37, 38, 42, 45, 47, 50, 62, 77,…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0600abcc12149658df92e7d218d049e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_attention, sample_id_selector, layer_selector, head_selector = visualization.display_multi_attantion(attentions=last_test_attention, tokenizer=tokenizer, input_ids=input_ids,  layers=range(1, num_layers+1),  heads=range(1, num_heads+1))\n",
        "ui = widgets.HBox([sample_id_selector, layer_selector, head_selector])  \n",
        "out = widgets.interactive_output(visualize_attention, {'sample_id': sample_id_selector, 'layer': layer_selector, 'head': head_selector})\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Use your pretrained model for promoter detection (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You already have the embeddings for the DNA sequence. Now, you are going to build a classifier based on the DNA embeddings. The classifier is to perform promoter detection. Specifically, the DNA sequence will be classified into *'contains promoter'* or *'does not contain promoter'*.\n",
        "\n",
        "Hint: \n",
        "- We now want to annotate data (get the label for each sample), not predict masked data anymore!\n",
        "- You can reuse some parts of the code in the previous sections, e.g. dataloader and training pipeline in Section 2.2.\n",
        "- If you implement the previous section correctly (the Eval Acc > 0.2 in Section 2.2), you already have an pre-trained object named 'model' of class models.BertForMaskedLM. You can directly use it.\n",
        "- The evaluation accuracy of this task should be around 0.6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a binary classifier\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, embedder):\n",
        "        super().__init__()\n",
        "        self.embedder = embedder\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "      # Get the output embeddings of BertForMaskedLM\n",
        "      hidden_states = self.embedder(x)[1]\n",
        "\n",
        "      # Get the output embeddings\n",
        "      outputs = hidden_states[-1]\n",
        "\n",
        "      # Use the [CLS] token embedding\n",
        "      x = outputs[:, 0, :]\n",
        "\n",
        "      # Run the classifier\n",
        "      x = self.linear1(x)\n",
        "      x = self.linear2(x)\n",
        "      x = self.sigmoid(x)\n",
        "\n",
        "      return x.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data (without masks indeed)\n",
        "input_ids, labels = data.generate_labeled_data(raw_training_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "test_input_ids, test_labels = data.generate_labeled_data(raw_test_data, tokenizer, max_len=dna_max_len, max_size=dataset_size)\n",
        "\n",
        "# Convert labels to float\n",
        "labels = labels.float()\n",
        "test_labels = test_labels.float()\n",
        "\n",
        "# Define data loaders\n",
        "train_dataset = TensorDataset(input_ids, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the trained model...\n",
            "Test cost = 308.532658 test acc = 0.586000\n"
          ]
        }
      ],
      "source": [
        "# Train the model first\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if not os.path.exists('models/promoter_clf.pth'):\n",
        "\n",
        "  print(\"Training the model...\")\n",
        "  # Freeze the weights of the embedder model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Define the classifier\n",
        "  clf = BinaryClassifier(input_size=60, hidden_size=30, output_size=1, embedder=model).to(device)\n",
        "  clf_optimizer = optim.AdamW(clf.parameters(), lr=1e-2)\n",
        "\n",
        "  # Train the classifier\n",
        "  clf.train()\n",
        "  for epoch in range(50):\n",
        "\n",
        "    # Training\n",
        "    total_train_loss = 0\n",
        "    preds = []\n",
        "    labs = []\n",
        "    for batch_input_ids, batch_labels in train_loader:\n",
        "\n",
        "      # Reset the gradients\n",
        "      clf_optimizer.zero_grad()\n",
        "\n",
        "      # Get the output of the classifier\n",
        "      batch_output = clf(batch_input_ids)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = criterion(batch_output.to(device), batch_labels.to(device))\n",
        "\n",
        "      # Add prediction and labels\n",
        "      bpred = (batch_output > 0.5).float().tolist()\n",
        "      blabs = batch_labels.tolist()\n",
        "      preds.extend(bpred)\n",
        "      labs.extend(blabs)\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      clf_optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "    # Evaluate the epoch\n",
        "    preds, labs = np.array(preds), np.array(labs)\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_acc = np.mean(preds == labs)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'train acc =', '{:.6f}'.format(train_acc))\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(clf, 'models/promoter_clf.pth')\n",
        "\n",
        "# Load the trained model\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  clf = torch.load('models/promoter_clf.pth')\n",
        "\n",
        "# Evaluation\n",
        "total_test_loss = 0\n",
        "preds = []\n",
        "labs = []\n",
        "clf.eval()\n",
        "for batch_input_ids, batch_labels in test_loader:\n",
        "\n",
        "  # Get the output of the classifier\n",
        "  batch_output = clf(batch_input_ids)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = criterion(batch_output.to(device), batch_labels.to(device))\n",
        "\n",
        "  # Add prediction and labels\n",
        "  bpred = (batch_output > 0.5).float().tolist()\n",
        "  blabs = batch_labels.tolist()\n",
        "  preds.extend(bpred)\n",
        "  labs.extend(blabs)\n",
        "\n",
        "  total_test_loss += loss.item()\n",
        "\n",
        "preds, labs = np.array(preds), np.array(labs)\n",
        "avg_test_loss = total_test_loss / len(test_loader)\n",
        "test_acc = np.mean(preds == labs)\n",
        "print('Test cost =', '{:.6f}'.format(avg_test_loss), 'test acc =', '{:.6f}'.format(test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5. Additional question (1 pt)\n",
        "\n",
        "Now we change mask_length = 1 (already changed, you do not need to implement anything).\n",
        "Let's run the code below and check the accuracy.\n",
        "\n",
        "Question: What is the final masked token prediction accuracy? How do you explain this?\n",
        "\n",
        "**Answer:** It is relatively high (92 %) compare to the training with potentially way higher number of masked tokens. Since in this case we only mask one token, the model has much more context to predict the masked token. This is in contrast to the previous case where the model had to predict multiple tokens at once. Thus, we see the rapid increase in performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the trained model...\n",
            "Number of trainable model parameters: 118869\n",
            "Train Acc = 0.921650 Eval Acc = 0.923077\n"
          ]
        }
      ],
      "source": [
        "kmer = 3\n",
        "mask_length = 1\n",
        "\n",
        "dna_max_len = 300\n",
        "batch_size = 128\n",
        "max_dna_mask = 100\n",
        "dataset_size = 1000\n",
        "num_layers = 3\n",
        "num_heads = 6\n",
        "dna_config = SimpleNamespace(\n",
        "        vocab_size=len(VOCAB_3MER),\n",
        "        hidden_size=60,\n",
        "        max_position_embeddings=dna_max_len,\n",
        "        type_vocab_size=1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        num_attention_heads=num_heads,\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=160,\n",
        "        num_hidden_layers=num_layers,\n",
        "        is_decoder=False,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        pruned_heads = {},\n",
        "        initializer_range=0.02,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "tokenizer = DNATokenizer(k=3, vocab=VOCAB_3MER)\n",
        "input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks = data.generate_masked_data(raw_training_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks = data.generate_masked_data(raw_test_data, tokenizer, max_len=dna_max_len, max_mask=max_dna_mask, k=mask_length, mask_rate=0.05, max_size=dataset_size)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_segment_ids, test_masked_lm_labels, test_labels_idx, test_labels, test_attention_masks)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "if not os.path.exists('models/bert_shortmask.pth'):\n",
        "  print(\"Training the model...\")\n",
        "  model = models.BertForMaskedLM(config=dna_config, positional_embedding=PositionalEmbedding, attention=BertSelfAttention).to(device)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "  for epoch in range(50):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss, outputs, hidden_states, _ = model(\n",
        "          input_ids=batch_input_ids.to(device),\n",
        "          token_type_ids=batch_segment_ids.to(device),\n",
        "          masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "          attention_mask=batch_attention_mask.to(device)\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      model.eval()\n",
        "      total_eval_loss = 0\n",
        "      for batch_input_ids, batch_segment_ids, batch_masked_lm_labels, _, _, batch_attention_mask in test_loader:\n",
        "        with torch.no_grad():\n",
        "          loss, outputs, hidden_states, _ = model(\n",
        "            input_ids=batch_input_ids.to(device),\n",
        "            token_type_ids=batch_segment_ids.to(device),\n",
        "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
        "            attention_mask=batch_attention_mask.to(device)\n",
        "          )\n",
        "          if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0 :\n",
        "            print(\"found patting\", batch_attention_mask.sum())\n",
        "          total_eval_loss += loss.item()\n",
        "      avg_eval_loss = total_eval_loss / len(test_loader)\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'train cost =', '{:.6f}'.format(avg_train_loss), 'eval cost =', '{:.6f}'.format(avg_eval_loss))\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(model, 'models/bert_shortmask.pth')\n",
        "\n",
        "else:\n",
        "  print(\"Loading the trained model...\")\n",
        "  model = torch.load('models/bert_shortmask.pth')\n",
        "  print(f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\")\n",
        "\n",
        "average_train_acc, _ = evaluation.model_masked_label_accuracy(model, train_loader, device)\n",
        "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(model, test_loader, device)\n",
        "print('Train Acc =', '{:.6f}'.format(average_train_acc), 'Eval Acc =', '{:.6f}'.format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using foundation model (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Introduction\n",
        "\n",
        "In this section, we aim to use a foundation model, DNABERT, to perform promoter detection.\n",
        "A foundation model is a model pretrained on large datasets. Foundation models serve as the foundational building blocks upon which various applications can be constructed.\n",
        "\n",
        "Here, we use DNABERT as the foundation model. We first apply it on DNA sequence to get the embedding. Then, we train a classifier on the embedding as in Section 2. Please follow this [link](https://github.com/Zhihan1996/DNABERT_2) to load the foundation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Implementation\n",
        "\n",
        "**Consider this situation:** You get a dataset about promoter detection, and you build your model to perform the task as in Section 2. However, the performance is not good since the model is not strong enough. Suddenly, you think we can use a large pre-trained model to embed DNA sequences. Then, you search online and find the pre-trained model [DNABERT](https://github.com/Zhihan1996/DNABERT_2). Now, you want to perform promoter detection using the pre-trained DNABERT.\n",
        "\n",
        "There is no coding framework in this section. Just make things work (get good test accuracy) using the pre-trained model!\n",
        "\n",
        "Hint: \n",
        "- We encourage you to create a **new environment** following the instructions of Section 3 in this [link](https://github.com/Zhihan1996/DNABERT_2). (When you face the error \"The model class you are passing has a config_class attribute that is not consistent with the config class you passed ...\", creating a new environment can save you.)\n",
        "- Section 4 in this [link](https://github.com/Zhihan1996/DNABERT_2) shows you how to load and use the pre-trained foundation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a. Load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(input_ids, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. Get the embeddings of the DNA sequences using pretrained model.\n",
        "\n",
        "Hint: \n",
        "- This step can take some time. Thus, you can start with a small sample size, and then increase it when you have made sure that everything works correctly.\n",
        "- After getting the embeddings, you can save them so that you can directly load them next time without running the foundation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ludekcizinsky/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained model\n",
        "pretokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "premodel = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "\n",
        "# Freeze the weights of the pretrained model\n",
        "for param in premodel.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 300, 768])\n",
            "torch.Size([2, 768])\n"
          ]
        }
      ],
      "source": [
        "for batch_input_ids, batch_labels in train_loader:\n",
        "\n",
        "    # Get the output\n",
        "    hidden_states = premodel(batch_input_ids[:2])[0]\n",
        "    \n",
        "    # Use the [CLS] token embedding\n",
        "    x = hidden_states[:, 0, :]\n",
        "    \n",
        "    # Batch size x 768\n",
        "    print(x.shape)\n",
        "    break   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Using tsne or umap to visualize the embedding space.\n",
        "# Hint: you can import other packages here for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. Train a classifier.\n",
        "\n",
        "Hint: It is easy to overfit on the training set. Try to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Define your classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Train your classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
